{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pip\n",
    "try:\n",
    "    import pandas as pd\n",
    "except ImportError:\n",
    "        pip.main(['install', pandas])  \n",
    "import os\n",
    "\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "try:\n",
    "    import numpy as np\n",
    "except ImportError:\n",
    "    pip.main(['install', numpy])  \n",
    "    \n",
    "try:\n",
    "    import warnings\n",
    "except ImportError:\n",
    "        pip.main(['install', warnings])  \n",
    "\n",
    "try:\n",
    "    from openpyxl import load_workbook\n",
    "except ImportError:\n",
    "        pip.main(['install', openpyxl])  \n",
    "\n",
    "try:\n",
    "    import xlsxwriter\n",
    "except ImportError:\n",
    "        pip.main(['install', xlsxwriter])  \n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "except ImportError:\n",
    "        pip.main(['install', matplotlib])  \n",
    "        \n",
    "from scipy.interpolate import interp1d\n",
    "        \n",
    "from datetime import datetime\n",
    "\n",
    "try:\n",
    "    from scipy.stats import t\n",
    "except ImportError:\n",
    "    pip.main(['install', scipy.stats]) \n",
    "    \n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "working_directory = os.getcwd()\n",
    "print(working_directory)\n",
    "try:\n",
    "    desktop = os.path.join(os.path.join(os.path.expanduser('~')), 'Desktop') \n",
    "except:\n",
    "    desktop = os.path.join(os.path.join(os.environ['USERPROFILE']), 'Desktop') \n",
    "print(desktop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vacancy data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute net yields from gross yields, we need to know the percentage of rental homes that sit vacant.  We can get this information from the AHS as well.  We use the same dataset(including removing units in housing projects, those with bars on the windows, those thatare rent stabilized, and those missing data).  We label a home as a vacant rental if the survey identifies it as for rent only, for rent or for sale, or rented but not yet occupied.  The vacancy rate is the ratio of this number to this number plus the number of renter-occupied homes.For those city-year cells without enough datapoints, we use a projection from the rest of thedataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Response codes\n",
    "01: For rent only\n",
    "02: For rent or for sale\n",
    "03: For sale only\n",
    "04: Rented, but not yet occupied\n",
    "05: Sold, but not yet occupied\n",
    "06: Held for occasional use throughout the year\n",
    "07: Other (specify)\n",
    "08: Seasonal-Summer only\n",
    "09: Seasonal-Winter only\n",
    "10: Other seasonal (SPECIFY)\n",
    "11: Migratory\n",
    "N or -6: Not applicable\n",
    "\n",
    "\n",
    "\n",
    "Response Codes that we will consider vacant\n",
    "\n",
    "01: For rent only\n",
    "02: For rent or for sale\n",
    "03: For sale only\n",
    "04: Rented, but not yet occupied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distribution(data, filter_code):\n",
    "    dct = {}\n",
    "\n",
    "    for i in data.year.unique():\n",
    "            dct[i] = data.loc[data.year == i, filter_code].value_counts()\n",
    "\n",
    "    dct = pd.DataFrame(dct)\n",
    "\n",
    "    return dct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tax Rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need a panel of tax rates to compute net yields.  Our sources are Emrath (2002) for1990 and 2000 tax rates from Census data, and the National Association of Home Builders(NAHB) for 2005 to 2012 tax rates from ACS data.  The tax rate data are available by state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Interpolating missing years"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As  the  survey  is  biannual  and  the  tax  rates  are  from  Census,  we  linearly  interpolate  therent-to-price ratios, vacancy rates, and tax rates to even-numbered years and other missingyears (in the case of the tax data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Net Yields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting from gross yields, we compute net yields using the follow costs, some of which areexpressed as a percentage of rent and some of which are a percentage of home value.  Weuse expense ratios from Morgan Stanley,  “The New Age of Buy-To-Rent,”  July 31,  2013.Similar, but less comprehensive, assumptions appear in Bernanke (2012) “The US HousingMarket:  Current conditions and policy considerations.”  The assumptions underlying CoreLogic’s Rental Trends, discussed below, are also broadly consistent with ours, however someof  their  cost  estimates  rely  on  direct  proprietary  data  rather  than  ratios  of  rent  or  houseprice.\n",
    "\n",
    "- Insurance:  0.375% of price\n",
    "- Repairs:  0.6% of price\n",
    "- Capex:  1.15% of price\n",
    "- Property manager:  5.9% of rent\n",
    "- Credit loss:  0.73% of ren\n",
    "- Tax: on price\n",
    "- Vacancy:  on rent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get the rental data, vacancy data, and owned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_outlier(hedonic_model_data, quant_remove, field):\n",
    "\n",
    "    keep_ix = []\n",
    "    for i in hedonic_model_data.CITY.unique():\n",
    "        city_data = hedonic_model_data.loc[hedonic_model_data.CITY == i]\n",
    "        lower_quant = city_data.loc[:,field].quantile(quant_remove)\n",
    "        upper_quant = city_data.loc[:,field].quantile(1.0-quant_remove)\n",
    "        city_data = city_data.loc[city_data[field] > lower_quant]\n",
    "        city_data = city_data.loc[city_data[field] < upper_quant]\n",
    "        keep_ix = keep_ix + city_data.index.tolist()\n",
    "    return hedonic_model_data.loc[hedonic_model_data.index.isin(keep_ix)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rental_data = pd.read_pickle(os.path.join(desktop,'rented_cleaned.pkl'))\n",
    "vacant = pd.read_pickle(os.path.join(desktop,'vacancy.pkl'))\n",
    "owned_data = pd.read_pickle(os.path.join(desktop,'owned_cleaned_weighted.pkl'))\n",
    "\n",
    "\n",
    "\n",
    "print(owned_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_taxes(data):\n",
    "      \n",
    "    data.loc[:,'states']=data.loc[:,'CITY']\n",
    "    diction={'Anaheim':'California',\n",
    "                                          'Atlanta': 'Georgia',\n",
    "                                        'Baltimore':  'Maryland',\n",
    "                                        'Boston':  'Massachusetts', \n",
    "                                        'Chicago': 'Illinois',\n",
    "                                        'Cleveland':'Ohio',\n",
    "                                          'Dallas':'Texas',\n",
    "                                        'Detroit':'Michigan',\n",
    "                                          'Houston':'Texas',\n",
    "                                        'Kansas City': 'Missouri',\n",
    "                                      'Los Angeles':'California',\n",
    "                                          'Miami': 'Florida',\n",
    "                                     'Minneapolis':'Minnesota',\t\n",
    "                                       'Nassau-Suffolk':'New York',\n",
    "                                              'New York': 'New York',\n",
    "                                            'Newark':'New Jersey',\t\n",
    "                                           'Oakland':'California',\n",
    "                                    'Oklahoma City':'Oklahoma',\n",
    "                                  'Philadelphia':'Pennsylvania',\n",
    "                                           'Phoenix':'Arizona',\n",
    "                                   'Pittsburgh':'Pennsylvania',\n",
    "                                       'Riverside': 'California',\n",
    "                                            'San Diego':'California',\n",
    "                                           'San Francisco':'California',\n",
    "                                            'San Jose':'California',\n",
    "                                            'Washington':'DC',\n",
    "                                         'Seattle':'Washington',\n",
    "                                          'St. Louis':'Missouri',\n",
    "                                              'Tampa':'Florida'}\n",
    "\n",
    "    data['state']= data.states.map(diction)  \n",
    "    data.loc[:, 'tax'] = 0.0\n",
    "\n",
    "\n",
    "    tax_file=os.path.join(working_directory, 'tax_data.xlsx')\n",
    "    tax_dta=pd.read_excel(tax_file)\n",
    "    tax_dta.columns=['state_code','state',1990,2000,2009,2014]\n",
    "    tax_dta.loc[:,[1990,2000,2009,2014]] = tax_dta.loc[:,[1990,2000,2009,2014]] / 1000.\n",
    "\n",
    "    tax_dta.columns=['state_code','state','1990','2000','2009','2014']\n",
    "    tax_complete=pd.DataFrame(np.nan,index=tax_dta.index, columns=range(1985,2018))\n",
    "    \n",
    "\n",
    "    # linear interpolation\n",
    "    for i in range(tax_dta.shape[0]):\n",
    "        x=[1990,2000,2009,2014]\n",
    "        y= tax_dta.iloc[i,:][2:].values\n",
    "        f=interp1d(x,y, fill_value='extrapolate')\n",
    "        xnew=np.linspace(1985,2017,num=33,endpoint=True)\n",
    "        tax_complete.iloc[i,:]=f(xnew)\n",
    "\n",
    "    tax_complete.insert(0,'state_code',tax_dta.state_code)\n",
    "    tax_complete.insert(1,'state',tax_dta.state)\n",
    "\n",
    "    tax_complete.head(5)\n",
    "\n",
    "    for i in tax_complete['state'].tolist():\n",
    "        for j in tax_complete.columns[2:].tolist():\n",
    "            data.loc[(data['state'] == i)&(data['year'] == j), 'tax'] = \\\n",
    "            tax_complete.loc[tax_complete['state'] == i,j].values[0]\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "function for helping to find the pooled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pooled_data(net_yield_owned, string_var):\n",
    "    net_yield_dict = {}\n",
    "\n",
    "\n",
    "    for city in net_yield_owned['CITY'].unique():\n",
    "        net_yield_dict[city] = {}\n",
    "        for y in net_yield_owned['year'].unique():\n",
    "            net_yield_dict[city][y] = net_yield_owned.loc[(net_yield_owned['CITY']== city) & (net_yield_owned['year']==y), string_var].mean()\n",
    "    ny_df_pooled = pd.DataFrame.from_dict(net_yield_dict, orient = 'index')\n",
    "    \n",
    "    return pd.DataFrame(ny_df_pooled.dropna())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fixed costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insurance = 0.375 / 100.0\n",
    "repairs = 0.6 / 100.0\n",
    "capex = 1.15 / 100.0\n",
    "property_manager = 5.9 / 100.0\n",
    "credit_loss = 0.73 / 100.0 \n",
    "\n",
    "fixed_rent_costs = property_manager + credit_loss\n",
    "fixed_price_costs = insurance + repairs + capex \n",
    "print(\"fixed rent costs\")\n",
    "print(fixed_rent_costs)\n",
    "print(\"fixed price costs\")\n",
    "print(fixed_price_costs)\n",
    "\n",
    "owned_data.loc[:, 'fixed rent costs'] = fixed_rent_costs\n",
    "owned_data.loc[:, 'fixed price costs'] = fixed_price_costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get the vacancy costs. note the vacancy costs are on rent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "owned_data.loc[:, 'vacancy costs'] = 0.0\n",
    "\n",
    "for i in vacant['CITY'].unique():\n",
    "    print(\"The vacancy rate for \" + str(i) + ' is')\n",
    "    vacancy_rate = vacant.loc[vacant['CITY'] == i, 'Is_Vacant'].sum() / len(vacant.loc[vacant['CITY'] == i, 'Is_Vacant'])\n",
    "    print(vacancy_rate)\n",
    "    owned_data.loc[owned_data['CITY'] == i, 'vacancy costs'] = vacancy_rate\n",
    "    \n",
    "owned_data.loc[:, 'vacancy costs'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get the tax costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "owned_data=owned_data.reset_index(drop = True)\n",
    "rental_data=rental_data.reset_index(drop = True)\n",
    "\n",
    "owned_data = get_taxes(owned_data)\n",
    "rental_data = get_taxes(rental_data)\n",
    "       \n",
    "print(owned_data['tax'].hist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rental_data['tax'].hist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get the total costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "owned_data.loc[:, 'total costs'] = (owned_data.loc[:, 'fixed rent costs']  + owned_data.loc[:, 'vacancy costs']) * owned_data.loc[:,'RENT'] \\\n",
    "+ (owned_data.loc[:, 'tax'] + owned_data.loc[:, 'fixed price costs']) * owned_data.loc[:, 'VALUE']\n",
    "\n",
    "\n",
    "#histogram of total costs\n",
    "owned_data.loc[:, 'total costs'].hist()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "(owned_data.loc[:, 'total costs'] / owned_data.loc[:,'RENT']).hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "appreciation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "appreciation_data = pd.read_csv(os.path.join(working_directory, 'appreciation_data/City_zhvi_uc_sfr_tier_0.33_0.67_sm_sa_mon.csv'))\n",
    "\n",
    "\n",
    "change_data = appreciation_data[appreciation_data.columns[8:]]\n",
    "\n",
    "change_data.columns = pd.to_datetime(change_data.columns)\n",
    "\n",
    "\n",
    "appreciation_data.drop(labels = appreciation_data.columns[8:] , axis = 1, inplace = True)\n",
    "\n",
    "\n",
    "rates = pd.DataFrame()\n",
    "\n",
    "count = 1997\n",
    "\n",
    "while count <= 2013:\n",
    "\n",
    "    end = datetime(count+2, 6, 1)\n",
    "\n",
    "    start = datetime(count, 6, 1)\n",
    "\n",
    "    \n",
    "\n",
    "    s = change_data[change_data.columns[(change_data.columns >= start)]]\n",
    "\n",
    "    e = s[s.columns[(s.columns <= end)]]\n",
    "            \n",
    "\n",
    "    appreciation_data[count] = ((1.+((e[e.columns[-1]] - e[e.columns[0]]) / e[e.columns[0]]))**.5)-1.\n",
    "    #appreciation_data[count] = ((e[e.columns[-1]] - e[e.columns[0]]) / e[e.columns[0]]) / 2.0\n",
    "    \n",
    "    #get the biweekly change\n",
    "\n",
    "    count = count + 2\n",
    "\n",
    "    \n",
    "\n",
    "print(appreciation_data)\n",
    "\n",
    "\n",
    "net_yield_owned = owned_data.loc[owned_data['year'] >= 1997]\n",
    "net_yield_owned.loc[:, 'appreciation'] = 0.0\n",
    "net_yield_owned = net_yield_owned.loc[net_yield_owned['CITY'] != 'Nassau-Suffolk']\n",
    "\n",
    "\n",
    "error_cities = []\n",
    "\n",
    "app_dict = {}\n",
    "\n",
    "for city in net_yield_owned['CITY'].unique():\n",
    "    app_dict[city] = {}\n",
    "\n",
    "    for y in net_yield_owned['year'].unique():\n",
    "\n",
    "        if len(appreciation_data.loc[(appreciation_data['RegionName'] == city.rstrip()), y]) >= 1:\n",
    "\n",
    "            if len(net_yield_owned.loc[(net_yield_owned['CITY']== city) & (net_yield_owned['year']==y), 'appreciation']) >= 1:\n",
    "\n",
    "                net_yield_owned.loc[(net_yield_owned['CITY']== city) & (net_yield_owned['year']==y), 'appreciation'] = appreciation_data.loc[(appreciation_data['RegionName'] == city.strip()), y].values[0]\n",
    "                app_dict[city][y] = appreciation_data.loc[(appreciation_data['RegionName'] == city.strip()), y].values[0]\n",
    "        else:\n",
    "\n",
    "            error_cities.append([city, y])\n",
    "\n",
    "\n",
    "print(error_cities)\n",
    "print(net_yield_owned['CITY'].unique())\n",
    "\n",
    "app_df = pd.DataFrame.from_dict(app_dict, orient = 'index')\n",
    "print(app_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_yield_owned.loc[:, 'appreciation'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(net_yield_owned))\n",
    "net_yield_owned = net_yield_owned.dropna(subset = ['VALUE', 'appreciation', 'total costs', 'RENT'], axis = 0, how = 'any')\n",
    "print(len(net_yield_owned))\n",
    "\n",
    "net_yield_owned.loc[:, 'total returns'] = (net_yield_owned .loc[:, 'VALUE'] * net_yield_owned.loc[:, 'appreciation'] + net_yield_owned.loc[:,'RENT'] - \\\n",
    "                                   net_yield_owned.loc[:, 'total costs']) / net_yield_owned .loc[:, 'VALUE']\n",
    "\n",
    "\n",
    "net_yield_owned.loc[:, 'net yields'] = (net_yield_owned.loc[:,'RENT'] - \\\n",
    "                                   net_yield_owned.loc[:, 'total costs']) / net_yield_owned .loc[:, 'VALUE']\n",
    "\n",
    "net_yield_owned.loc[:, 'net yields'].hist()\n",
    "\n",
    "net_yield_owned.loc[:, 'gross yields'] = (net_yield_owned.loc[:,'RENT'] ) / net_yield_owned .loc[:, 'VALUE']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_yield_owned.loc[:,'RENT'].mean()\n",
    "print(( net_yield_owned.loc[:, 'net yields'] / net_yield_owned.loc[:, 'gross yields']).mean())\n",
    "print(( net_yield_owned.loc[:, 'net yields'] / net_yield_owned.loc[:, 'gross yields']).median())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_yields_by_year = {}\n",
    "\n",
    "for i in net_yield_owned['year'].unique():\n",
    "    net_yields_by_year[i] = net_yield_owned.loc[net_yield_owned['year'] == i, 'net yields'].mean()\n",
    "    \n",
    "net_yields_by_year = pd.DataFrame.from_dict(net_yields_by_year, orient = 'index')\n",
    "\n",
    "net_yields_by_year.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gross yields by year\n",
    "gross_yields_by_year = {}\n",
    "\n",
    "for i in net_yield_owned['year'].unique():\n",
    "    gross_yields_by_year[i] = net_yield_owned.loc[net_yield_owned['year'] == i, 'gross yields'].mean()\n",
    "    \n",
    "gross_yields_by_year = pd.DataFrame.from_dict(gross_yields_by_year, orient = 'index')\n",
    "\n",
    "gross_yields_by_year.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_yield_owned.loc[:, 'gross yields'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get the quintile information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quintiles = pd.DataFrame(pd.qcut(net_yield_owned['VALUE'], 5, labels=np.arange(5, 0, -1)))\n",
    "quintiles.columns = ['quint']\n",
    "quintiles = pd.concat([net_yield_owned[['net yields', 'appreciation', 'total returns']], quintiles], axis = 1)\n",
    "print(quintiles)\n",
    "\n",
    "quintile_dict = {}\n",
    "for i in quintiles['quint'].unique():\n",
    "    quintile_dict[i] = [quintiles.loc[quintiles['quint'] == i, 'appreciation'].mean(), \\\n",
    "                        quintiles.loc[quintiles['quint'] == i, 'net yields'].mean(), \\\n",
    "                        quintiles.loc[quintiles['quint'] == i, 'total returns'].mean()]\n",
    "                        \n",
    "    \n",
    "quintile_dict = pd.DataFrame.from_dict(quintile_dict, orient = 'index')\n",
    "\n",
    "quintile_dict.columns = ['appreciation', 'net yield', 'total returns']\n",
    "\n",
    "print('mean')\n",
    "print(quintile_dict.sort_values(by = ['net yield'], ascending = False))\n",
    "\n",
    "quintile_dict = {}\n",
    "for i in quintiles['quint'].unique():\n",
    "    quintile_dict[i] = [quintiles.loc[quintiles['quint'] == i, 'appreciation'].median(), \\\n",
    "                        quintiles.loc[quintiles['quint'] == i, 'net yields'].median(), \\\n",
    "                        quintiles.loc[quintiles['quint'] == i, 'total returns'].median()]\n",
    "                        \n",
    "    \n",
    "quintile_dict = pd.DataFrame.from_dict(quintile_dict, orient = 'index')\n",
    "\n",
    "quintile_dict.columns = ['appreciation', 'net yield', 'total returns']\n",
    "\n",
    "print('median')\n",
    "print(quintile_dict.sort_values(by = ['net yield'], ascending = False))\n",
    "\n",
    "\n",
    "quintile_dict = {}\n",
    "for i in quintiles['quint'].unique():\n",
    "    quintile_dict[i] = [quintiles.loc[quintiles['quint'] == i, 'appreciation'].std(), \\\n",
    "                        quintiles.loc[quintiles['quint'] == i, 'net yields'].std(), \\\n",
    "                        quintiles.loc[quintiles['quint'] == i, 'total returns'].std()]\n",
    "                        \n",
    "    \n",
    "quintile_dict = pd.DataFrame.from_dict(quintile_dict, orient = 'index')\n",
    "\n",
    "quintile_dict.columns = ['appreciation', 'net yield', 'total returns']\n",
    "\n",
    "print('std')\n",
    "print(quintile_dict.sort_values(by = ['net yield'], ascending = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_yield_dict = {}\n",
    "\n",
    "\n",
    "for city in net_yield_owned['CITY'].unique():\n",
    "    net_yield_dict[city] = {}\n",
    "    for y in net_yield_owned['year'].unique():\n",
    "        net_yield_dict[city][y] = net_yield_owned.loc[(net_yield_owned['CITY']== city) & (net_yield_owned['year']==y), 'net yields'].mean()\n",
    "ny_df_pooled = pd.DataFrame.from_dict(net_yield_dict, orient = 'index')\n",
    "\n",
    "print(\"Net Yield By City By Year\")\n",
    "print(pd.DataFrame(ny_df_pooled.dropna()))\n",
    "pd.DataFrame(ny_df_pooled.dropna()).to_csv('net_yields_city_year.csv')\n",
    "csv_r = pd.DataFrame(ny_df_pooled.dropna().mean(axis = 1))\n",
    "print(\"Mean Net Yield By City Pooled Over City\")\n",
    "print(csv_r)\n",
    "print(\"Mean Net Yield Pooled Over Year\")\n",
    "print(pd.DataFrame(ny_df_pooled.dropna().mean(axis = 0)))\n",
    "\n",
    "csv_r.columns = ['net yields']\n",
    "csv_r.to_csv('net_yields_city.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "ax = sns.heatmap(csv_r.sort_values(by = csv_r.columns[0]),annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.heatmap(pd.DataFrame(ny_df_pooled.dropna().mean(axis = 0)),annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.heatmap(pd.DataFrame(ny_df_pooled.dropna()),cmap=\"Reds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geocodes = pd.read_excel('fip_codes.xls',encoding=\"utf-8\",dtype = str)\n",
    "geocodes['FIPS'] = ''\n",
    "geocodes['FIPS'] = geocodes['FIPS State'].map(str) + geocodes['FIPS County'].map(str)\n",
    "geocodes['County Name'] = geocodes['County Name'].map(lambda x: x.replace('St ', 'St. '))\n",
    "print(geocodes.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = get_pooled_data(net_yield_owned, 'net yields')\n",
    "print(df)\n",
    "\n",
    "print(' ')\n",
    "print(\"Net Yield City Level\")\n",
    "df = pd.DataFrame(df.mean(axis = 1))\n",
    "df.columns = ['net yield' ]\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Net Yields "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "import json\n",
    "with urlopen('https://raw.githubusercontent.com/plotly/datasets/master/geojson-counties-fips.json') as response:\n",
    "    counties = json.load(response)\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "\n",
    "mapping = pd.read_csv('city_county_mapping.csv', index_col = 0)\n",
    "df = pd.concat([df, mapping], axis = 1)\n",
    "\n",
    "for ix, row in df.iterrows():\n",
    "    state = net_yield_owned.loc[net_yield_owned.CITY == ix, 'state'].unique()[0]\n",
    "    if state == 'DC':\n",
    "        state = 'District of Columbia'\n",
    "    df.loc[ix,'FIPS'] = geocodes.loc[(row.county.strip() == geocodes['County Name']) & (state == geocodes['State'])]['FIPS'].values[0]\n",
    "\n",
    "\n",
    "fig = px.choropleth(df, geojson=counties, locations='FIPS', color='net yield',\n",
    "                           color_continuous_scale=\"Rainbow\",\n",
    "                           range_color=(df['net yield'].min()-.01,df['net yield'].max()+.01),\n",
    "                           scope=\"usa\"\n",
    "                          )\n",
    "fig.update_geos(fitbounds=\"locations\", visible=True)\n",
    "fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appreciation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_df_pooled = get_pooled_data(net_yield_owned, 'appreciation')\n",
    "print(app_df_pooled)\n",
    "\n",
    "print(' ')\n",
    "print(\"Appreciation City Level\")\n",
    "app_df_pooled = pd.DataFrame(app_df_pooled.mean(axis = 1))\n",
    "app_df_pooled.columns = ['appreciation' ]\n",
    "print(app_df_pooled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = pd.read_csv('city_county_mapping.csv', index_col = 0)\n",
    "app_df_pooled = pd.concat([app_df_pooled, mapping], axis = 1)\n",
    "\n",
    "for ix, row in app_df_pooled.iterrows():\n",
    "    state = net_yield_owned.loc[net_yield_owned.CITY == ix, 'state'].unique()[0]\n",
    "    if state == 'DC':\n",
    "        state = 'District of Columbia'\n",
    "    app_df_pooled.loc[ix,'FIPS'] = geocodes.loc[(row.county.strip() == geocodes['County Name']) & (state == geocodes['State'])]['FIPS'].values[0]\n",
    "\n",
    "fig = px.choropleth(app_df_pooled, geojson=counties, locations='FIPS', color='appreciation',\n",
    "                           color_continuous_scale=\"Rainbow\",\n",
    "                           range_color=(app_df_pooled['appreciation'].min()-.01,app_df_pooled['appreciation'].max()+.01),\n",
    "                           scope=\"usa\"\n",
    "                          )\n",
    "fig.update_geos(fitbounds=\"locations\", visible=True)\n",
    "fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Net Yields 2007-2009"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot_city = pd.read_csv('net_yields_2007_county.csv')\n",
    "plot_city.columns = ['city','net yield','county','fips_manual']\n",
    "print(plot_city.head(10))\n",
    "plot_city['FIPS'] = ''\n",
    "\n",
    "for ix, row in plot_city.iterrows():\n",
    "    state = net_yield_owned.loc[net_yield_owned.CITY == row.city, 'state'].unique()[0]\n",
    "    if state == 'DC':\n",
    "        state = 'District of Columbia'\n",
    "    plot_city.loc[plot_city.index == ix,'FIPS'] = geocodes.loc[(row.county.strip() == geocodes['County Name']) & (state == geocodes['State'])]['FIPS'].values[0]\n",
    "\n",
    "fig = px.choropleth(plot_city, geojson=counties, locations='FIPS', color='net yield',\n",
    "                           color_continuous_scale=\"Rainbow\",\n",
    "                           range_color=(plot_city['net yield'].min()-.01,plot_city['net yield'].max()+.01),\n",
    "                           scope=\"usa\"\n",
    "                          )\n",
    "fig.update_geos(fitbounds=\"locations\", visible=True)\n",
    "fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total Returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "total_returns_df_pooled = get_pooled_data(net_yield_owned, 'total returns')\n",
    "print(total_returns_df_pooled)\n",
    "\n",
    "print(\"Total Returns House Level\")\n",
    "city_total_returns = pd.DataFrame(total_returns_df_pooled.mean(axis = 1))\n",
    "city_total_returns.columns = ['total returns' ]\n",
    "print(city_total_returns)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mapping = pd.read_csv('city_county_mapping.csv', index_col = 0)\n",
    "city_total_returns = pd.concat([city_total_returns, mapping], axis = 1)\n",
    "\n",
    "for ix, row in city_total_returns.iterrows():\n",
    "    state = net_yield_owned.loc[net_yield_owned.CITY == ix, 'state'].unique()[0]\n",
    "    if state == 'DC':\n",
    "        state = 'District of Columbia'\n",
    "    city_total_returns.loc[ix,'FIPS'] = geocodes.loc[(row.county.strip() == geocodes['County Name']) & (state == geocodes['State'])]['FIPS'].values[0]\n",
    "\n",
    "fig = px.choropleth(city_total_returns, geojson=counties, locations='FIPS', color='total returns',\n",
    "                           color_continuous_scale=\"Rainbow\",\n",
    "                           range_color=(city_total_returns['total returns'].min()-.01,city_total_returns['total returns'].max()+.01),\n",
    "                           scope=\"usa\"\n",
    "                          )\n",
    "fig.update_geos(fitbounds=\"locations\", visible=True)\n",
    "fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"table3.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replicaction of Table 2 p. 44 of pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table2 = pd.concat([city_total_returns,app_df_pooled,df ], axis = 1)[['total returns', 'net yield', 'appreciation']]\n",
    "\n",
    "table2 = table2.sort_values(by = ['total returns'], ascending = False)\n",
    "print(table2)\n",
    "\n",
    "print('rankings')\n",
    "ranking = table2.rank(axis = 0, ascending = False)\n",
    "ranking['total returns'] = table2['total returns']\n",
    "print(ranking)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replicaction of Table 3 p. 44 of pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_pooled_data(net_yield_owned, 'net yields')\n",
    "table3 = table2.copy()\n",
    "std = pd.DataFrame(total_returns_df_pooled.std(axis = 1)) * 100.0\n",
    "std.columns = ['Std Total Returns']\n",
    "std_ny = pd.DataFrame(df.std(axis = 1))* 100.0\n",
    "std_ny.columns = ['Std Net Yields']\n",
    "\n",
    "sharpe = pd.DataFrame(total_returns_df_pooled.mean(axis = 1) / total_returns_df_pooled.std(axis = 1))\n",
    "sharpe.columns= ['Sharpe Total Returns']\n",
    "sharpe_ny = pd.DataFrame(df.mean(axis = 1) / df.std(axis = 1))\n",
    "sharpe_ny.columns= ['Sharpe Net Yields']\n",
    "\n",
    "table3 = pd.concat([table3, sharpe, std, sharpe_ny, std_ny], axis = 1)\n",
    "table3['% of Total Return from Net Yield'] = (table3['net yield'] / table3['total returns'])*100.0\n",
    "\n",
    "print(table3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applications to AssetPricing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(total_returns_df_pooled.head(5))\n",
    "\n",
    "equal_weighted = total_returns_df_pooled.mean(axis = 0)\n",
    "equal_weighted = pd.DataFrame(equal_weighted)\n",
    "equal_weighted.columns = ['equal-weighted']\n",
    "equal_weighted.index = pd.to_datetime(equal_weighted.index, format= '%Y')\n",
    "equal_weighted.cumsum().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factors = pd.read_csv(os.path.join(working_directory,r'F-F_Research_Data_Factors.CSV'), skiprows = 3, index_col=0)\n",
    "factors = factors.dropna(how = 'any', axis = 0)\n",
    "factors = factors.apply(lambda x: x/ 100)\n",
    "factors.index = pd.to_datetime(factors.index, format= '%Y')\n",
    "print(factors.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "asset_pricing = pd.concat([equal_weighted, factors], axis = 1)\n",
    "asset_pricing = asset_pricing.loc[(asset_pricing.index >= equal_weighted.index[0]) & (asset_pricing.index <= equal_weighted.index[-1]), :]\n",
    "#forward fill\n",
    "asset_pricing = asset_pricing.fillna(method='ffill')\n",
    "\n",
    "Y = asset_pricing['equal-weighted']\n",
    "X = asset_pricing[['Mkt-RF',    'SMB',    'HML',    'RF']]\n",
    "X = sm.add_constant(X)\n",
    "model = sm.OLS(Y,X)\n",
    "results = model.fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### multi bandit optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "state_returns = total_returns_df_pooled.T\n",
    "print(state_returns.head(5))\n",
    "\n",
    "epsilon = .25\n",
    "backtest = {}\n",
    "Q = {}\n",
    "k = {}\n",
    "\n",
    "symbols = list(state_returns.columns)\n",
    "\n",
    "count = 0\n",
    "for ix, row in state_returns.iterrows():\n",
    "    if count >= 1:\n",
    "        sortedQ = sorted(Q.items(), key=lambda kv: kv[1], reverse = True)\n",
    "        if np.random.uniform() >= epsilon:\n",
    "            k[sortedQ[0][0]] = k[sortedQ[0][0]] + 1\n",
    "            Q[sortedQ[0][0]] = Q[sortedQ[0][0]] + (1./k[sortedQ[0][0]])*(row[sortedQ[0][0]] - Q[sortedQ[0][0]])\n",
    "            backtest[ix] = row[sortedQ[0][0]]\n",
    "            \n",
    "        else:\n",
    "            other_choices = symbols.copy()\n",
    "            other_choices.remove(sortedQ[0][0])\n",
    "            random_choice = random.choice(other_choices)\n",
    "            k[random_choice] = k[random_choice] + 1\n",
    "            Q[random_choice] = Q[random_choice] + (1./k[random_choice])*(row[random_choice] - Q[random_choice])\n",
    "            backtest[ix] = row[random_choice]\n",
    "        \n",
    "    else:\n",
    "        for i in state_returns.columns:\n",
    "            Q[i] = row[i]\n",
    "            k[i] = 1.\n",
    "            \n",
    "    count = count + 1\n",
    "    \n",
    "backtest = pd.DataFrame.from_dict(backtest, orient = 'index')\n",
    "backtest = pd.DataFrame(backtest)\n",
    "backtest.columns = ['epsilon greedy']\n",
    "\n",
    "cum_backtest = backtest.cumsum()\n",
    "cum_backtest.plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "equal_weighted = total_returns_df_pooled.mean(axis = 0)\n",
    "equal_weighted = pd.DataFrame(equal_weighted)\n",
    "\n",
    "ix_val = cum_backtest.index.values\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(ix_val , equal_weighted.cumsum().loc[ix_val,:], label = 'equal-weighted')\n",
    "plt.plot(ix_val , cum_backtest.loc[ix_val,:], label = 'epsilon greedy portfolios')\n",
    "\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Test')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
