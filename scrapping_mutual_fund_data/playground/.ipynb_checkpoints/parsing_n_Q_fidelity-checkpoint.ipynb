{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from lxml import html\n",
    "import requests\n",
    "import re\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import Request, urlopen\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input the CIK list for the fund family you are looking at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fund_name = 'fidelity'\n",
    "\n",
    "\n",
    "CIK_LIST = ['24238','35331','35315', '722574', '225322', '795422', '35315', '803013', '729218', '205323', '1303459', '751199', \\\n",
    "            '81205', '320351', '354046', '1401097', '35341', '754510', '35348', '275309', '819118', '880195', \\\n",
    "            '1364924', '1061130', '744822', '278001', '719451', '225323', '880709', '708191', '917286', '61397']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get the current data working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data directory\n",
      "D:\\wei\\wei_mutual_fund_project\\data\n",
      "output directory\n",
      "D:\\wei\\wei_mutual_fund_project\\output\\working\n"
     ]
    }
   ],
   "source": [
    "cur_wd = os.getcwd()\n",
    "if 'trunk' in cur_wd:\n",
    "    output_directory = os.path.join(\"\\\\\".join(cur_wd.split('\\\\')[0:-1]),'output')\n",
    "else:\n",
    "    output_directory = os.path.join(\"\\\\\".join(cur_wd.split('\\\\')[0:-1]),'output','working')\n",
    "    \n",
    "cur_wd = os.path.join(\"\\\\\".join(cur_wd.split('\\\\')[0:-1]),'data')\n",
    "print('data directory')\n",
    "print(cur_wd)\n",
    "print('output directory')\n",
    "print(output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions for Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cleaned(lines):\n",
    "    cleaned = []\n",
    "    for i in lines:\n",
    "        #gets rid of tags\n",
    "        cleaned_i = re.sub('<[^>]*>', ' ', i)\n",
    "        #gets rid of &nbsp; and replaces with space\n",
    "        cleaned_i = re.sub('&nbsp;', ' ', cleaned_i)\n",
    "        #re.sub(\"<.*?>\",\"\",st)\n",
    "        if not re.match(r'^\\s*$', cleaned_i):\n",
    "            cleaned.append(cleaned_i)\n",
    "    return cleaned\n",
    "\n",
    "def find(s, ch):\n",
    "    return [i for i, ltr in enumerate(s) if ltr == ch]\n",
    "\n",
    "\n",
    "def hasNumbers(inputString):\n",
    "    return bool(re.search(r'\\d', inputString))\n",
    "\n",
    "\n",
    "def remove_junk(addline):\n",
    "    addline = re.sub('<[^>]*>', ' ', addline)\n",
    "    addline = re.sub('&nbsp;', ' ', addline)\n",
    "    addline = re.sub('&#174;', ' ', addline)\n",
    "    addline = re.sub('&#8480;', ' ', addline)\n",
    "    addline = re.sub(r'&reg;',' ',addline)\n",
    "    addline = re.sub(r'\\n', ' ', addline)\n",
    "    addline = re.sub(r'\\t', ' ', addline)\n",
    "    return addline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "weblink = {}\n",
    "for CIK in CIK_LIST:\n",
    "    weblink[CIK]  = []\n",
    "    txt_files = os.listdir(os.path.join(cur_wd, CIK, 'n-q'))\n",
    "    for i in txt_files:\n",
    "        weblink[CIK].append(r\"https://www.sec.gov/Archives/edgar/data/\" + str(CIK)+r\"/\"  + i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Panel 2 Information- This should be same for every file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get panel 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "panel2 = pd.DataFrame()\n",
    "error_panel2 = []\n",
    "\n",
    "    \n",
    "for CIK in CIK_LIST:\n",
    "    txt_files = os.listdir(os.path.join(cur_wd, CIK, 'n-q'))\n",
    "    \n",
    "    for i in txt_files:\n",
    "        if '.csv' not in i:\n",
    "            try:\n",
    "                series = {}\n",
    "                series_keys = ['SERIES-ID', 'OWNER-CIK', 'SERIES-NAME']\n",
    "                for s in series_keys:\n",
    "                    series[s] = []\n",
    "\n",
    "                contract = {}\n",
    "                contract_keys = ['CLASS-CONTRACT-ID','CLASS-CONTRACT-NAME','CLASS-CONTRACT-TICKER-SYMBOL', 'LAST-READ-SERIES']\n",
    "                for c in contract_keys:\n",
    "                    contract[c] = []\n",
    "\n",
    "                uncleaned_file = open(os.path.join(cur_wd, CIK, 'n-q',i), mode='r') \n",
    "                for u in uncleaned_file:\n",
    "                    \n",
    "\n",
    "                    if 'COMPANY CONFORMED NAME' in u:\n",
    "                        conformed_name = u.replace('COMPANY CONFORMED NAME:', '').strip()\n",
    "\n",
    "                    if 'FILED AS OF DATE:' in u:\n",
    "                        filed_date = str(u).replace('FILED AS OF DATE:','').strip()\n",
    "\n",
    "                    #series information\n",
    "                    for s in series_keys:\n",
    "                        if s in u:\n",
    "                                series[s].append(str(u).replace('<' + s + '>', \"\").strip())\n",
    "                    for c in contract_keys:\n",
    "                        if c != 'LAST-READ-SERIES':\n",
    "                            if c in u:\n",
    "                                contract[c].append(str(u).replace('<' + c + '>', \"\").strip())\n",
    "                                if c == 'CLASS-CONTRACT-ID':\n",
    "                                    contract['LAST-READ-SERIES'].append(series['SERIES-ID'][-1])\n",
    "\n",
    "\n",
    "                series = pd.DataFrame.from_dict(series, orient = 'index').T\n",
    "                contract = pd.DataFrame.from_dict(contract, orient = 'index').T\n",
    "\n",
    "\n",
    "                for s in series_keys:\n",
    "                    contract.loc[:, s] = ''\n",
    "\n",
    "                for ix,row in contract.iterrows():\n",
    "                    for s in series_keys:\n",
    "                        contract.loc[contract.index == ix, s] = series.loc[series['SERIES-ID'] == row['LAST-READ-SERIES'],s].values[0]\n",
    "                contract.loc[:,'file_read'] = i\n",
    "                contract.loc[:,'date_filed'] = filed_date\n",
    "                contract.loc[:, 'company conformed name'] = conformed_name\n",
    "                contract = contract.drop(['LAST-READ-SERIES'], axis=1)\n",
    "\n",
    "                if panel2.empty:\n",
    "                    panel2 = contract.copy()\n",
    "                else:\n",
    "                    panel2 = pd.concat([panel2, contract], axis = 0)\n",
    "            except ValueError:\n",
    "                error_panel2.append(i)\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get files that did not have panel 2 data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(error_panel2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output Panel 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(panel2)\n",
    "panel2.to_csv(os.path.join(output_directory, fund_name + '_panel2.csv'), sep = ',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Cleaning on One File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info_dict(i):\n",
    "    \n",
    "    cf = open(os.path.join(cur_wd, CIK, 'n-q',i), mode='r') \n",
    "    tables = {}\n",
    "    r_add = False\n",
    "    add = False\n",
    "    split_qtly = {}\n",
    "    \n",
    "    hold = False\n",
    "    legendFound = False\n",
    "    acqFound = False\n",
    "    getNexyLine = False\n",
    "    \n",
    "    value_mult = ''\n",
    "    \n",
    "    reporting_date = 'not found'\n",
    "    \n",
    "    for j in cf:\n",
    "        j = j.lower()\n",
    "        \n",
    "        if reporting_date is 'not found':\n",
    "            \n",
    "            if 'date of reporting period' in j and r'<div' in j:\n",
    "                r_addstring = remove_junk(j)\n",
    "                date = re.search(\"(jan(uary)?|feb(ruary)?|mar(ch)?|apr(il)?|may|jun(e)?|jul(y)?|aug(ust)?|sep(tember)?|oct(ober)?|nov(ember)?|dec(ember)?)\\s+\\d{1,2},\\s+\\d{4}\",r_addstring)\n",
    "                if date is not None:\n",
    "                    reporting_date = date.group()\n",
    "                \n",
    "            \n",
    "            if r'<tr>' in j and r'</tr>' not in j:\n",
    "                r_addstring = ''\n",
    "                r_add = True\n",
    "                \n",
    "            try:\n",
    "                if r'</tr>' in j and r'<tr>' not in j:\n",
    "                    r_addstring = \" \".join(r_addstring.split())\n",
    "                    r_add = False\n",
    "                    if 'date of reporting period' in r_addstring:\n",
    "                        date = re.search(\"(jan(uary)?|feb(ruary)?|mar(ch)?|apr(il)?|may|jun(e)?|jul(y)?|aug(ust)?|sep(tember)?|oct(ober)?|nov(ember)?|dec(ember)?)\\s+\\d{1,2},\\s+\\d{4}\",r_addstring)\n",
    "                        if date is not None:\n",
    "                            reporting_date = date.group()\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            \n",
    "            if r_add:\n",
    "                r_addline = remove_junk(j)\n",
    "                r_addstring = r_addstring + ' ' + r_addline\n",
    "                \n",
    "            \n",
    "        \n",
    "        \n",
    "        if r'value (' in j:\n",
    "            try:\n",
    "                value_mult = remove_junk(j)\n",
    "                value_mult = re.findall('\\((.*?)\\)',value_mult)\n",
    "                split_qtly[key]['holdings'].append(value_mult[0])\n",
    "            except IndexError:\n",
    "                #this is a value we do not want to scrap\n",
    "                pass\n",
    "            \n",
    "        if 'quarterly holdings' in j:\n",
    "            getNexyLine = True\n",
    "            \n",
    "        if getNexyLine and 'quarterly holdings' not in j:\n",
    "            if remove_junk(j) != '' and ('fidelity' in j or 'fund' in j or 'index' in j):\n",
    "                fund_quarterly_name = remove_junk(j)\n",
    "                fund_quarterly_name = \" \".join(fund_quarterly_name.split())\n",
    "                getNexyLine = False\n",
    "            \n",
    "        if '-qtly-' in j:\n",
    "            key = remove_junk(j)\n",
    "            split_qtly[key] = {}\n",
    "            split_qtly[key]['holdings'] = []\n",
    "            split_qtly[key]['legend'] = []\n",
    "            split_qtly[key]['acq'] = []\n",
    "            split_qtly[key]['name'] = fund_quarterly_name\n",
    "            \n",
    "            hold = True\n",
    "            legendFound = False\n",
    "            acqFound = False\n",
    "            \n",
    "        \n",
    "            \n",
    "        if r'>legend<' in j:\n",
    "            hold = False\n",
    "            legendFound = True\n",
    "            acqFound = False\n",
    "\n",
    "        if 'acquisition cost' in j:\n",
    "            hold = False\n",
    "            legendFound = False\n",
    "            acqFound = True\n",
    "        \n",
    "        if hold or legendFound or acqFound:\n",
    "            \n",
    "            if r'<tr>' in j:\n",
    "                addstring = ''\n",
    "                add = True\n",
    "\n",
    "            try:\n",
    "                if r'</tr>' in j:\n",
    "                    addstring = \" \".join(addstring.split())\n",
    "                    add = False\n",
    "                    if addstring != '':\n",
    "                        if hold:\n",
    "                            split_qtly[key]['holdings'].append(addstring)\n",
    "                        if legendFound and addstring not in split_qtly[key]['legend']:\n",
    "                            split_qtly[key]['legend'].append(addstring)\n",
    "                        if acqFound and addstring not in split_qtly[key]['acq']:\n",
    "                            split_qtly[key]['acq'].append(addstring)\n",
    "                            \n",
    "                        if 'date of reporting period' in addstring:\n",
    "                            print(addstring)\n",
    "            except:\n",
    "                pass\n",
    "                        \n",
    "                        \n",
    "\n",
    "            if add:\n",
    "                addline = remove_junk(j)\n",
    "                addstring = addstring + ' ' + addline\n",
    "                \n",
    "\n",
    "            if legendFound and r'<p' in j:\n",
    "                try:\n",
    "                    add_leg = remove_junk(j)\n",
    "                    if add_leg != '' :\n",
    "                        if add_leg not in split_qtly[key]['legend']:\n",
    "                            split_qtly[key]['legend'].append(add_leg)\n",
    "                except UnboundLocalError:\n",
    "                    pass\n",
    "                    #the key has not been found yet\n",
    "                \n",
    "            if acqFound and r'<td' in j:\n",
    "                add_acq = remove_junk(j)\n",
    "                if add_acq != '':\n",
    "                    if add_acq not in split_qtly[key]['acq']:\n",
    "                        split_qtly[key]['acq'].append(add_acq)\n",
    "                        \n",
    "    return split_qtly, reporting_date\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CIK  ='24238'\n",
    "i = '0000024238-04-000014.txt'\n",
    "text = i\n",
    "fundNames = panel2.loc[panel2['file_read'] == text, 'SERIES-NAME'].unique()\n",
    "\n",
    "series_names = panel2.loc[panel2['file_read'] == text, 'SERIES-NAME'].unique().tolist()\n",
    "series_names = list(map(lambda x:x.lower(),series_names))\n",
    "\n",
    "if '.csv' not in i:\n",
    "    for z in weblink[CIK]:\n",
    "        if i in z:\n",
    "            matching_link = z\n",
    "    print(matching_link)\n",
    "    \n",
    "    split_qtly, reporting_date = get_info_dict(i)\n",
    "    key = list(split_qtly.keys())[0]\n",
    "    print('reporting date')\n",
    "    print(reporting_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series_names = panel2.loc[panel2['file_read'] == text, 'SERIES-NAME'].unique().tolist()\n",
    "series_names = list(map(lambda x:x.lower(),series_names))\n",
    "series_names = list(map(lambda x: \"\".join(x.split()),series_names))\n",
    "\n",
    "print(series_names)\n",
    "\n",
    "print(panel2.loc[panel2['file_read'] == text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clean the legend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(split_qtly[key]['legend'])\n",
    "\n",
    "leg_dict = {}\n",
    "\n",
    "for j in split_qtly.keys():\n",
    "    count = 0\n",
    "    split_qtly[j]['cleaned legend'] = []\n",
    "    leg_dict[j]= {}\n",
    "    \n",
    "    restricted_securities = []\n",
    "    for i in split_qtly[j]['legend']:\n",
    "        i = i.rstrip().lstrip()\n",
    "        keys = re.search('\\(([^()]*)\\)',i)\n",
    "        try: \n",
    "            keys = keys.group()\n",
    "            if r'(' in i and r')' in i and len(keys) == 3:\n",
    "                split_qtly[j]['cleaned legend'].append(i.rstrip().lstrip()) \n",
    "                if i.rstrip().lstrip()[0:3] not in leg_dict[j].keys():\n",
    "                    print(i)\n",
    "                    leg_dict[j][i.rstrip().lstrip()[0:3]] = i.rstrip().lstrip()[3:]\n",
    "        except:\n",
    "            pass\n",
    "        count = count + 1\n",
    "        \n",
    "df_legend = pd.DataFrame()\n",
    "\n",
    "for j in leg_dict.keys():\n",
    "    try:\n",
    "        add = pd.DataFrame.from_dict(leg_dict[j], orient = 'index')\n",
    "        add.columns = ['code']\n",
    "        add.loc[:, 'identifer'] = j\n",
    "        add.loc[:, 'weblink'] = matching_link\n",
    "        add.loc[:, 'textfile'] = text\n",
    "\n",
    "        if df_legend.empty:\n",
    "            df_legend = add.copy()\n",
    "        else:\n",
    "            df_legend = pd.concat([df_legend, add], axis = 0)\n",
    "    except ValueError:\n",
    "        #there is no legend\n",
    "        pass\n",
    "        \n",
    "print(df_legend)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clean the acquisition data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(split_qtly[key]['acq'])\n",
    "acq_dict = {}\n",
    "\n",
    "for j in split_qtly.keys():\n",
    "    split_qtly[j]['cleaned acq'] = []\n",
    "    for i in split_qtly[j]['acq']:\n",
    "        if hasNumbers(i) and r'$' in str(i):\n",
    "            clean_i = str(i).replace(r'$', r' $')\n",
    "            if clean_i.count(r\"$\") == 1 and clean_i.count(r\"/\") >= 2:\n",
    "                clean_i = clean_i.split(r' $')\n",
    "                cost = clean_i[1]\n",
    "                date = re.findall('\\d{1,2}/\\d{1,2}/\\d{2,4}', clean_i[0])\n",
    "                date = \" - \".join(date)\n",
    "                clean_i[0] = clean_i[0].replace(date, '')\n",
    "                clean_i[0] = clean_i[0].lstrip()\n",
    "                clean_i[0] = clean_i[0].rstrip()\n",
    "                if clean_i[0] != 'equities':\n",
    "                    split_qtly[j]['cleaned acq'].append([clean_i[0], date, cost])\n",
    "    acq_dict[j] = split_qtly[j]['cleaned acq']\n",
    "    \n",
    "\n",
    "df_acq = pd.DataFrame()\n",
    "for j in acq_dict.keys():\n",
    "    if len(acq_dict[j]) > 0:\n",
    "        add = pd.DataFrame(acq_dict[j])\n",
    "        add.columns = ['name', 'acq date', 'acq cost']\n",
    "        add.loc[:, 'identifer'] = j\n",
    "        add.loc[:, 'weblink'] = matching_link\n",
    "        add.loc[:, 'textfile'] = text\n",
    "\n",
    "        if df_acq.empty:\n",
    "            df_acq = add.copy()\n",
    "        else:\n",
    "            df_acq = pd.concat([df_acq, add], axis = 0)\n",
    "        \n",
    "\n",
    "print(df_acq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get the holdings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keys(leg_dict, j, i):\n",
    "    legend_keys = list(leg_dict[j].keys())\n",
    "    keys_to_add = ''\n",
    "\n",
    "    for k in legend_keys:\n",
    "        if k in i:\n",
    "            #i = i.replace(k,'')\n",
    "            if keys_to_add == '':\n",
    "                keys_to_add = k\n",
    "            else:\n",
    "                keys_to_add = keys_to_add + ',' + k\n",
    "\n",
    "    return keys_to_add, i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    len(split_qtly[key]['holdings'])\n",
    "    holdings_dict = {}\n",
    "    for j in split_qtly.keys():\n",
    "        \n",
    "        split_qtly[j]['cleaned holdings'] = []\n",
    "        for i in split_qtly[j]['holdings']:\n",
    "\n",
    "            if hasNumbers(i) and i[-1] != r'%' and 'total investment' not in i and 'principalamount' not in i:\n",
    "                keys_found, cleaned_i = get_keys(leg_dict, j, i)\n",
    "                cleaned_i  = cleaned_i.replace(r'$', ' ')\n",
    "                cleaned_i  = cleaned_i.replace(r',', '')\n",
    "                cleaned_i = cleaned_i.lstrip()\n",
    "                cleaned_i = cleaned_i.rstrip()\n",
    "                cleaned_i = \" \".join(cleaned_i.split())\n",
    "                isplit = cleaned_i.split(' ')\n",
    "                if len(isplit) >= 3:\n",
    "                    if hasNumbers(isplit[-1]) and hasNumbers(isplit[-2]):\n",
    "                        if 'warrant' in cleaned_i or 'loan' in cleaned_i or 'lending' in cleaned_i or 'tranche' in \\\n",
    "                        cleaned_i or 'cash central' in cleaned_i :\n",
    "                            if 'warrant' in cleaned_i:\n",
    "                                split_qtly[j]['cleaned holdings'].append([i, 'warrant', ''])\n",
    "                            elif 'loan' in cleaned_i:\n",
    "                                split_qtly[j]['cleaned holdings'].append([i, 'loan', ''])\n",
    "                            elif 'trance' in cleaned_i:\n",
    "                                split_qtly[j]['cleaned holdings'].append([i, 'pooled-security', ''])\n",
    "                            else:\n",
    "                                split_qtly[j]['cleaned holdings'].append([i, 'non-equity', '',keys_found])\n",
    "                            \n",
    "                        else:\n",
    "                            non_dig = re.findall(r'\\D+', isplit[-2])\n",
    "                            non_dig = \" \".join(non_dig)\n",
    "                            isplit[-3] = isplit[-3] +' ' + non_dig\n",
    "                            isplit[-2] = isplit[-2].replace(non_dig, '')\n",
    "                            name = ' '.join(isplit[0:-2])\n",
    "                            name = name.lstrip()\n",
    "                            name = name.rstrip()\n",
    "                            split_qtly[j]['cleaned holdings'].append([name , isplit[-2], isplit[-1], keys_found])\n",
    "            elif ':' in i:\n",
    "                keys_found, cleaned_i = get_keys(leg_dict, j, i)\n",
    "                cleaned_i  = cleaned_i.replace(r'$', ' ')\n",
    "                cleaned_i  = cleaned_i.replace(r',', '')\n",
    "                cleaned_i = cleaned_i.lstrip()\n",
    "                cleaned_i = cleaned_i.rstrip()\n",
    "                cleaned_i = \" \".join(cleaned_i.split())\n",
    "                split_qtly[j]['cleaned holdings'].append([i, 'header', '',keys_found])\n",
    "                \n",
    "        holdings_dict[j] = split_qtly[j]['cleaned holdings']\n",
    "\n",
    "    df_holdings = pd.DataFrame()\n",
    "    for j in holdings_dict.keys():\n",
    "        add = pd.DataFrame(holdings_dict[j])\n",
    "        add.columns = ['holdings name', 'holdings shares', 'holdings value', 'keys']\n",
    "        add.loc[:, 'identifer'] = j\n",
    "        add.loc[:, 'weblink'] = matching_link\n",
    "        add.loc[:, 'textfile'] = text\n",
    "\n",
    "        if df_holdings.empty:\n",
    "            df_holdings = add.copy()\n",
    "        else:\n",
    "            df_holdings = pd.concat([df_holdings, add], axis = 0)\n",
    "\n",
    "    df_holdings = df_holdings.drop_duplicates(subset = ['holdings name', 'holdings shares', 'holdings value'])\n",
    "    \n",
    "    print(df_holdings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Panel 1 FOR ALL CIKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getPanel1_add(split_qtly, matching_link, text, cik):\n",
    "    \n",
    "    #get the security legend\n",
    "    \n",
    "    leg_dict = {}\n",
    "\n",
    "    for j in split_qtly.keys():\n",
    "        count = 0\n",
    "        split_qtly[j]['cleaned legend'] = []\n",
    "        leg_dict[j]= {}\n",
    "\n",
    "        restricted_securities = []\n",
    "        for i in split_qtly[j]['legend']:\n",
    "            i = i.rstrip().lstrip()\n",
    "            keys = re.search('\\(([^()]*)\\)',i)\n",
    "            try: \n",
    "                keys = keys.group()\n",
    "                if r'(' in i and r')' in i and len(keys) == 3:\n",
    "                    split_qtly[j]['cleaned legend'].append(i.rstrip().lstrip()) \n",
    "                    if i.rstrip().lstrip()[0:3] not in leg_dict[j].keys():\n",
    "                        leg_dict[j][i.rstrip().lstrip()[0:3]] = i.rstrip().lstrip()[3:]\n",
    "            except:\n",
    "                pass\n",
    "            count = count + 1\n",
    "\n",
    "    df_legend = pd.DataFrame()\n",
    "\n",
    "    for j in leg_dict.keys():\n",
    "        try:\n",
    "            add = pd.DataFrame.from_dict(leg_dict[j], orient = 'index')\n",
    "            add.columns = ['code']\n",
    "            add.loc[:, 'identifer'] = j\n",
    "            add.loc[:, 'weblink'] = matching_link\n",
    "            add.loc[:, 'textfile'] = text\n",
    "\n",
    "            if df_legend.empty:\n",
    "                df_legend = add.copy()\n",
    "            else:\n",
    "                df_legend = pd.concat([df_legend, add], axis = 0)\n",
    "        except ValueError:\n",
    "            #there is no legend\n",
    "            pass\n",
    "            \n",
    "            \n",
    "    #get the security acquisition data\n",
    "    \n",
    "    acq_dict = {}\n",
    "\n",
    "    for j in split_qtly.keys():\n",
    "        split_qtly[j]['cleaned acq'] = []\n",
    "        for i in split_qtly[j]['acq']:\n",
    "            if hasNumbers(i) and r'$' in str(i):\n",
    "                clean_i = str(i).replace(r'$', r' $')\n",
    "                if clean_i.count(r\"$\") == 1 and clean_i.count(r\"/\") >= 2:\n",
    "                    clean_i = clean_i.split(r' $')\n",
    "                    cost = clean_i[1].lstrip().rstrip()\n",
    "                    date = re.findall('\\d{1,2}/\\d{1,2}/\\d{2,4}', clean_i[0])\n",
    "                    date = \" - \".join(date)\n",
    "                    date = date.lstrip().rstrip()\n",
    "                    name = clean_i[0].replace(date, '')\n",
    "                    name = name.lstrip()\n",
    "                    name = name.rstrip()\n",
    "                    if name != 'equities':\n",
    "                        split_qtly[j]['cleaned acq'].append([name, date, cost])\n",
    "        acq_dict[j] = split_qtly[j]['cleaned acq']\n",
    "\n",
    "    df_acq = pd.DataFrame()\n",
    "    for j in acq_dict.keys():\n",
    "        if len(acq_dict[j]) > 0:\n",
    "            add = pd.DataFrame(acq_dict[j])\n",
    "            add.columns = ['acq name', 'acq date', 'acq cost']\n",
    "            add.loc[:, 'identifer'] = j\n",
    "            add.loc[:, 'weblink'] = matching_link\n",
    "            add.loc[:, 'textfile'] = text\n",
    "            add.loc[:, 'CIK'] = cik\n",
    "            add.loc[:, 'fund name'] = split_qtly[j]['name']\n",
    "\n",
    "            if df_acq.empty:\n",
    "                df_acq = add.copy()\n",
    "            else:\n",
    "                df_acq = pd.concat([df_acq, add], axis = 0)\n",
    "            \n",
    "    #get the holdings data\n",
    "    \n",
    "    holdings_dict = {}\n",
    "    value_mult = ''\n",
    "    for j in split_qtly.keys():\n",
    "        \n",
    "        leg_dict[j]\n",
    "        split_qtly[j]['cleaned holdings'] = []\n",
    "        for i in split_qtly[j]['holdings']:\n",
    "            \n",
    "            if r'value (' in i:\n",
    "                value_mult = remove_junk(i)\n",
    "                value_mult = str(re.findall('\\((.*?)\\)',value_mult)[0])\n",
    "\n",
    "            elif hasNumbers(i) and i[-1] != r'%' and 'total investment' not in i and 'principalamount' not in i \\\n",
    "            and r'cost)' not in i and r'(cost' not in i and 'net asset' not in i:\n",
    "                keys_found, cleaned_i = get_keys(leg_dict, j, i)\n",
    "                cleaned_i  = cleaned_i.replace(r'$', ' ')\n",
    "                cleaned_i  = cleaned_i.replace(r',', '')\n",
    "                cleaned_i = cleaned_i.lstrip()\n",
    "                cleaned_i = cleaned_i.rstrip()\n",
    "                cleaned_i = \" \".join(cleaned_i.split())\n",
    "                isplit = cleaned_i.split(' ')\n",
    "                if len(isplit) >= 3:\n",
    "                    if hasNumbers(isplit[-1]) and hasNumbers(isplit[-2]):\n",
    "                        if 'warrant' in cleaned_i or 'loan' in cleaned_i or 'lending' in cleaned_i or 'tranche' in \\\n",
    "                        cleaned_i or 'cash central' in cleaned_i or r'%' in cleaned_i:\n",
    "                            if 'warrant' in cleaned_i:\n",
    "                                split_qtly[j]['cleaned holdings'].append([i, 'warrant', '',keys_found, value_mult])\n",
    "                            elif 'loan' in cleaned_i and 'tranche' not in cleaned_i:\n",
    "                                split_qtly[j]['cleaned holdings'].append([i, 'loan', '',keys_found, value_mult])\n",
    "                            elif 'tranche' in cleaned_i:\n",
    "                                split_qtly[j]['cleaned holdings'].append([i, 'pooled-security', '',keys_found, value_mult])\n",
    "                            elif 'cash central' in cleaned_i:\n",
    "                                split_qtly[j]['cleaned holdings'].append([i, 'cash central fund', '',keys_found, value_mult])\n",
    "                            elif r'%' in cleaned_i and r'/' in cleaned_i:\n",
    "                                split_qtly[j]['cleaned holdings'].append([i, 'corporate bond', '',keys_found, value_mult])\n",
    "                            elif r'%' in cleaned_i and r'/' not in cleaned_i:\n",
    "                                split_qtly[j]['cleaned holdings'].append([i, 'pref stock', '',keys_found, value_mult])\n",
    "                            else:\n",
    "                                split_qtly[j]['cleaned holdings'].append([i, 'non-equity', '',keys_found, value_mult])\n",
    "                        else:\n",
    "                            non_dig = re.findall(r'\\D+', isplit[-2])\n",
    "                            non_dig = \" \".join(non_dig)\n",
    "                            isplit[-3] = isplit[-3] +' ' + non_dig\n",
    "                            isplit[-2] = isplit[-2].replace(non_dig, '')\n",
    "                            name = ' '.join(isplit[0:-2])\n",
    "                            name = name.lstrip()\n",
    "                            name = name.rstrip()\n",
    "                            split_qtly[j]['cleaned holdings'].append([name , isplit[-2], isplit[-1],keys_found, value_mult])\n",
    "            elif ':' in i:\n",
    "                if 'total investment' not in i and 'principalamount' not in i \\\n",
    "                and r'cost)' not in i and r'(cost' not in i and 'net asset' not in i:\n",
    "                    keys_found, cleaned_i = get_keys(leg_dict, j, i)\n",
    "                    cleaned_i  = cleaned_i.replace(r'$', ' ')\n",
    "                    cleaned_i  = cleaned_i.replace(r',', '')\n",
    "                    cleaned_i = cleaned_i.lstrip()\n",
    "                    cleaned_i = cleaned_i.rstrip()\n",
    "                    cleaned_i  = cleaned_i.replace(r':', '')\n",
    "                    cleaned_i = \" \".join(cleaned_i.split())\n",
    "                    split_qtly[j]['cleaned holdings'].append([i, 'header', '',keys_found, value_mult])\n",
    "            else:\n",
    "                pass\n",
    "                \n",
    "        holdings_dict[j] = split_qtly[j]['cleaned holdings']\n",
    "\n",
    "    df_holdings = pd.DataFrame()\n",
    "    for j in holdings_dict.keys():\n",
    "        add = pd.DataFrame(holdings_dict[j])\n",
    "        add.columns = ['holdings name', 'holdings shares', 'holdings value', 'key', 'value multiplier']\n",
    "        add.loc[:, 'identifer'] = j\n",
    "        add.loc[:, 'weblink'] = matching_link\n",
    "        add.loc[:, 'textfile'] = text\n",
    "        add.loc[:, 'CIK'] = cik\n",
    "        add.loc[:, 'fund name'] = split_qtly[j]['name']\n",
    "\n",
    "        if df_holdings.empty:\n",
    "            df_holdings = add.copy()\n",
    "        else:\n",
    "            df_holdings = pd.concat([df_holdings, add], axis = 0)\n",
    "\n",
    "    df_holdings = df_holdings.drop_duplicates(subset = ['holdings name', 'holdings shares', 'holdings value'])\n",
    "    df_acq = df_acq.drop_duplicates(subset = ['acq name', 'acq date', 'acq cost'])\n",
    "    \n",
    "    \n",
    "    return df_legend, df_acq, df_holdings\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "panel1 = pd.DataFrame()\n",
    "panel1_legend = pd.DataFrame()\n",
    "error_panel1 = []\n",
    "\n",
    "for CIK in CIK_LIST:\n",
    "    txt_files = os.listdir(os.path.join(cur_wd, CIK, 'n-q'))\n",
    "    \n",
    "    for i in txt_files:\n",
    "        if '.csv' not in i and i not in error_panel2:\n",
    "            for z in weblink[CIK]:\n",
    "                if i in z:\n",
    "                    matching_link = z\n",
    "            print(matching_link)\n",
    "            \n",
    "            try:\n",
    "                split_qtly, reporting_date = get_info_dict(i)\n",
    "                \n",
    "                #get the panel1 data\n",
    "                df_legend, df_acq, df_holdings = getPanel1_add(split_qtly, matching_link, i, CIK)\n",
    "                \n",
    "                #combine the holdings and acq data in one panel\n",
    "                df_holdings = pd.concat([df_holdings, df_acq], axis = 0)\n",
    "                \n",
    "                #add the conformed data and date filed to panel 1 by reading the date_filed\n",
    "                date_filed = panel2.loc[panel2['file_read'] == i, 'date_filed'].unique()[0]\n",
    "                conformed_name = panel2.loc[panel2['file_read'] == i, 'company conformed name'].unique()[0]\n",
    "                \n",
    "                df_holdings.loc[:, 'date_filed'] = date_filed\n",
    "                df_holdings.loc[:, 'reporting_date'] = reporting_date\n",
    "                df_holdings.loc[:, 'company conformed name'] = conformed_name\n",
    "                \n",
    "                #add the new holdings data modified to panel 1 and also add legend information to output panels\n",
    "                if panel1.empty:\n",
    "                    panel1 = df_holdings.copy()\n",
    "                    panel1_legend = df_legend.copy()\n",
    "                else:\n",
    "                    panel1 = pd.concat([panel1, df_holdings] , axis = 0)\n",
    "                    panel1_legend = pd.concat([panel1_legend, df_legend], axis = 0)\n",
    "            except:\n",
    "                print(\"Unexpected error:\", sys.exc_info()[0])\n",
    "                error_panel1.append(i)\n",
    "                print(CIK)\n",
    "                print(i)\n",
    "         \n",
    "print('errors')\n",
    "print(error_panel1)\n",
    "\n",
    "panel1_legend.to_csv(os.path.join(output_directory, fund_name + '_panel1_legend.csv'), sep = ',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "map the header information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "panel1 = panel1.reset_index(drop=True)\n",
    "for index, row in panel1.iterrows():\n",
    "    if row['holdings shares'] == 'header':\n",
    "        lastheader = str(row['holdings name'].replace(r':', ''))\n",
    "    if not pd.isnull(row['holdings name']):\n",
    "        if str(row['holdings name'][0:len('class')]).lower() == 'class':\n",
    "            panel1.loc[panel1.index == index, 'holdings name'] = lastheader + r'_' + str(row['holdings name'])\n",
    "        elif str(row['holdings name'][0:len('series')]).lower() == 'series':\n",
    "            panel1.loc[panel1.index == index, 'holdings name'] = lastheader + r'_' + str(row['holdings name'])\n",
    "        elif str(row['holdings name'][0:len('warrants')]).lower() == 'warrants':\n",
    "            panel1.loc[panel1.index == index, 'holdings name'] = lastheader + r'_' + str(row['holdings name'])\n",
    "        elif str(row['holdings name'][0:len('adr')]).lower() == 'adr':\n",
    "            panel1.loc[panel1.index == index, 'holdings name'] = lastheader + r'_' + str(row['holdings name'])            \n",
    "        elif r'%' in row['holdings name'] and  hasNumbers(row['holdings name'][0]):\n",
    "            panel1.loc[panel1.index == index, 'holdings name'] = lastheader + r'_' + str(row['holdings name'])\n",
    "\n",
    "panel1 = panel1.loc[panel1['holdings shares'] != 'header']  \n",
    "panel1.to_csv(os.path.join(output_directory, fund_name + '_panel1.csv'), sep = ',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get the restricted securities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "panel1_legend.loc[:,'restricted'] = 'no'\n",
    "\n",
    "acq = panel1.loc[~panel1['acq date'].isnull()]\n",
    "\n",
    "panel1_legend.loc[panel1_legend['code'].str.contains(\"restrict\") | panel1_legend['code'].str.contains(\"level 3 security\") \\\n",
    "              | panel1_legend['code'].str.contains(\"exempt from registration\"), 'restricted'] = 'yes'\n",
    "\n",
    "restricted = panel1_legend.loc[panel1_legend['restricted'].str.contains(\"yes\")]\n",
    "restricted.to_csv(os.path.join(output_directory, fund_name + '_panel1_restricted_legend.csv'), sep = ',')\n",
    "\n",
    "panel1.loc[:,'restricted'] = 'no'\n",
    "\n",
    "for index, row in restricted.iterrows():\n",
    "    panel1.loc[(panel1['identifer'] == row['identifer']) & (panel1['textfile'] == row.textfile) \\\n",
    "                   & (panel1['key'].str.contains(index)), 'restricted'] = 'yes'\n",
    "\n",
    "#add the acquisition data\n",
    "\n",
    "print(len(panel1))\n",
    "restricted_panel1 = panel1.loc[(panel1['restricted'] == 'yes')]\n",
    "print(len(restricted_panel1))\n",
    "restricted_panel1 = pd.concat([panel1.loc[(panel1['restricted'] == 'yes')], acq], axis = 0)\n",
    "print(len(restricted_panel1))\n",
    "restricted_panel1.to_csv(os.path.join(output_directory, fund_name + '_panel1_restricted.csv'), sep = ',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
