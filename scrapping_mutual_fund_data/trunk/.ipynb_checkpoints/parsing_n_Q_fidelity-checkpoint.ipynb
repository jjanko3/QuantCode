{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from lxml import html\n",
    "import requests\n",
    "import re\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import Request, urlopen\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input the CIK list for the fund family you are looking at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "fund_name = 'fidelity'\n",
    "\n",
    "\n",
    "CIK_LIST = ['24238','35331','35315', '722574', '225322', '795422', '35315', '803013', '729218', '205323', '1303459', '751199', \\\n",
    "            '81205', '320351', '354046', '1401097', '35341', '754510', '35348', '275309', '819118', '880195', \\\n",
    "            '1364924', '1061130', '744822', '278001', '719451', '225323', '880709', '708191', '917286', '61397']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get the current data working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data directory\n",
      "D:\\wei\\wei_mutual_fund_project\\data\n",
      "output directory\n",
      "D:\\wei\\wei_mutual_fund_project\\output\n"
     ]
    }
   ],
   "source": [
    "cur_wd = os.getcwd()\n",
    "if 'trunk' in cur_wd:\n",
    "    output_directory = os.path.join(\"\\\\\".join(cur_wd.split('\\\\')[0:-1]),'output')\n",
    "else:\n",
    "    output_directory = os.path.join(\"\\\\\".join(cur_wd.split('\\\\')[0:-1]),'output','working')\n",
    "    \n",
    "cur_wd = os.path.join(\"\\\\\".join(cur_wd.split('\\\\')[0:-1]),'data')\n",
    "print('data directory')\n",
    "print(cur_wd)\n",
    "print('output directory')\n",
    "print(output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions for Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cleaned(lines):\n",
    "    cleaned = []\n",
    "    for i in lines:\n",
    "        #gets rid of tags\n",
    "        cleaned_i = re.sub('<[^>]*>', ' ', i)\n",
    "        #gets rid of &nbsp; and replaces with space\n",
    "        cleaned_i = re.sub('&nbsp;', ' ', cleaned_i)\n",
    "        #re.sub(\"<.*?>\",\"\",st)\n",
    "        if not re.match(r'^\\s*$', cleaned_i):\n",
    "            cleaned.append(cleaned_i)\n",
    "    return cleaned\n",
    "\n",
    "def find(s, ch):\n",
    "    return [i for i, ltr in enumerate(s) if ltr == ch]\n",
    "\n",
    "\n",
    "def hasNumbers(inputString):\n",
    "    return bool(re.search(r'\\d', inputString))\n",
    "\n",
    "\n",
    "def remove_junk(addline):\n",
    "    addline = re.sub('<[^>]*>', ' ', addline)\n",
    "    addline = re.sub('&nbsp;', ' ', addline)\n",
    "    addline = re.sub('&#174;', ' ', addline)\n",
    "    addline = re.sub('&#8480;', ' ', addline)\n",
    "    addline = re.sub(r'&reg;',' ',addline)\n",
    "    addline = re.sub(r'\\n', ' ', addline)\n",
    "    addline = re.sub(r'\\t', ' ', addline)\n",
    "    return addline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "weblink = {}\n",
    "for CIK in CIK_LIST:\n",
    "    weblink[CIK]  = []\n",
    "    txt_files = os.listdir(os.path.join(cur_wd, CIK, 'n-q'))\n",
    "    for i in txt_files:\n",
    "        weblink[CIK].append(r\"https://www.sec.gov/Archives/edgar/data/\" + str(CIK)+r\"/\"  + i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Panel 2 Information- This should be same for every file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get panel 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "panel2 = pd.DataFrame()\n",
    "error_panel2 = []\n",
    "\n",
    "    \n",
    "for CIK in CIK_LIST:\n",
    "    txt_files = os.listdir(os.path.join(cur_wd, CIK, 'n-q'))\n",
    "    \n",
    "    for i in txt_files:\n",
    "        if '.csv' not in i:\n",
    "            try:\n",
    "                series = {}\n",
    "                series_keys = ['SERIES-ID', 'OWNER-CIK', 'SERIES-NAME']\n",
    "                for s in series_keys:\n",
    "                    series[s] = []\n",
    "\n",
    "                contract = {}\n",
    "                contract_keys = ['CLASS-CONTRACT-ID','CLASS-CONTRACT-NAME','CLASS-CONTRACT-TICKER-SYMBOL', 'LAST-READ-SERIES']\n",
    "                for c in contract_keys:\n",
    "                    contract[c] = []\n",
    "\n",
    "                uncleaned_file = open(os.path.join(cur_wd, CIK, 'n-q',i), mode='r') \n",
    "                for u in uncleaned_file:\n",
    "                    \n",
    "\n",
    "                    if 'COMPANY CONFORMED NAME' in u:\n",
    "                        conformed_name = u.replace('COMPANY CONFORMED NAME:', '').strip()\n",
    "\n",
    "                    if 'FILED AS OF DATE:' in u:\n",
    "                        filed_date = str(u).replace('FILED AS OF DATE:','').strip()\n",
    "\n",
    "                    #series information\n",
    "                    for s in series_keys:\n",
    "                        if s in u:\n",
    "                                series[s].append(str(u).replace('<' + s + '>', \"\").strip())\n",
    "                    for c in contract_keys:\n",
    "                        if c != 'LAST-READ-SERIES':\n",
    "                            if c in u:\n",
    "                                contract[c].append(str(u).replace('<' + c + '>', \"\").strip())\n",
    "                                if c == 'CLASS-CONTRACT-ID':\n",
    "                                    contract['LAST-READ-SERIES'].append(series['SERIES-ID'][-1])\n",
    "\n",
    "\n",
    "                series = pd.DataFrame.from_dict(series, orient = 'index').T\n",
    "                contract = pd.DataFrame.from_dict(contract, orient = 'index').T\n",
    "\n",
    "\n",
    "                for s in series_keys:\n",
    "                    contract.loc[:, s] = ''\n",
    "\n",
    "                for ix,row in contract.iterrows():\n",
    "                    for s in series_keys:\n",
    "                        contract.loc[contract.index == ix, s] = series.loc[series['SERIES-ID'] == row['LAST-READ-SERIES'],s].values[0]\n",
    "                contract.loc[:,'file_read'] = i\n",
    "                contract.loc[:,'date_filed'] = filed_date\n",
    "                contract.loc[:, 'company conformed name'] = conformed_name\n",
    "                contract = contract.drop(['LAST-READ-SERIES'], axis=1)\n",
    "\n",
    "                if panel2.empty:\n",
    "                    panel2 = contract.copy()\n",
    "                else:\n",
    "                    panel2 = pd.concat([panel2, contract], axis = 0)\n",
    "            except ValueError:\n",
    "                error_panel2.append(i)\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get files that did not have panel 2 data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0000024238-04-000014.txt', '0000024238-05-000005.txt', '0000795422-05-000048.txt']\n"
     ]
    }
   ],
   "source": [
    "print(error_panel2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output Panel 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   CLASS-CONTRACT-ID                                CLASS-CONTRACT-NAME  \\\n",
      "0         C000016596                                            Class A   \n",
      "1         C000016597                                            Class B   \n",
      "2         C000016598                                            Class C   \n",
      "3         C000016599                                            Class T   \n",
      "4         C000016600                                Institutional Class   \n",
      "5         C000016601                                Fidelity Contrafund   \n",
      "0         C000016596                                            Class A   \n",
      "1         C000016597                                            Class B   \n",
      "2         C000016598                                            Class C   \n",
      "3         C000016599                                            Class T   \n",
      "4         C000016600                                Institutional Class   \n",
      "5         C000016601                                Fidelity Contrafund   \n",
      "0         C000016596                                            Class A   \n",
      "1         C000016597                                            Class B   \n",
      "2         C000016598                                            Class C   \n",
      "3         C000016599                                            Class T   \n",
      "4         C000016600                                Institutional Class   \n",
      "5         C000016601                                Fidelity Contrafund   \n",
      "0         C000016596                                            Class A   \n",
      "1         C000016597                                            Class B   \n",
      "2         C000016598                                            Class C   \n",
      "3         C000016599                                            Class T   \n",
      "4         C000016600                                Institutional Class   \n",
      "5         C000016601                                Fidelity Contrafund   \n",
      "6         C000064233                                            Class K   \n",
      "7         C000120656        Fidelity Series Opportunistic Insights Fund   \n",
      "8         C000120657                                            Class F   \n",
      "9         C000120658  Fidelity Advisor Series Opportunistic Insights...   \n",
      "0         C000016596                                            Class A   \n",
      "1         C000016597                                            Class B   \n",
      "..               ...                                                ...   \n",
      "3         C000016632   Fidelity Advisor Diversified Stock Fund: Class M   \n",
      "4         C000016633   Fidelity Advisor Diversified Stock Fund: Class I   \n",
      "5         C000130143                                            Class Z   \n",
      "6         C000016635                                            Class O   \n",
      "7         C000016636  Fidelity Advisor Capital Development Fund: Cla...   \n",
      "8         C000016638  Fidelity Advisor Capital Development Fund: Cla...   \n",
      "9         C000016639  Fidelity Advisor Capital Development Fund: Cla...   \n",
      "10        C000016640  Fidelity Advisor Capital Development Fund: Cla...   \n",
      "0         C000016627   Fidelity Advisor Diversified Stock Fund: Class A   \n",
      "1         C000016628                                            Class O   \n",
      "2         C000016631   Fidelity Advisor Diversified Stock Fund: Class C   \n",
      "3         C000016632   Fidelity Advisor Diversified Stock Fund: Class M   \n",
      "4         C000016633   Fidelity Advisor Diversified Stock Fund: Class I   \n",
      "5         C000130143                                            Class Z   \n",
      "6         C000016635                                            Class O   \n",
      "7         C000016636  Fidelity Advisor Capital Development Fund: Cla...   \n",
      "8         C000016638  Fidelity Advisor Capital Development Fund: Cla...   \n",
      "9         C000016639  Fidelity Advisor Capital Development Fund: Cla...   \n",
      "10        C000016640  Fidelity Advisor Capital Development Fund: Cla...   \n",
      "0         C000016627   Fidelity Advisor Diversified Stock Fund: Class A   \n",
      "1         C000016628                                            Class O   \n",
      "2         C000016631   Fidelity Advisor Diversified Stock Fund: Class C   \n",
      "3         C000016632   Fidelity Advisor Diversified Stock Fund: Class M   \n",
      "4         C000016633   Fidelity Advisor Diversified Stock Fund: Class I   \n",
      "5         C000130143                                            Class Z   \n",
      "6         C000016635                                            Class O   \n",
      "7         C000016636  Fidelity Advisor Capital Development Fund: Cla...   \n",
      "8         C000016638  Fidelity Advisor Capital Development Fund: Cla...   \n",
      "9         C000016639  Fidelity Advisor Capital Development Fund: Cla...   \n",
      "10        C000016640  Fidelity Advisor Capital Development Fund: Cla...   \n",
      "\n",
      "   CLASS-CONTRACT-TICKER-SYMBOL   SERIES-ID   OWNER-CIK  \\\n",
      "0                         FNIAX  S000006036  0000024238   \n",
      "1                         FNIBX  S000006036  0000024238   \n",
      "2                         FNICX  S000006036  0000024238   \n",
      "3                         FNITX  S000006036  0000024238   \n",
      "4                         FINSX  S000006036  0000024238   \n",
      "5                         FCNTX  S000006037  0000024238   \n",
      "0                         FNIAX  S000006036  0000024238   \n",
      "1                         FNIBX  S000006036  0000024238   \n",
      "2                         FNICX  S000006036  0000024238   \n",
      "3                         FNITX  S000006036  0000024238   \n",
      "4                         FINSX  S000006036  0000024238   \n",
      "5                         FCNTX  S000006037  0000024238   \n",
      "0                         FNIAX  S000006036  0000024238   \n",
      "1                         FNIBX  S000006036  0000024238   \n",
      "2                         FNICX  S000006036  0000024238   \n",
      "3                         FNITX  S000006036  0000024238   \n",
      "4                         FINSX  S000006036  0000024238   \n",
      "5                         FCNTX  S000006037  0000024238   \n",
      "0                         FNIAX  S000006036  0000024238   \n",
      "1                         FNIBX  S000006036  0000024238   \n",
      "2                         FNICX  S000006036  0000024238   \n",
      "3                         FNITX  S000006036  0000024238   \n",
      "4                         FINSX  S000006036  0000024238   \n",
      "5                         FCNTX  S000006037  0000024238   \n",
      "6                         FCNKX  S000006037  0000024238   \n",
      "7                         FVWSX  S000039220  0000024238   \n",
      "8                         FWWEX  S000039220  0000024238   \n",
      "9                         FAMGX  S000039221  0000024238   \n",
      "0                         FNIAX  S000006036  0000024238   \n",
      "1                         FNIBX  S000006036  0000024238   \n",
      "..                          ...         ...         ...   \n",
      "3                         FDTEX  S000006055  0000035331   \n",
      "4                         FDTIX  S000006055  0000035331   \n",
      "5                         FZACX  S000006055  0000035331   \n",
      "6                         FDETX  S000006056  0000035331   \n",
      "7                         FDTTX  S000006056  0000035331   \n",
      "8                         FDECX  S000006056  0000035331   \n",
      "9                         FDTZX  S000006056  0000035331   \n",
      "10                        FDEIX  S000006056  0000035331   \n",
      "0                         FDTOX  S000006055  0000035331   \n",
      "1                         FDESX  S000006055  0000035331   \n",
      "2                         FDTCX  S000006055  0000035331   \n",
      "3                         FDTEX  S000006055  0000035331   \n",
      "4                         FDTIX  S000006055  0000035331   \n",
      "5                         FZACX  S000006055  0000035331   \n",
      "6                         FDETX  S000006056  0000035331   \n",
      "7                         FDTTX  S000006056  0000035331   \n",
      "8                         FDECX  S000006056  0000035331   \n",
      "9                         FDTZX  S000006056  0000035331   \n",
      "10                        FDEIX  S000006056  0000035331   \n",
      "0                         FDTOX  S000006055  0000035331   \n",
      "1                         FDESX  S000006055  0000035331   \n",
      "2                         FDTCX  S000006055  0000035331   \n",
      "3                         FDTEX  S000006055  0000035331   \n",
      "4                         FDTIX  S000006055  0000035331   \n",
      "5                         FZACX  S000006055  0000035331   \n",
      "6                         FDETX  S000006056  0000035331   \n",
      "7                         FDTTX  S000006056  0000035331   \n",
      "8                         FDECX  S000006056  0000035331   \n",
      "9                         FDTZX  S000006056  0000035331   \n",
      "10                        FDEIX  S000006056  0000035331   \n",
      "\n",
      "                                          SERIES-NAME  \\\n",
      "0                  Fidelity Advsior New Insights Fund   \n",
      "1                  Fidelity Advsior New Insights Fund   \n",
      "2                  Fidelity Advsior New Insights Fund   \n",
      "3                  Fidelity Advsior New Insights Fund   \n",
      "4                  Fidelity Advsior New Insights Fund   \n",
      "5                                 Fidelity Contrafund   \n",
      "0                  Fidelity Advisor New Insights Fund   \n",
      "1                  Fidelity Advisor New Insights Fund   \n",
      "2                  Fidelity Advisor New Insights Fund   \n",
      "3                  Fidelity Advisor New Insights Fund   \n",
      "4                  Fidelity Advisor New Insights Fund   \n",
      "5                                 Fidelity Contrafund   \n",
      "0                  Fidelity Advisor New Insights Fund   \n",
      "1                  Fidelity Advisor New Insights Fund   \n",
      "2                  Fidelity Advisor New Insights Fund   \n",
      "3                  Fidelity Advisor New Insights Fund   \n",
      "4                  Fidelity Advisor New Insights Fund   \n",
      "5                                 Fidelity Contrafund   \n",
      "0                  Fidelity Advisor New Insights Fund   \n",
      "1                  Fidelity Advisor New Insights Fund   \n",
      "2                  Fidelity Advisor New Insights Fund   \n",
      "3                  Fidelity Advisor New Insights Fund   \n",
      "4                  Fidelity Advisor New Insights Fund   \n",
      "5                                 Fidelity Contrafund   \n",
      "6                                 Fidelity Contrafund   \n",
      "7         Fidelity Series Opportunistic Insights Fund   \n",
      "8         Fidelity Series Opportunistic Insights Fund   \n",
      "9   Fidelity Advisor Series Opportunistic Insights...   \n",
      "0                  Fidelity Advisor New Insights Fund   \n",
      "1                  Fidelity Advisor New Insights Fund   \n",
      "..                                                ...   \n",
      "3             Fidelity Advisor Diversified Stock Fund   \n",
      "4             Fidelity Advisor Diversified Stock Fund   \n",
      "5             Fidelity Advisor Diversified Stock Fund   \n",
      "6           Fidelity Advisor Capital Development Fund   \n",
      "7           Fidelity Advisor Capital Development Fund   \n",
      "8           Fidelity Advisor Capital Development Fund   \n",
      "9           Fidelity Advisor Capital Development Fund   \n",
      "10          Fidelity Advisor Capital Development Fund   \n",
      "0             Fidelity Advisor Diversified Stock Fund   \n",
      "1             Fidelity Advisor Diversified Stock Fund   \n",
      "2             Fidelity Advisor Diversified Stock Fund   \n",
      "3             Fidelity Advisor Diversified Stock Fund   \n",
      "4             Fidelity Advisor Diversified Stock Fund   \n",
      "5             Fidelity Advisor Diversified Stock Fund   \n",
      "6           Fidelity Advisor Capital Development Fund   \n",
      "7           Fidelity Advisor Capital Development Fund   \n",
      "8           Fidelity Advisor Capital Development Fund   \n",
      "9           Fidelity Advisor Capital Development Fund   \n",
      "10          Fidelity Advisor Capital Development Fund   \n",
      "0             Fidelity Advisor Diversified Stock Fund   \n",
      "1             Fidelity Advisor Diversified Stock Fund   \n",
      "2             Fidelity Advisor Diversified Stock Fund   \n",
      "3             Fidelity Advisor Diversified Stock Fund   \n",
      "4             Fidelity Advisor Diversified Stock Fund   \n",
      "5             Fidelity Advisor Diversified Stock Fund   \n",
      "6           Fidelity Advisor Capital Development Fund   \n",
      "7           Fidelity Advisor Capital Development Fund   \n",
      "8           Fidelity Advisor Capital Development Fund   \n",
      "9           Fidelity Advisor Capital Development Fund   \n",
      "10          Fidelity Advisor Capital Development Fund   \n",
      "\n",
      "                   file_read date_filed       company conformed name  \n",
      "0   0000024238-06-000007.txt   20060530          FIDELITY CONTRAFUND  \n",
      "1   0000024238-06-000007.txt   20060530          FIDELITY CONTRAFUND  \n",
      "2   0000024238-06-000007.txt   20060530          FIDELITY CONTRAFUND  \n",
      "3   0000024238-06-000007.txt   20060530          FIDELITY CONTRAFUND  \n",
      "4   0000024238-06-000007.txt   20060530          FIDELITY CONTRAFUND  \n",
      "5   0000024238-06-000007.txt   20060530          FIDELITY CONTRAFUND  \n",
      "0   0000024238-07-000011.txt   20070530          FIDELITY CONTRAFUND  \n",
      "1   0000024238-07-000011.txt   20070530          FIDELITY CONTRAFUND  \n",
      "2   0000024238-07-000011.txt   20070530          FIDELITY CONTRAFUND  \n",
      "3   0000024238-07-000011.txt   20070530          FIDELITY CONTRAFUND  \n",
      "4   0000024238-07-000011.txt   20070530          FIDELITY CONTRAFUND  \n",
      "5   0000024238-07-000011.txt   20070530          FIDELITY CONTRAFUND  \n",
      "0   0000024238-07-000024.txt   20071129          FIDELITY CONTRAFUND  \n",
      "1   0000024238-07-000024.txt   20071129          FIDELITY CONTRAFUND  \n",
      "2   0000024238-07-000024.txt   20071129          FIDELITY CONTRAFUND  \n",
      "3   0000024238-07-000024.txt   20071129          FIDELITY CONTRAFUND  \n",
      "4   0000024238-07-000024.txt   20071129          FIDELITY CONTRAFUND  \n",
      "5   0000024238-07-000024.txt   20071129          FIDELITY CONTRAFUND  \n",
      "0   0000024238-13-000025.txt   20130530          FIDELITY CONTRAFUND  \n",
      "1   0000024238-13-000025.txt   20130530          FIDELITY CONTRAFUND  \n",
      "2   0000024238-13-000025.txt   20130530          FIDELITY CONTRAFUND  \n",
      "3   0000024238-13-000025.txt   20130530          FIDELITY CONTRAFUND  \n",
      "4   0000024238-13-000025.txt   20130530          FIDELITY CONTRAFUND  \n",
      "5   0000024238-13-000025.txt   20130530          FIDELITY CONTRAFUND  \n",
      "6   0000024238-13-000025.txt   20130530          FIDELITY CONTRAFUND  \n",
      "7   0000024238-13-000025.txt   20130530          FIDELITY CONTRAFUND  \n",
      "8   0000024238-13-000025.txt   20130530          FIDELITY CONTRAFUND  \n",
      "9   0000024238-13-000025.txt   20130530          FIDELITY CONTRAFUND  \n",
      "0   0000035348-11-000024.txt   20110531          FIDELITY CONTRAFUND  \n",
      "1   0000035348-11-000024.txt   20110531          FIDELITY CONTRAFUND  \n",
      "..                       ...        ...                          ...  \n",
      "3   0001379491-18-000940.txt   20180228  FIDELITY DESTINY PORTFOLIOS  \n",
      "4   0001379491-18-000940.txt   20180228  FIDELITY DESTINY PORTFOLIOS  \n",
      "5   0001379491-18-000940.txt   20180228  FIDELITY DESTINY PORTFOLIOS  \n",
      "6   0001379491-18-000940.txt   20180228  FIDELITY DESTINY PORTFOLIOS  \n",
      "7   0001379491-18-000940.txt   20180228  FIDELITY DESTINY PORTFOLIOS  \n",
      "8   0001379491-18-000940.txt   20180228  FIDELITY DESTINY PORTFOLIOS  \n",
      "9   0001379491-18-000940.txt   20180228  FIDELITY DESTINY PORTFOLIOS  \n",
      "10  0001379491-18-000940.txt   20180228  FIDELITY DESTINY PORTFOLIOS  \n",
      "0   0001379491-18-004143.txt   20180828  FIDELITY DESTINY PORTFOLIOS  \n",
      "1   0001379491-18-004143.txt   20180828  FIDELITY DESTINY PORTFOLIOS  \n",
      "2   0001379491-18-004143.txt   20180828  FIDELITY DESTINY PORTFOLIOS  \n",
      "3   0001379491-18-004143.txt   20180828  FIDELITY DESTINY PORTFOLIOS  \n",
      "4   0001379491-18-004143.txt   20180828  FIDELITY DESTINY PORTFOLIOS  \n",
      "5   0001379491-18-004143.txt   20180828  FIDELITY DESTINY PORTFOLIOS  \n",
      "6   0001379491-18-004143.txt   20180828  FIDELITY DESTINY PORTFOLIOS  \n",
      "7   0001379491-18-004143.txt   20180828  FIDELITY DESTINY PORTFOLIOS  \n",
      "8   0001379491-18-004143.txt   20180828  FIDELITY DESTINY PORTFOLIOS  \n",
      "9   0001379491-18-004143.txt   20180828  FIDELITY DESTINY PORTFOLIOS  \n",
      "10  0001379491-18-004143.txt   20180828  FIDELITY DESTINY PORTFOLIOS  \n",
      "0   0001379491-19-001009.txt   20190228  FIDELITY DESTINY PORTFOLIOS  \n",
      "1   0001379491-19-001009.txt   20190228  FIDELITY DESTINY PORTFOLIOS  \n",
      "2   0001379491-19-001009.txt   20190228  FIDELITY DESTINY PORTFOLIOS  \n",
      "3   0001379491-19-001009.txt   20190228  FIDELITY DESTINY PORTFOLIOS  \n",
      "4   0001379491-19-001009.txt   20190228  FIDELITY DESTINY PORTFOLIOS  \n",
      "5   0001379491-19-001009.txt   20190228  FIDELITY DESTINY PORTFOLIOS  \n",
      "6   0001379491-19-001009.txt   20190228  FIDELITY DESTINY PORTFOLIOS  \n",
      "7   0001379491-19-001009.txt   20190228  FIDELITY DESTINY PORTFOLIOS  \n",
      "8   0001379491-19-001009.txt   20190228  FIDELITY DESTINY PORTFOLIOS  \n",
      "9   0001379491-19-001009.txt   20190228  FIDELITY DESTINY PORTFOLIOS  \n",
      "10  0001379491-19-001009.txt   20190228  FIDELITY DESTINY PORTFOLIOS  \n",
      "\n",
      "[483 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "print(panel2)\n",
    "panel2.to_csv(os.path.join(output_directory, fund_name + '_panel2.csv'), sep = ',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Cleaning on One File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info_dict(i):\n",
    "    \n",
    "    cf = open(os.path.join(cur_wd, CIK, 'n-q',i), mode='r') \n",
    "    tables = {}\n",
    "    r_add = False\n",
    "    add = False\n",
    "    split_qtly = {}\n",
    "    \n",
    "    hold = False\n",
    "    legendFound = False\n",
    "    acqFound = False\n",
    "    getNexyLine = False\n",
    "    \n",
    "    value_mult = ''\n",
    "    \n",
    "    reporting_date = 'not found'\n",
    "    \n",
    "    for j in cf:\n",
    "        j = j.lower()\n",
    "        \n",
    "        if reporting_date is 'not found':\n",
    "            \n",
    "            if 'date of reporting period' in j and r'<div' in j:\n",
    "                r_addstring = remove_junk(j)\n",
    "                date = re.search(\"(jan(uary)?|feb(ruary)?|mar(ch)?|apr(il)?|may|jun(e)?|jul(y)?|aug(ust)?|sep(tember)?|oct(ober)?|nov(ember)?|dec(ember)?)\\s+\\d{1,2},\\s+\\d{4}\",r_addstring)\n",
    "                if date is not None:\n",
    "                    reporting_date = date.group()\n",
    "                \n",
    "            \n",
    "            if r'<tr>' in j and r'</tr>' not in j:\n",
    "                r_addstring = ''\n",
    "                r_add = True\n",
    "                \n",
    "            try:\n",
    "                if r'</tr>' in j and r'<tr>' not in j:\n",
    "                    r_addstring = \" \".join(r_addstring.split())\n",
    "                    r_add = False\n",
    "                    if 'date of reporting period' in r_addstring:\n",
    "                        date = re.search(\"(jan(uary)?|feb(ruary)?|mar(ch)?|apr(il)?|may|jun(e)?|jul(y)?|aug(ust)?|sep(tember)?|oct(ober)?|nov(ember)?|dec(ember)?)\\s+\\d{1,2},\\s+\\d{4}\",r_addstring)\n",
    "                        if date is not None:\n",
    "                            reporting_date = date.group()\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            \n",
    "            if r_add:\n",
    "                r_addline = remove_junk(j)\n",
    "                r_addstring = r_addstring + ' ' + r_addline\n",
    "                \n",
    "            \n",
    "        \n",
    "        \n",
    "        if r'value (' in j:\n",
    "            try:\n",
    "                value_mult = remove_junk(j)\n",
    "                value_mult = re.findall('\\((.*?)\\)',value_mult)\n",
    "                split_qtly[key]['holdings'].append(value_mult[0])\n",
    "            except IndexError:\n",
    "                #this is a value we do not want to scrap\n",
    "                pass\n",
    "            \n",
    "        if 'quarterly holdings' in j:\n",
    "            getNexyLine = True\n",
    "            \n",
    "        if getNexyLine and 'quarterly holdings' not in j:\n",
    "            if remove_junk(j) != '' and ('fidelity' in j or 'fund' in j or 'index' in j):\n",
    "                fund_quarterly_name = remove_junk(j)\n",
    "                fund_quarterly_name = \" \".join(fund_quarterly_name.split())\n",
    "                getNexyLine = False\n",
    "            \n",
    "        if '-qtly-' in j:\n",
    "            key = remove_junk(j)\n",
    "            split_qtly[key] = {}\n",
    "            split_qtly[key]['holdings'] = []\n",
    "            split_qtly[key]['legend'] = []\n",
    "            split_qtly[key]['acq'] = []\n",
    "            split_qtly[key]['name'] = fund_quarterly_name\n",
    "            \n",
    "            hold = True\n",
    "            legendFound = False\n",
    "            acqFound = False\n",
    "            \n",
    "        \n",
    "            \n",
    "        if r'>legend<' in j:\n",
    "            hold = False\n",
    "            legendFound = True\n",
    "            acqFound = False\n",
    "\n",
    "        if 'acquisition cost' in j:\n",
    "            hold = False\n",
    "            legendFound = False\n",
    "            acqFound = True\n",
    "        \n",
    "        if hold or legendFound or acqFound:\n",
    "            \n",
    "            if r'<tr>' in j:\n",
    "                addstring = ''\n",
    "                add = True\n",
    "\n",
    "            try:\n",
    "                if r'</tr>' in j:\n",
    "                    addstring = \" \".join(addstring.split())\n",
    "                    add = False\n",
    "                    if addstring != '':\n",
    "                        if hold:\n",
    "                            split_qtly[key]['holdings'].append(addstring)\n",
    "                        if legendFound and addstring not in split_qtly[key]['legend']:\n",
    "                            split_qtly[key]['legend'].append(addstring)\n",
    "                        if acqFound and addstring not in split_qtly[key]['acq']:\n",
    "                            split_qtly[key]['acq'].append(addstring)\n",
    "                            \n",
    "                        if 'date of reporting period' in addstring:\n",
    "                            print(addstring)\n",
    "            except:\n",
    "                pass\n",
    "                        \n",
    "                        \n",
    "\n",
    "            if add:\n",
    "                addline = remove_junk(j)\n",
    "                addstring = addstring + ' ' + addline\n",
    "                \n",
    "\n",
    "            if legendFound and r'<p' in j:\n",
    "                try:\n",
    "                    add_leg = remove_junk(j)\n",
    "                    if add_leg != '' :\n",
    "                        if add_leg not in split_qtly[key]['legend']:\n",
    "                            split_qtly[key]['legend'].append(add_leg)\n",
    "                except UnboundLocalError:\n",
    "                    pass\n",
    "                    #the key has not been found yet\n",
    "                \n",
    "            if acqFound and r'<td' in j:\n",
    "                add_acq = remove_junk(j)\n",
    "                if add_acq != '':\n",
    "                    if add_acq not in split_qtly[key]['acq']:\n",
    "                        split_qtly[key]['acq'].append(add_acq)\n",
    "                        \n",
    "    return split_qtly, reporting_date\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.sec.gov/Archives/edgar/data/24238/0000024238-04-000014.txt\n",
      "reporting date\n",
      "september 30, 2004\n"
     ]
    }
   ],
   "source": [
    "CIK  ='24238'\n",
    "i = '0000024238-04-000014.txt'\n",
    "text = i\n",
    "fundNames = panel2.loc[panel2['file_read'] == text, 'SERIES-NAME'].unique()\n",
    "\n",
    "series_names = panel2.loc[panel2['file_read'] == text, 'SERIES-NAME'].unique().tolist()\n",
    "series_names = list(map(lambda x:x.lower(),series_names))\n",
    "\n",
    "if '.csv' not in i:\n",
    "    for z in weblink[CIK]:\n",
    "        if i in z:\n",
    "            matching_link = z\n",
    "    print(matching_link)\n",
    "    \n",
    "    split_qtly, reporting_date = get_info_dict(i)\n",
    "    key = list(split_qtly.keys())[0]\n",
    "    print('reporting date')\n",
    "    print(reporting_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "Empty DataFrame\n",
      "Columns: [CLASS-CONTRACT-ID, CLASS-CONTRACT-NAME, CLASS-CONTRACT-TICKER-SYMBOL, SERIES-ID, OWNER-CIK, SERIES-NAME, file_read, date_filed, company conformed name]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "series_names = panel2.loc[panel2['file_read'] == text, 'SERIES-NAME'].unique().tolist()\n",
    "series_names = list(map(lambda x:x.lower(),series_names))\n",
    "series_names = list(map(lambda x: \"\".join(x.split()),series_names))\n",
    "\n",
    "print(series_names)\n",
    "\n",
    "print(panel2.loc[panel2['file_read'] == text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clean the legend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(a) non-income producing\n",
      "(b) affiliated fund that is available only to investment companies and other accounts managed\n",
      "(c) includes investment made with cash collateral received from securities on loan.\n",
      "(d) security exempt from registration under rule 144a of the securities act of 1933.  these\n",
      "(a) non-income producing\n",
      "(b) affiliated fund that is available only to investment companies and other accounts managed\n",
      "(c) includes investment made with cash collateral received from securities on loan.\n",
      "(d) affiliated company\n",
      "(e) security exempt from registration under rule 144a of the securities act of 1933.  these\n",
      "(f) restricted securities - investment in securities not registered under the securities act of 1933\n",
      "                                                  code           identifer  \\\n",
      "(a)                               non-income producing    anif-qtly-1104     \n",
      "(b)   affiliated fund that is available only to inv...    anif-qtly-1104     \n",
      "(c)   includes investment made with cash collateral...    anif-qtly-1104     \n",
      "(d)   security exempt from registration under rule ...    anif-qtly-1104     \n",
      "(a)                               non-income producing     con-qtly-1104     \n",
      "(b)   affiliated fund that is available only to inv...     con-qtly-1104     \n",
      "(c)   includes investment made with cash collateral...     con-qtly-1104     \n",
      "(d)                                 affiliated company     con-qtly-1104     \n",
      "(e)   security exempt from registration under rule ...     con-qtly-1104     \n",
      "(f)   restricted securities - investment in securit...     con-qtly-1104     \n",
      "\n",
      "                                               weblink  \\\n",
      "(a)  https://www.sec.gov/Archives/edgar/data/24238/...   \n",
      "(b)  https://www.sec.gov/Archives/edgar/data/24238/...   \n",
      "(c)  https://www.sec.gov/Archives/edgar/data/24238/...   \n",
      "(d)  https://www.sec.gov/Archives/edgar/data/24238/...   \n",
      "(a)  https://www.sec.gov/Archives/edgar/data/24238/...   \n",
      "(b)  https://www.sec.gov/Archives/edgar/data/24238/...   \n",
      "(c)  https://www.sec.gov/Archives/edgar/data/24238/...   \n",
      "(d)  https://www.sec.gov/Archives/edgar/data/24238/...   \n",
      "(e)  https://www.sec.gov/Archives/edgar/data/24238/...   \n",
      "(f)  https://www.sec.gov/Archives/edgar/data/24238/...   \n",
      "\n",
      "                     textfile  \n",
      "(a)  0000024238-04-000014.txt  \n",
      "(b)  0000024238-04-000014.txt  \n",
      "(c)  0000024238-04-000014.txt  \n",
      "(d)  0000024238-04-000014.txt  \n",
      "(a)  0000024238-04-000014.txt  \n",
      "(b)  0000024238-04-000014.txt  \n",
      "(c)  0000024238-04-000014.txt  \n",
      "(d)  0000024238-04-000014.txt  \n",
      "(e)  0000024238-04-000014.txt  \n",
      "(f)  0000024238-04-000014.txt  \n"
     ]
    }
   ],
   "source": [
    "len(split_qtly[key]['legend'])\n",
    "\n",
    "leg_dict = {}\n",
    "\n",
    "for j in split_qtly.keys():\n",
    "    count = 0\n",
    "    split_qtly[j]['cleaned legend'] = []\n",
    "    leg_dict[j]= {}\n",
    "    \n",
    "    restricted_securities = []\n",
    "    for i in split_qtly[j]['legend']:\n",
    "        i = i.rstrip().lstrip()\n",
    "        keys = re.search('\\(([^()]*)\\)',i)\n",
    "        try: \n",
    "            keys = keys.group()\n",
    "            if r'(' in i and r')' in i and len(keys) == 3:\n",
    "                split_qtly[j]['cleaned legend'].append(i.rstrip().lstrip()) \n",
    "                if i.rstrip().lstrip()[0:3] not in leg_dict[j].keys():\n",
    "                    print(i)\n",
    "                    leg_dict[j][i.rstrip().lstrip()[0:3]] = i.rstrip().lstrip()[3:]\n",
    "        except:\n",
    "            pass\n",
    "        count = count + 1\n",
    "        \n",
    "df_legend = pd.DataFrame()\n",
    "\n",
    "for j in leg_dict.keys():\n",
    "    try:\n",
    "        add = pd.DataFrame.from_dict(leg_dict[j], orient = 'index')\n",
    "        add.columns = ['code']\n",
    "        add.loc[:, 'identifer'] = j\n",
    "        add.loc[:, 'weblink'] = matching_link\n",
    "        add.loc[:, 'textfile'] = text\n",
    "\n",
    "        if df_legend.empty:\n",
    "            df_legend = add.copy()\n",
    "        else:\n",
    "            df_legend = pd.concat([df_legend, add], axis = 0)\n",
    "    except ValueError:\n",
    "        #there is no legend\n",
    "        pass\n",
    "        \n",
    "print(df_legend)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clean the acquisition data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    name acq date acq cost          identifer  \\\n",
      "0  jetsgo corp. warrants   3/1/04        0    con-qtly-1104     \n",
      "1   jetsgo corp. class b   3/1/04    9,334    con-qtly-1104     \n",
      "\n",
      "                                             weblink                  textfile  \n",
      "0  https://www.sec.gov/Archives/edgar/data/24238/...  0000024238-04-000014.txt  \n",
      "1  https://www.sec.gov/Archives/edgar/data/24238/...  0000024238-04-000014.txt  \n"
     ]
    }
   ],
   "source": [
    "len(split_qtly[key]['acq'])\n",
    "acq_dict = {}\n",
    "\n",
    "for j in split_qtly.keys():\n",
    "    split_qtly[j]['cleaned acq'] = []\n",
    "    for i in split_qtly[j]['acq']:\n",
    "        if hasNumbers(i) and r'$' in str(i):\n",
    "            clean_i = str(i).replace(r'$', r' $')\n",
    "            if clean_i.count(r\"$\") == 1 and clean_i.count(r\"/\") >= 2:\n",
    "                clean_i = clean_i.split(r' $')\n",
    "                cost = clean_i[1]\n",
    "                date = re.findall('\\d{1,2}/\\d{1,2}/\\d{2,4}', clean_i[0])\n",
    "                date = \" - \".join(date)\n",
    "                clean_i[0] = clean_i[0].replace(date, '')\n",
    "                clean_i[0] = clean_i[0].lstrip()\n",
    "                clean_i[0] = clean_i[0].rstrip()\n",
    "                if clean_i[0] != 'equities':\n",
    "                    split_qtly[j]['cleaned acq'].append([clean_i[0], date, cost])\n",
    "    acq_dict[j] = split_qtly[j]['cleaned acq']\n",
    "    \n",
    "\n",
    "df_acq = pd.DataFrame()\n",
    "for j in acq_dict.keys():\n",
    "    if len(acq_dict[j]) > 0:\n",
    "        add = pd.DataFrame(acq_dict[j])\n",
    "        add.columns = ['name', 'acq date', 'acq cost']\n",
    "        add.loc[:, 'identifer'] = j\n",
    "        add.loc[:, 'weblink'] = matching_link\n",
    "        add.loc[:, 'textfile'] = text\n",
    "\n",
    "        if df_acq.empty:\n",
    "            df_acq = add.copy()\n",
    "        else:\n",
    "            df_acq = pd.concat([df_acq, add], axis = 0)\n",
    "        \n",
    "\n",
    "print(df_acq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get the holdings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keys(leg_dict, j, i):\n",
    "    legend_keys = list(leg_dict[j].keys())\n",
    "    keys_to_add = ''\n",
    "\n",
    "    for k in legend_keys:\n",
    "        if k in i:\n",
    "            #i = i.replace(k,'')\n",
    "            if keys_to_add == '':\n",
    "                keys_to_add = k\n",
    "            else:\n",
    "                keys_to_add = keys_to_add + ',' + k\n",
    "\n",
    "    return keys_to_add, i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         holdings name holdings shares  \\\n",
      "0                                johnson controls inc.            8100   \n",
      "1                                            lkq corp.             100   \n",
      "2                                 honda motor co. ltd.            2200   \n",
      "3                                   toyota motor corp.           24600   \n",
      "4                              aristocrat leisure ltd.          175000   \n",
      "5                                    boyd gaming corp.           16500   \n",
      "6                              buffalo wild wings inc.            2400   \n",
      "7                             four seasons hotels inc.           29700   \n",
      "8                                 gtech holdings corp.            6200   \n",
      "9                                     hilton group plc          150470   \n",
      "10                      kerzner international ltd. (a)           14700   \n",
      "11                     krispy kreme doughnuts inc. (a)            2400   \n",
      "12                              life time fitness inc.            4900   \n",
      "13                                     paddy power plc           76000   \n",
      "14                       penn national gaming inc. (a)            7000   \n",
      "15                  red robin gourmet burgers inc. (a)           48500   \n",
      "16                             shuffle master inc. (a)           44400   \n",
      "17                                 starbucks corp. (a)            8000   \n",
      "18                                station casinos inc.          104700   \n",
      "19                               wynn resorts ltd. (a)           32200   \n",
      "20                            black &amp; decker corp.           10200   \n",
      "21                       blount international inc. (a)           15000   \n",
      "22                harman international industries inc.           26700   \n",
      "23                                             kb home            2100   \n",
      "24                            leggett &amp; platt inc.           12400   \n",
      "25                                 lg electronics inc.            4970   \n",
      "26                                    pulte homes inc.           19200   \n",
      "27                              toll brothers inc. (a)            2300   \n",
      "28                                      blue nile inc.             100   \n",
      "29                                       ebay inc. (a)           91700   \n",
      "..                                                 ...             ...   \n",
      "499                   vodafone group plc sponsored adr         9283100   \n",
      "500                 western wireless corp. class a (a)         1037400   \n",
      "501                                      entergy corp.          607500   \n",
      "502                                       exelon corp.         1035300   \n",
      "503                                  firstenergy corp.          505700   \n",
      "504                                 pg&amp;e corp. (a)         1672300   \n",
      "505                                          txu corp.          116200   \n",
      "506                             southern union co. (a)          262605   \n",
      "507                                      aes corp. (a)         1909100   \n",
      "508                            dominion resources inc.          253600   \n",
      "509                                  duke energy corp.          697100   \n",
      "510                                nrg energy inc. (a)          573100   \n",
      "511                                      sempra energy          107700   \n",
      "512                        total common stocks (cost )        25419413   \n",
      "513                   xerox capital trust ii 7.50% (e)          674000   \n",
      "514                                txu corp. 8.75% (a)         1187500   \n",
      "515         total convertible preferred stocks (cost )           85908   \n",
      "516                  eastman kodak co. 3.375% 10/15/33           41280   \n",
      "517  tyco international group sa yankee 3.125% 1/15/23           10860   \n",
      "518                             aes corp. 4.5% 8/15/05            7900   \n",
      "519                    total convertible bonds (cost )           70399   \n",
      "521                                      4.25% 8/15/13          233650   \n",
      "522                                     4.25% 11/15/13          194800   \n",
      "523                                      4.75% 5/15/14          211300   \n",
      "524            total u.s. treasury obligations (cost )          635437   \n",
      "525  fidelity cash central fund, 1.74% (b) 3,922,57...      non-equity   \n",
      "526  fidelity securities lending cash central fund,...      non-equity   \n",
      "527                   total money market funds (cost )         4289195   \n",
      "528  investments in repurchase agreements (collater...           19162   \n",
      "529                                     net assets - %             100   \n",
      "\n",
      "    holdings value     keys           identifer  \\\n",
      "0           460161             anif-qtly-1104     \n",
      "1             1827             anif-qtly-1104     \n",
      "2           107184             anif-qtly-1104     \n",
      "3           939474             anif-qtly-1104     \n",
      "4           987147             anif-qtly-1104     \n",
      "5           464475             anif-qtly-1104     \n",
      "6            67296             anif-qtly-1104     \n",
      "7          1903342             anif-qtly-1104     \n",
      "8           156984             anif-qtly-1104     \n",
      "9           754584             anif-qtly-1104     \n",
      "10          646359      (a)    anif-qtly-1104     \n",
      "11           30384      (a)    anif-qtly-1104     \n",
      "12          125734             anif-qtly-1104     \n",
      "13          897879             anif-qtly-1104     \n",
      "14          282800      (a)    anif-qtly-1104     \n",
      "15         2117995      (a)    anif-qtly-1104     \n",
      "16         1663224      (a)    anif-qtly-1104     \n",
      "17          363680      (a)    anif-qtly-1104     \n",
      "18         5134488             anif-qtly-1104     \n",
      "19         1664418      (a)    anif-qtly-1104     \n",
      "20          789888             anif-qtly-1104     \n",
      "21          196500      (a)    anif-qtly-1104     \n",
      "22         2876925             anif-qtly-1104     \n",
      "23          177429             anif-qtly-1104     \n",
      "24          348440             anif-qtly-1104     \n",
      "25          285726             anif-qtly-1104     \n",
      "26         1178304             anif-qtly-1104     \n",
      "27          106559      (a)    anif-qtly-1104     \n",
      "28            3368             anif-qtly-1104     \n",
      "29         8430898      (a)    anif-qtly-1104     \n",
      "..             ...      ...                 ...   \n",
      "499         223816              con-qtly-1104     \n",
      "500          26672      (a)     con-qtly-1104     \n",
      "501          36821              con-qtly-1104     \n",
      "502          37985              con-qtly-1104     \n",
      "503          20774              con-qtly-1104     \n",
      "504          50838      (a)     con-qtly-1104     \n",
      "505           5568              con-qtly-1104     \n",
      "506           5383      (a)     con-qtly-1104     \n",
      "507          19072      (a)     con-qtly-1104     \n",
      "508          16547              con-qtly-1104     \n",
      "509          15957              con-qtly-1104     \n",
      "510          15439      (a)     con-qtly-1104     \n",
      "511           3898              con-qtly-1104     \n",
      "512       34763684              con-qtly-1104     \n",
      "513          51996      (e)     con-qtly-1104     \n",
      "514          62122      (a)     con-qtly-1104     \n",
      "515         114118              con-qtly-1104     \n",
      "516          52104              con-qtly-1104     \n",
      "517          16235              con-qtly-1104     \n",
      "518           7920              con-qtly-1104     \n",
      "519          76259              con-qtly-1104     \n",
      "521         237146              con-qtly-1104     \n",
      "522         197311              con-qtly-1104     \n",
      "523         221799              con-qtly-1104     \n",
      "524         656256              con-qtly-1104     \n",
      "525                     (b)     con-qtly-1104     \n",
      "526                 (b),(c)     con-qtly-1104     \n",
      "527        4289195              con-qtly-1104     \n",
      "528          19161              con-qtly-1104     \n",
      "529       39464795              con-qtly-1104     \n",
      "\n",
      "                                               weblink  \\\n",
      "0    https://www.sec.gov/Archives/edgar/data/24238/...   \n",
      "1    https://www.sec.gov/Archives/edgar/data/24238/...   \n",
      "2    https://www.sec.gov/Archives/edgar/data/24238/...   \n",
      "3    https://www.sec.gov/Archives/edgar/data/24238/...   \n",
      "4    https://www.sec.gov/Archives/edgar/data/24238/...   \n",
      "5    https://www.sec.gov/Archives/edgar/data/24238/...   \n",
      "6    https://www.sec.gov/Archives/edgar/data/24238/...   \n",
      "7    https://www.sec.gov/Archives/edgar/data/24238/...   \n",
      "8    https://www.sec.gov/Archives/edgar/data/24238/...   \n",
      "9    https://www.sec.gov/Archives/edgar/data/24238/...   \n",
      "10   https://www.sec.gov/Archives/edgar/data/24238/...   \n",
      "11   https://www.sec.gov/Archives/edgar/data/24238/...   \n",
      "12   https://www.sec.gov/Archives/edgar/data/24238/...   \n",
      "13   https://www.sec.gov/Archives/edgar/data/24238/...   \n",
      "14   https://www.sec.gov/Archives/edgar/data/24238/...   \n",
      "15   https://www.sec.gov/Archives/edgar/data/24238/...   \n",
      "16   https://www.sec.gov/Archives/edgar/data/24238/...   \n",
      "17   https://www.sec.gov/Archives/edgar/data/24238/...   \n",
      "18   https://www.sec.gov/Archives/edgar/data/24238/...   \n",
      "19   https://www.sec.gov/Archives/edgar/data/24238/...   \n",
      "20   https://www.sec.gov/Archives/edgar/data/24238/...   \n",
      "21   https://www.sec.gov/Archives/edgar/data/24238/...   \n",
      "22   https://www.sec.gov/Archives/edgar/data/24238/...   \n",
      "23   https://www.sec.gov/Archives/edgar/data/24238/...   \n",
      "24   https://www.sec.gov/Archives/edgar/data/24238/...   \n",
      "25   https://www.sec.gov/Archives/edgar/data/24238/...   \n",
      "26   https://www.sec.gov/Archives/edgar/data/24238/...   \n",
      "27   https://www.sec.gov/Archives/edgar/data/24238/...   \n",
      "28   https://www.sec.gov/Archives/edgar/data/24238/...   \n",
      "29   https://www.sec.gov/Archives/edgar/data/24238/...   \n",
      "..                                                 ...   \n",
      "499  https://www.sec.gov/Archives/edgar/data/24238/...   \n",
      "500  https://www.sec.gov/Archives/edgar/data/24238/...   \n",
      "501  https://www.sec.gov/Archives/edgar/data/24238/...   \n",
      "502  https://www.sec.gov/Archives/edgar/data/24238/...   \n",
      "503  https://www.sec.gov/Archives/edgar/data/24238/...   \n",
      "504  https://www.sec.gov/Archives/edgar/data/24238/...   \n",
      "505  https://www.sec.gov/Archives/edgar/data/24238/...   \n",
      "506  https://www.sec.gov/Archives/edgar/data/24238/...   \n",
      "507  https://www.sec.gov/Archives/edgar/data/24238/...   \n",
      "508  https://www.sec.gov/Archives/edgar/data/24238/...   \n",
      "509  https://www.sec.gov/Archives/edgar/data/24238/...   \n",
      "510  https://www.sec.gov/Archives/edgar/data/24238/...   \n",
      "511  https://www.sec.gov/Archives/edgar/data/24238/...   \n",
      "512  https://www.sec.gov/Archives/edgar/data/24238/...   \n",
      "513  https://www.sec.gov/Archives/edgar/data/24238/...   \n",
      "514  https://www.sec.gov/Archives/edgar/data/24238/...   \n",
      "515  https://www.sec.gov/Archives/edgar/data/24238/...   \n",
      "516  https://www.sec.gov/Archives/edgar/data/24238/...   \n",
      "517  https://www.sec.gov/Archives/edgar/data/24238/...   \n",
      "518  https://www.sec.gov/Archives/edgar/data/24238/...   \n",
      "519  https://www.sec.gov/Archives/edgar/data/24238/...   \n",
      "521  https://www.sec.gov/Archives/edgar/data/24238/...   \n",
      "522  https://www.sec.gov/Archives/edgar/data/24238/...   \n",
      "523  https://www.sec.gov/Archives/edgar/data/24238/...   \n",
      "524  https://www.sec.gov/Archives/edgar/data/24238/...   \n",
      "525  https://www.sec.gov/Archives/edgar/data/24238/...   \n",
      "526  https://www.sec.gov/Archives/edgar/data/24238/...   \n",
      "527  https://www.sec.gov/Archives/edgar/data/24238/...   \n",
      "528  https://www.sec.gov/Archives/edgar/data/24238/...   \n",
      "529  https://www.sec.gov/Archives/edgar/data/24238/...   \n",
      "\n",
      "                     textfile  \n",
      "0    0000024238-04-000014.txt  \n",
      "1    0000024238-04-000014.txt  \n",
      "2    0000024238-04-000014.txt  \n",
      "3    0000024238-04-000014.txt  \n",
      "4    0000024238-04-000014.txt  \n",
      "5    0000024238-04-000014.txt  \n",
      "6    0000024238-04-000014.txt  \n",
      "7    0000024238-04-000014.txt  \n",
      "8    0000024238-04-000014.txt  \n",
      "9    0000024238-04-000014.txt  \n",
      "10   0000024238-04-000014.txt  \n",
      "11   0000024238-04-000014.txt  \n",
      "12   0000024238-04-000014.txt  \n",
      "13   0000024238-04-000014.txt  \n",
      "14   0000024238-04-000014.txt  \n",
      "15   0000024238-04-000014.txt  \n",
      "16   0000024238-04-000014.txt  \n",
      "17   0000024238-04-000014.txt  \n",
      "18   0000024238-04-000014.txt  \n",
      "19   0000024238-04-000014.txt  \n",
      "20   0000024238-04-000014.txt  \n",
      "21   0000024238-04-000014.txt  \n",
      "22   0000024238-04-000014.txt  \n",
      "23   0000024238-04-000014.txt  \n",
      "24   0000024238-04-000014.txt  \n",
      "25   0000024238-04-000014.txt  \n",
      "26   0000024238-04-000014.txt  \n",
      "27   0000024238-04-000014.txt  \n",
      "28   0000024238-04-000014.txt  \n",
      "29   0000024238-04-000014.txt  \n",
      "..                        ...  \n",
      "499  0000024238-04-000014.txt  \n",
      "500  0000024238-04-000014.txt  \n",
      "501  0000024238-04-000014.txt  \n",
      "502  0000024238-04-000014.txt  \n",
      "503  0000024238-04-000014.txt  \n",
      "504  0000024238-04-000014.txt  \n",
      "505  0000024238-04-000014.txt  \n",
      "506  0000024238-04-000014.txt  \n",
      "507  0000024238-04-000014.txt  \n",
      "508  0000024238-04-000014.txt  \n",
      "509  0000024238-04-000014.txt  \n",
      "510  0000024238-04-000014.txt  \n",
      "511  0000024238-04-000014.txt  \n",
      "512  0000024238-04-000014.txt  \n",
      "513  0000024238-04-000014.txt  \n",
      "514  0000024238-04-000014.txt  \n",
      "515  0000024238-04-000014.txt  \n",
      "516  0000024238-04-000014.txt  \n",
      "517  0000024238-04-000014.txt  \n",
      "518  0000024238-04-000014.txt  \n",
      "519  0000024238-04-000014.txt  \n",
      "521  0000024238-04-000014.txt  \n",
      "522  0000024238-04-000014.txt  \n",
      "523  0000024238-04-000014.txt  \n",
      "524  0000024238-04-000014.txt  \n",
      "525  0000024238-04-000014.txt  \n",
      "526  0000024238-04-000014.txt  \n",
      "527  0000024238-04-000014.txt  \n",
      "528  0000024238-04-000014.txt  \n",
      "529  0000024238-04-000014.txt  \n",
      "\n",
      "[987 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "    len(split_qtly[key]['holdings'])\n",
    "    holdings_dict = {}\n",
    "    for j in split_qtly.keys():\n",
    "        \n",
    "        split_qtly[j]['cleaned holdings'] = []\n",
    "        for i in split_qtly[j]['holdings']:\n",
    "\n",
    "            if hasNumbers(i) and i[-1] != r'%' and 'total investment' not in i and 'principalamount' not in i:\n",
    "                keys_found, cleaned_i = get_keys(leg_dict, j, i)\n",
    "                cleaned_i  = cleaned_i.replace(r'$', ' ')\n",
    "                cleaned_i  = cleaned_i.replace(r',', '')\n",
    "                cleaned_i = cleaned_i.lstrip()\n",
    "                cleaned_i = cleaned_i.rstrip()\n",
    "                cleaned_i = \" \".join(cleaned_i.split())\n",
    "                isplit = cleaned_i.split(' ')\n",
    "                if len(isplit) >= 3:\n",
    "                    if hasNumbers(isplit[-1]) and hasNumbers(isplit[-2]):\n",
    "                        if 'warrant' in cleaned_i or 'loan' in cleaned_i or 'lending' in cleaned_i or 'tranche' in \\\n",
    "                        cleaned_i or 'cash central' in cleaned_i :\n",
    "                            if 'warrant' in cleaned_i:\n",
    "                                split_qtly[j]['cleaned holdings'].append([i, 'warrant', ''])\n",
    "                            elif 'loan' in cleaned_i:\n",
    "                                split_qtly[j]['cleaned holdings'].append([i, 'loan', ''])\n",
    "                            elif 'trance' in cleaned_i:\n",
    "                                split_qtly[j]['cleaned holdings'].append([i, 'pooled-security', ''])\n",
    "                            else:\n",
    "                                split_qtly[j]['cleaned holdings'].append([i, 'non-equity', '',keys_found])\n",
    "                            \n",
    "                        else:\n",
    "                            non_dig = re.findall(r'\\D+', isplit[-2])\n",
    "                            non_dig = \" \".join(non_dig)\n",
    "                            isplit[-3] = isplit[-3] +' ' + non_dig\n",
    "                            isplit[-2] = isplit[-2].replace(non_dig, '')\n",
    "                            name = ' '.join(isplit[0:-2])\n",
    "                            name = name.lstrip()\n",
    "                            name = name.rstrip()\n",
    "                            split_qtly[j]['cleaned holdings'].append([name , isplit[-2], isplit[-1], keys_found])\n",
    "            elif ':' in i:\n",
    "                keys_found, cleaned_i = get_keys(leg_dict, j, i)\n",
    "                cleaned_i  = cleaned_i.replace(r'$', ' ')\n",
    "                cleaned_i  = cleaned_i.replace(r',', '')\n",
    "                cleaned_i = cleaned_i.lstrip()\n",
    "                cleaned_i = cleaned_i.rstrip()\n",
    "                cleaned_i = \" \".join(cleaned_i.split())\n",
    "                split_qtly[j]['cleaned holdings'].append([i, 'header', '',keys_found])\n",
    "                \n",
    "        holdings_dict[j] = split_qtly[j]['cleaned holdings']\n",
    "\n",
    "    df_holdings = pd.DataFrame()\n",
    "    for j in holdings_dict.keys():\n",
    "        add = pd.DataFrame(holdings_dict[j])\n",
    "        add.columns = ['holdings name', 'holdings shares', 'holdings value', 'keys']\n",
    "        add.loc[:, 'identifer'] = j\n",
    "        add.loc[:, 'weblink'] = matching_link\n",
    "        add.loc[:, 'textfile'] = text\n",
    "\n",
    "        if df_holdings.empty:\n",
    "            df_holdings = add.copy()\n",
    "        else:\n",
    "            df_holdings = pd.concat([df_holdings, add], axis = 0)\n",
    "\n",
    "    df_holdings = df_holdings.drop_duplicates(subset = ['holdings name', 'holdings shares', 'holdings value'])\n",
    "    \n",
    "    print(df_holdings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Panel 1 FOR ALL CIKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getPanel1_add(split_qtly, matching_link, text, cik):\n",
    "    \n",
    "    #get the security legend\n",
    "    \n",
    "    leg_dict = {}\n",
    "\n",
    "    for j in split_qtly.keys():\n",
    "        count = 0\n",
    "        split_qtly[j]['cleaned legend'] = []\n",
    "        leg_dict[j]= {}\n",
    "\n",
    "        restricted_securities = []\n",
    "        for i in split_qtly[j]['legend']:\n",
    "            i = i.rstrip().lstrip()\n",
    "            keys = re.search('\\(([^()]*)\\)',i)\n",
    "            try: \n",
    "                keys = keys.group()\n",
    "                if r'(' in i and r')' in i and len(keys) == 3:\n",
    "                    split_qtly[j]['cleaned legend'].append(i.rstrip().lstrip()) \n",
    "                    if i.rstrip().lstrip()[0:3] not in leg_dict[j].keys():\n",
    "                        leg_dict[j][i.rstrip().lstrip()[0:3]] = i.rstrip().lstrip()[3:]\n",
    "            except:\n",
    "                pass\n",
    "            count = count + 1\n",
    "\n",
    "    df_legend = pd.DataFrame()\n",
    "\n",
    "    for j in leg_dict.keys():\n",
    "        try:\n",
    "            add = pd.DataFrame.from_dict(leg_dict[j], orient = 'index')\n",
    "            add.columns = ['code']\n",
    "            add.loc[:, 'identifer'] = j\n",
    "            add.loc[:, 'weblink'] = matching_link\n",
    "            add.loc[:, 'textfile'] = text\n",
    "\n",
    "            if df_legend.empty:\n",
    "                df_legend = add.copy()\n",
    "            else:\n",
    "                df_legend = pd.concat([df_legend, add], axis = 0)\n",
    "        except ValueError:\n",
    "            #there is no legend\n",
    "            pass\n",
    "            \n",
    "            \n",
    "    #get the security acquisition data\n",
    "    \n",
    "    acq_dict = {}\n",
    "\n",
    "    for j in split_qtly.keys():\n",
    "        split_qtly[j]['cleaned acq'] = []\n",
    "        for i in split_qtly[j]['acq']:\n",
    "            if hasNumbers(i) and r'$' in str(i):\n",
    "                clean_i = str(i).replace(r'$', r' $')\n",
    "                if clean_i.count(r\"$\") == 1 and clean_i.count(r\"/\") >= 2:\n",
    "                    clean_i = clean_i.split(r' $')\n",
    "                    cost = clean_i[1].lstrip().rstrip()\n",
    "                    date = re.findall('\\d{1,2}/\\d{1,2}/\\d{2,4}', clean_i[0])\n",
    "                    date = \" - \".join(date)\n",
    "                    date = date.lstrip().rstrip()\n",
    "                    name = clean_i[0].replace(date, '')\n",
    "                    name = name.lstrip()\n",
    "                    name = name.rstrip()\n",
    "                    if name != 'equities':\n",
    "                        split_qtly[j]['cleaned acq'].append([name, date, cost])\n",
    "        acq_dict[j] = split_qtly[j]['cleaned acq']\n",
    "\n",
    "    df_acq = pd.DataFrame()\n",
    "    for j in acq_dict.keys():\n",
    "        if len(acq_dict[j]) > 0:\n",
    "            add = pd.DataFrame(acq_dict[j])\n",
    "            add.columns = ['acq name', 'acq date', 'acq cost']\n",
    "            add.loc[:, 'identifer'] = j\n",
    "            add.loc[:, 'weblink'] = matching_link\n",
    "            add.loc[:, 'textfile'] = text\n",
    "            add.loc[:, 'CIK'] = cik\n",
    "            add.loc[:, 'fund name'] = split_qtly[j]['name']\n",
    "\n",
    "            if df_acq.empty:\n",
    "                df_acq = add.copy()\n",
    "            else:\n",
    "                df_acq = pd.concat([df_acq, add], axis = 0)\n",
    "            \n",
    "    #get the holdings data\n",
    "    \n",
    "    holdings_dict = {}\n",
    "    value_mult = ''\n",
    "    for j in split_qtly.keys():\n",
    "        \n",
    "        leg_dict[j]\n",
    "        split_qtly[j]['cleaned holdings'] = []\n",
    "        for i in split_qtly[j]['holdings']:\n",
    "            \n",
    "            if r'value (' in i:\n",
    "                value_mult = remove_junk(i)\n",
    "                value_mult = str(re.findall('\\((.*?)\\)',value_mult)[0])\n",
    "\n",
    "            elif hasNumbers(i) and i[-1] != r'%' and 'total investment' not in i and 'principalamount' not in i \\\n",
    "            and r'cost)' not in i and r'(cost' not in i and 'net asset' not in i:\n",
    "                keys_found, cleaned_i = get_keys(leg_dict, j, i)\n",
    "                cleaned_i  = cleaned_i.replace(r'$', ' ')\n",
    "                cleaned_i  = cleaned_i.replace(r',', '')\n",
    "                cleaned_i = cleaned_i.lstrip()\n",
    "                cleaned_i = cleaned_i.rstrip()\n",
    "                cleaned_i = \" \".join(cleaned_i.split())\n",
    "                isplit = cleaned_i.split(' ')\n",
    "                if len(isplit) >= 3:\n",
    "                    if hasNumbers(isplit[-1]) and hasNumbers(isplit[-2]):\n",
    "                        if 'warrant' in cleaned_i or 'loan' in cleaned_i or 'lending' in cleaned_i or 'tranche' in \\\n",
    "                        cleaned_i or 'cash central' in cleaned_i or r'%' in cleaned_i:\n",
    "                            if 'warrant' in cleaned_i:\n",
    "                                split_qtly[j]['cleaned holdings'].append([i, 'warrant', '',keys_found, value_mult])\n",
    "                            elif 'loan' in cleaned_i and 'tranche' not in cleaned_i:\n",
    "                                split_qtly[j]['cleaned holdings'].append([i, 'loan', '',keys_found, value_mult])\n",
    "                            elif 'tranche' in cleaned_i:\n",
    "                                split_qtly[j]['cleaned holdings'].append([i, 'pooled-security', '',keys_found, value_mult])\n",
    "                            elif 'cash central' in cleaned_i:\n",
    "                                split_qtly[j]['cleaned holdings'].append([i, 'cash central fund', '',keys_found, value_mult])\n",
    "                            elif r'%' in cleaned_i and r'/' in cleaned_i:\n",
    "                                split_qtly[j]['cleaned holdings'].append([i, 'corporate bond', '',keys_found, value_mult])\n",
    "                            elif r'%' in cleaned_i and r'/' not in cleaned_i:\n",
    "                                split_qtly[j]['cleaned holdings'].append([i, 'pref stock', '',keys_found, value_mult])\n",
    "                            else:\n",
    "                                split_qtly[j]['cleaned holdings'].append([i, 'non-equity', '',keys_found, value_mult])\n",
    "                        else:\n",
    "                            non_dig = re.findall(r'\\D+', isplit[-2])\n",
    "                            non_dig = \" \".join(non_dig)\n",
    "                            isplit[-3] = isplit[-3] +' ' + non_dig\n",
    "                            isplit[-2] = isplit[-2].replace(non_dig, '')\n",
    "                            name = ' '.join(isplit[0:-2])\n",
    "                            name = name.lstrip()\n",
    "                            name = name.rstrip()\n",
    "                            split_qtly[j]['cleaned holdings'].append([name , isplit[-2], isplit[-1],keys_found, value_mult])\n",
    "            elif ':' in i:\n",
    "                if 'total investment' not in i and 'principalamount' not in i \\\n",
    "                and r'cost)' not in i and r'(cost' not in i and 'net asset' not in i:\n",
    "                    keys_found, cleaned_i = get_keys(leg_dict, j, i)\n",
    "                    cleaned_i  = cleaned_i.replace(r'$', ' ')\n",
    "                    cleaned_i  = cleaned_i.replace(r',', '')\n",
    "                    cleaned_i = cleaned_i.lstrip()\n",
    "                    cleaned_i = cleaned_i.rstrip()\n",
    "                    cleaned_i  = cleaned_i.replace(r':', '')\n",
    "                    cleaned_i = \" \".join(cleaned_i.split())\n",
    "                    split_qtly[j]['cleaned holdings'].append([i, 'header', '',keys_found, value_mult])\n",
    "            else:\n",
    "                pass\n",
    "                \n",
    "        holdings_dict[j] = split_qtly[j]['cleaned holdings']\n",
    "\n",
    "    df_holdings = pd.DataFrame()\n",
    "    for j in holdings_dict.keys():\n",
    "        add = pd.DataFrame(holdings_dict[j])\n",
    "        add.columns = ['holdings name', 'holdings shares', 'holdings value', 'key', 'value multiplier']\n",
    "        add.loc[:, 'identifer'] = j\n",
    "        add.loc[:, 'weblink'] = matching_link\n",
    "        add.loc[:, 'textfile'] = text\n",
    "        add.loc[:, 'CIK'] = cik\n",
    "        add.loc[:, 'fund name'] = split_qtly[j]['name']\n",
    "\n",
    "        if df_holdings.empty:\n",
    "            df_holdings = add.copy()\n",
    "        else:\n",
    "            df_holdings = pd.concat([df_holdings, add], axis = 0)\n",
    "\n",
    "    df_holdings = df_holdings.drop_duplicates(subset = ['holdings name', 'holdings shares', 'holdings value'])\n",
    "    df_acq = df_acq.drop_duplicates(subset = ['acq name', 'acq date', 'acq cost'])\n",
    "    \n",
    "    \n",
    "    return df_legend, df_acq, df_holdings\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.sec.gov/Archives/edgar/data/24238/0000024238-06-000007.txt\n",
      "https://www.sec.gov/Archives/edgar/data/24238/0000024238-07-000011.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jjanko\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.sec.gov/Archives/edgar/data/24238/0000024238-07-000024.txt\n",
      "https://www.sec.gov/Archives/edgar/data/24238/0000024238-13-000025.txt\n",
      "https://www.sec.gov/Archives/edgar/data/24238/0000035348-11-000024.txt\n",
      "https://www.sec.gov/Archives/edgar/data/24238/0000035402-12-000012.txt\n",
      "https://www.sec.gov/Archives/edgar/data/24238/0000315700-15-000076.txt\n",
      "https://www.sec.gov/Archives/edgar/data/24238/0000354046-10-000009.txt\n",
      "https://www.sec.gov/Archives/edgar/data/24238/0000356494-14-000054.txt\n",
      "https://www.sec.gov/Archives/edgar/data/24238/0000722574-08-000117.txt\n",
      "https://www.sec.gov/Archives/edgar/data/24238/0000722574-09-000089.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jjanko\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:37: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.sec.gov/Archives/edgar/data/24238/0000722574-09-000267.txt\n",
      "https://www.sec.gov/Archives/edgar/data/24238/0000722574-10-000311.txt\n",
      "https://www.sec.gov/Archives/edgar/data/24238/0000722574-11-000359.txt\n",
      "https://www.sec.gov/Archives/edgar/data/24238/0000729218-08-000022.txt\n",
      "https://www.sec.gov/Archives/edgar/data/24238/0000795422-12-000136.txt\n",
      "https://www.sec.gov/Archives/edgar/data/24238/0000878467-06-000026.txt\n",
      "https://www.sec.gov/Archives/edgar/data/24238/0000878467-14-000949.txt\n",
      "https://www.sec.gov/Archives/edgar/data/24238/0000880195-13-000956.txt\n",
      "https://www.sec.gov/Archives/edgar/data/24238/0001379491-15-001530.txt\n",
      "https://www.sec.gov/Archives/edgar/data/24238/0001379491-16-004311.txt\n",
      "https://www.sec.gov/Archives/edgar/data/24238/0001379491-16-007548.txt\n",
      "https://www.sec.gov/Archives/edgar/data/24238/0001379491-17-003201.txt\n",
      "https://www.sec.gov/Archives/edgar/data/24238/0001379491-17-007752.txt\n",
      "https://www.sec.gov/Archives/edgar/data/24238/0001379491-18-002500.txt\n",
      "https://www.sec.gov/Archives/edgar/data/24238/0001379491-18-006129.txt\n",
      "https://www.sec.gov/Archives/edgar/data/35331/0000024238-10-000020.txt\n",
      "https://www.sec.gov/Archives/edgar/data/35331/0000035315-14-000108.txt\n",
      "https://www.sec.gov/Archives/edgar/data/35331/0000035331-12-000002.txt\n",
      "https://www.sec.gov/Archives/edgar/data/35331/0000035373-13-000006.txt\n",
      "https://www.sec.gov/Archives/edgar/data/35331/0000356173-09-000006.txt\n",
      "https://www.sec.gov/Archives/edgar/data/35331/0000356173-15-000050.txt\n",
      "https://www.sec.gov/Archives/edgar/data/35331/0000831016-11-000014.txt\n",
      "https://www.sec.gov/Archives/edgar/data/35331/0000878467-13-000574.txt\n",
      "https://www.sec.gov/Archives/edgar/data/35331/0000880195-10-000065.txt\n",
      "https://www.sec.gov/Archives/edgar/data/35331/0000880195-14-001076.txt\n",
      "https://www.sec.gov/Archives/edgar/data/35331/0000880195-15-000857.txt\n",
      "https://www.sec.gov/Archives/edgar/data/35331/0001303459-09-000008.txt\n",
      "https://www.sec.gov/Archives/edgar/data/35331/0001303459-11-000004.txt\n",
      "https://www.sec.gov/Archives/edgar/data/35331/0001303459-12-000061.txt\n",
      "https://www.sec.gov/Archives/edgar/data/35331/0001379491-16-002662.txt\n",
      "https://www.sec.gov/Archives/edgar/data/35331/0001379491-16-005413.txt\n",
      "https://www.sec.gov/Archives/edgar/data/35331/0001379491-17-001325.txt\n",
      "https://www.sec.gov/Archives/edgar/data/35331/0001379491-17-005672.txt\n",
      "https://www.sec.gov/Archives/edgar/data/35331/0001379491-18-000940.txt\n",
      "https://www.sec.gov/Archives/edgar/data/35331/0001379491-18-004143.txt\n",
      "https://www.sec.gov/Archives/edgar/data/35331/0001379491-19-001009.txt\n",
      "errors\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "panel1 = pd.DataFrame()\n",
    "panel1_legend = pd.DataFrame()\n",
    "error_panel1 = []\n",
    "\n",
    "for CIK in CIK_LIST:\n",
    "    txt_files = os.listdir(os.path.join(cur_wd, CIK, 'n-q'))\n",
    "    \n",
    "    for i in txt_files:\n",
    "        if '.csv' not in i and i not in error_panel2:\n",
    "            for z in weblink[CIK]:\n",
    "                if i in z:\n",
    "                    matching_link = z\n",
    "            print(matching_link)\n",
    "            \n",
    "            try:\n",
    "                split_qtly, reporting_date = get_info_dict(i)\n",
    "                \n",
    "                #get the panel1 data\n",
    "                df_legend, df_acq, df_holdings = getPanel1_add(split_qtly, matching_link, i, CIK)\n",
    "                \n",
    "                #combine the holdings and acq data in one panel\n",
    "                df_holdings = pd.concat([df_holdings, df_acq], axis = 0)\n",
    "                \n",
    "                #add the conformed data and date filed to panel 1 by reading the date_filed\n",
    "                date_filed = panel2.loc[panel2['file_read'] == i, 'date_filed'].unique()[0]\n",
    "                conformed_name = panel2.loc[panel2['file_read'] == i, 'company conformed name'].unique()[0]\n",
    "                \n",
    "                df_holdings.loc[:, 'date_filed'] = date_filed\n",
    "                df_holdings.loc[:, 'reporting_date'] = reporting_date\n",
    "                df_holdings.loc[:, 'company conformed name'] = conformed_name\n",
    "                \n",
    "                #add the new holdings data modified to panel 1 and also add legend information to output panels\n",
    "                if panel1.empty:\n",
    "                    panel1 = df_holdings.copy()\n",
    "                    panel1_legend = df_legend.copy()\n",
    "                else:\n",
    "                    panel1 = pd.concat([panel1, df_holdings] , axis = 0)\n",
    "                    panel1_legend = pd.concat([panel1_legend, df_legend], axis = 0)\n",
    "            except:\n",
    "                print(\"Unexpected error:\", sys.exc_info()[0])\n",
    "                error_panel1.append(i)\n",
    "                print(CIK)\n",
    "                print(i)\n",
    "         \n",
    "print('errors')\n",
    "print(error_panel1)\n",
    "\n",
    "panel1_legend.to_csv(os.path.join(output_directory, fund_name + '_panel1_legend.csv'), sep = ',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "map the header information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "panel1 = panel1.reset_index(drop=True)\n",
    "for index, row in panel1.iterrows():\n",
    "    if row['holdings shares'] == 'header':\n",
    "        lastheader = str(row['holdings name'].replace(r':', ''))\n",
    "    if not pd.isnull(row['holdings name']):\n",
    "        if str(row['holdings name'][0:len('class')]).lower() == 'class':\n",
    "            panel1.loc[panel1.index == index, 'holdings name'] = lastheader + r'_' + str(row['holdings name'])\n",
    "        elif str(row['holdings name'][0:len('series')]).lower() == 'series':\n",
    "            panel1.loc[panel1.index == index, 'holdings name'] = lastheader + r'_' + str(row['holdings name'])\n",
    "        elif str(row['holdings name'][0:len('warrants')]).lower() == 'warrants':\n",
    "            panel1.loc[panel1.index == index, 'holdings name'] = lastheader + r'_' + str(row['holdings name'])\n",
    "        elif str(row['holdings name'][0:len('adr')]).lower() == 'adr':\n",
    "            panel1.loc[panel1.index == index, 'holdings name'] = lastheader + r'_' + str(row['holdings name'])            \n",
    "        elif r'%' in row['holdings name'] and  hasNumbers(row['holdings name'][0]):\n",
    "            panel1.loc[panel1.index == index, 'holdings name'] = lastheader + r'_' + str(row['holdings name'])\n",
    "\n",
    "panel1 = panel1.loc[panel1['holdings shares'] != 'header']  \n",
    "panel1.to_csv(os.path.join(output_directory, fund_name + '_panel1.csv'), sep = ',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get the restricted securities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jjanko\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:15: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39029\n",
      "1924\n",
      "3372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jjanko\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:22: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "panel1_legend.loc[:,'restricted'] = 'no'\n",
    "\n",
    "acq = panel1.loc[~panel1['acq date'].isnull()]\n",
    "\n",
    "panel1_legend.loc[panel1_legend['code'].str.contains(\"restrict\") | panel1_legend['code'].str.contains(\"level 3 security\") \\\n",
    "              | panel1_legend['code'].str.contains(\"exempt from registration\"), 'restricted'] = 'yes'\n",
    "\n",
    "restricted = panel1_legend.loc[panel1_legend['restricted'].str.contains(\"yes\")]\n",
    "restricted.to_csv(os.path.join(output_directory, fund_name + '_panel1_restricted_legend.csv'), sep = ',')\n",
    "\n",
    "panel1.loc[:,'restricted'] = 'no'\n",
    "\n",
    "for index, row in restricted.iterrows():\n",
    "    panel1.loc[(panel1['identifer'] == row['identifer']) & (panel1['textfile'] == row.textfile) \\\n",
    "                   & (panel1['key'].str.contains(index)), 'restricted'] = 'yes'\n",
    "\n",
    "#add the acquisition data\n",
    "\n",
    "print(len(panel1))\n",
    "restricted_panel1 = panel1.loc[(panel1['restricted'] == 'yes')]\n",
    "print(len(restricted_panel1))\n",
    "restricted_panel1 = pd.concat([panel1.loc[(panel1['restricted'] == 'yes')], acq], axis = 0)\n",
    "print(len(restricted_panel1))\n",
    "restricted_panel1.to_csv(os.path.join(output_directory, fund_name + '_panel1_restricted.csv'), sep = ',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
