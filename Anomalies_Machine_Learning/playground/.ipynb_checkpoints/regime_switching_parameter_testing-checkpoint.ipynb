{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from regime_module import *\n",
    "from statsmodels.iolib.summary2 import summary_col\n",
    "import csv\n",
    "import os\n",
    "import matplotlib.backends.backend_pdf\n",
    "\n",
    "cur_wd = os.getcwd()\n",
    "print(cur_wd)\n",
    "image_wd = os.path.join(cur_wd, 'backtest_images')\n",
    "backtest_wd = os.path.join(cur_wd, 'backtests')\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_ff5 = pd.DataFrame()\n",
    "alpha_capm = pd.DataFrame()\n",
    "\n",
    "alpha_ff5_scaled = pd.DataFrame()\n",
    "alpha_capm_scaled = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_outputs(bt, ff_factors_compare, umd, q_factor, returns, lookback_backtest, feature_set, binary_returns, backtest_write, image_write, cur_wd, data_type ):\n",
    "    \n",
    "    outname = 'output_results_' + str(lookback) + '_' + str(lookback_backtest) +  '.txt'\n",
    "    outnamelatex = 'output_results_latex_' + str(lookback) + '_' + str(lookback_backtest) +  '.txt'\n",
    "    \n",
    "    outname_scaled = 'output_results_scaled_' + str(lookback) + '_' + str(lookback_backtest) +  '.txt'\n",
    "    outnamelatex_scaled = 'output_results_latex_scaled_' + str(lookback) + '_' + str(lookback_backtest) +  '.txt'\n",
    "    \n",
    "    \n",
    "    output_reg = []\n",
    "    output_capm = []\n",
    "    output_umd = []\n",
    "    output_qfactor = []\n",
    "    output_tm = []\n",
    "    output_hm = []\n",
    "    \n",
    "    output_reg_scaled = []\n",
    "    output_capm_scaled = []\n",
    "    output_umd_scaled = []\n",
    "    output_qfactor_scaled = []\n",
    "    output_tm_scaled = []\n",
    "    output_hm_scaled = []\n",
    "    \n",
    "    #names = ['Logistic L/S', 'Logistic L', 'Logistic S', 'KNN L/S', 'KNN L', 'KNN S', 'Neural Net L/S', 'Neural Net L', 'Neural Net S', 'RF L/S', 'RF L', 'RF S','AdaBoost L/S', 'AdaBoost L', 'AdaBoost S', 'Ensemble', 'Binary']\n",
    "\n",
    "    names = ['KNN L/S', 'KNN L', 'KNN S', 'Neural Net L/S', 'Neural Net L', 'Neural Net S', 'RF L/S', 'RF L', 'RF S','AdaBoost L/S', 'AdaBoost L', 'AdaBoost S', 'Ensemble', 'Binary']\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    print(\"Logistic\")\n",
    "    print(\"Long and Short\")\n",
    "    backtest, weights, scaled_backtest = bt.run_backtest(returns, lookback_backtest, input_classifiers= [\"Logistic\"], inputs = feature_set, outputs = binary_returns,  ensemble = 'no', trade_type = 'both')\n",
    "    results, fm_sum = bt.factor_results(backtest, ff_factors_compare)\n",
    "    results_capm, fm_sum_capm = bt.factor_results_capm(backtest, ff_factors_compare)\n",
    "    results_umd, fm_sum_umd = bt.factor_results_umd(backtest, umd)\n",
    "    results_q, fm_sum_q = bt.factor_results_qfactor(backtest, q_factor)\n",
    "    results_tm = bt.treynor_mazuy(backtest, ff_factors_compare)\n",
    "    results_hm = bt.henriksson_merton(backtest, ff_factors_compare)\n",
    "\n",
    "    results_scaled, fm_sum = bt.factor_results(scaled_backtest, ff_factors_compare)\n",
    "    results_capm_scaled, fm_sum_capm = bt.factor_results_capm(scaled_backtest, ff_factors_compare)\n",
    "    results_umd_scaled, fm_sum_umd = bt.factor_results_umd(scaled_backtest, umd)\n",
    "    results_q_scaled, fm_sum_q = bt.factor_results_qfactor(scaled_backtest, q_factor)\n",
    "    results_tm_scaled = bt.treynor_mazuy(scaled_backtest, ff_factors_compare)\n",
    "    results_hm_scaled = bt.henriksson_merton(scaled_backtest, ff_factors_compare)\n",
    "\n",
    "    output_reg.append(results)\n",
    "    output_capm.append(results_capm)\n",
    "    output_umd.append(results_umd)\n",
    "    output_qfactor.append(results_q)\n",
    "    output_tm.append(results_tm)\n",
    "    output_hm.append(results_hm)\n",
    "\n",
    "    output_reg_scaled.append(results_scaled)\n",
    "    output_capm_scaled.append(results_capm_scaled)\n",
    "    output_umd_scaled.append(results_umd_scaled)\n",
    "    output_qfactor_scaled.append(results_q_scaled)\n",
    "    output_tm_scaled.append(results_tm_scaled)\n",
    "    output_hm_scaled.append(results_hm_scaled)\n",
    "\n",
    "    backtest.to_csv(os.path.join(backtest_write, data_type + '_Logistic_' + 'ls' + '.csv'))\n",
    "    scaled_backtest.to_csv(os.path.join(backtest_write, data_type + '_Logistic_scaled_' + 'ls' + '.csv'))\n",
    "    plot_bf = pd.DataFrame(1.0 + backtest.mean(axis = 1), columns = ['backtest']).cumprod()\n",
    "    plot_mkt = (1.0 + ff_factors['Mkt-RF'].loc[plot_bf.index]).cumprod()\n",
    "    plot_df = pd.concat([plot_bf, plot_mkt], axis = 1).dropna()\n",
    "    ax = plot_df.plot()\n",
    "    ax.figure.savefig(os.path.join(image_write, data_type + '_Logistic_' + 'ls' + '.pdf'))\n",
    "\n",
    "\n",
    "    print(\"Long Only\")\n",
    "    backtest, weights, scaled_backtest = bt.run_backtest(returns, lookback_backtest, input_classifiers= [\"Logistic\"], inputs = feature_set, outputs = binary_returns,  ensemble = 'no', trade_type = 'long')\n",
    "    results, fm_sum = bt.factor_results(backtest, ff_factors_compare)\n",
    "    results_capm, fm_sum_capm = bt.factor_results_capm(backtest, ff_factors_compare)\n",
    "    results_umd, fm_sum_umd = bt.factor_results_umd(backtest, umd)\n",
    "    results_q, fm_sum_q = bt.factor_results_qfactor(backtest, q_factor)\n",
    "    results_tm = bt.treynor_mazuy(backtest, ff_factors_compare)\n",
    "    results_hm = bt.henriksson_merton(backtest, ff_factors_compare)\n",
    "\n",
    "    results_scaled, fm_sum = bt.factor_results(scaled_backtest, ff_factors_compare)\n",
    "    results_capm_scaled, fm_sum_capm = bt.factor_results_capm(scaled_backtest, ff_factors_compare)\n",
    "    results_umd_scaled, fm_sum_umd = bt.factor_results_umd(scaled_backtest, umd)\n",
    "    results_q_scaled, fm_sum_q = bt.factor_results_qfactor(scaled_backtest, q_factor)\n",
    "    results_tm_scaled = bt.treynor_mazuy(scaled_backtest, ff_factors_compare)\n",
    "    results_hm_scaled = bt.henriksson_merton(scaled_backtest, ff_factors_compare)\n",
    "\n",
    "    output_reg.append(results)\n",
    "    output_capm.append(results_capm)\n",
    "    output_umd.append(results_umd)\n",
    "    output_qfactor.append(results_q)\n",
    "    output_tm.append(results_tm)\n",
    "    output_hm.append(results_hm)\n",
    "\n",
    "    output_reg_scaled.append(results_scaled)\n",
    "    output_capm_scaled.append(results_capm_scaled)\n",
    "    output_umd_scaled.append(results_umd_scaled)\n",
    "    output_qfactor_scaled.append(results_q_scaled)\n",
    "    output_tm_scaled.append(results_tm_scaled)\n",
    "    output_hm_scaled.append(results_hm_scaled)\n",
    "\n",
    "    backtest.to_csv(os.path.join(backtest_write, data_type + '_Logistic_' + 'l' + '.csv'))\n",
    "    scaled_backtest.to_csv(os.path.join(backtest_write, data_type + '_Logistic_scaled_' + 'l' + '.csv'))\n",
    "    plot_bf = pd.DataFrame(1.0 + backtest.mean(axis = 1), columns = ['backtest']).cumprod()\n",
    "    plot_mkt = (1.0 + ff_factors['Mkt-RF'].loc[plot_bf.index]).cumprod()\n",
    "    plot_df = pd.concat([plot_bf, plot_mkt], axis = 1).dropna()\n",
    "    ax = plot_df.plot()\n",
    "    ax.figure.savefig(os.path.join(image_write, data_type + '_Logistic_' + 'l' + '.pdf'))\n",
    "\n",
    "\n",
    "    print(\"Short Only\")\n",
    "    backtest, weights, scaled_backtest = bt.run_backtest(returns, lookback_backtest, input_classifiers= [\"Logistic\"], inputs = feature_set, outputs = binary_returns,  ensemble = 'no', trade_type = 'short')\n",
    "    results, fm_sum = bt.factor_results(backtest, ff_factors_compare)\n",
    "    results_capm, fm_sum_capm = bt.factor_results_capm(backtest, ff_factors_compare)\n",
    "    results_umd, fm_sum_umd = bt.factor_results_umd(backtest, umd)\n",
    "    results_q, fm_sum_q = bt.factor_results_qfactor(backtest, q_factor)\n",
    "    results_tm = bt.treynor_mazuy(backtest, ff_factors_compare)\n",
    "    results_hm = bt.henriksson_merton(backtest, ff_factors_compare)\n",
    "\n",
    "    results_scaled, fm_sum = bt.factor_results(scaled_backtest, ff_factors_compare)\n",
    "    results_capm_scaled, fm_sum_capm = bt.factor_results_capm(scaled_backtest, ff_factors_compare)\n",
    "    results_umd_scaled, fm_sum_umd = bt.factor_results_umd(scaled_backtest, umd)\n",
    "    results_q_scaled, fm_sum_q = bt.factor_results_qfactor(scaled_backtest, q_factor)\n",
    "    results_tm_scaled = bt.treynor_mazuy(scaled_backtest, ff_factors_compare)\n",
    "    results_hm_scaled = bt.henriksson_merton(scaled_backtest, ff_factors_compare)\n",
    "\n",
    "    output_reg.append(results)\n",
    "    output_capm.append(results_capm)\n",
    "    output_umd.append(results_umd)\n",
    "    output_qfactor.append(results_q)\n",
    "    output_tm.append(results_tm)\n",
    "    output_hm.append(results_hm)\n",
    "\n",
    "    output_reg_scaled.append(results_scaled)\n",
    "    output_capm_scaled.append(results_capm_scaled)\n",
    "    output_umd_scaled.append(results_umd_scaled)\n",
    "    output_qfactor_scaled.append(results_q_scaled)\n",
    "    output_tm_scaled.append(results_tm_scaled)\n",
    "    output_hm_scaled.append(results_hm_scaled)\n",
    "\n",
    "    backtest.to_csv(os.path.join(backtest_write, data_type + '_Logistic_' + 's' + '.csv'))\n",
    "    scaled_backtest.to_csv(os.path.join(backtest_write, data_type + '_Logistic_scaled_' + 's' + '.csv'))\n",
    "    plot_bf = pd.DataFrame(1.0 + backtest.mean(axis = 1), columns = ['backtest']).cumprod()\n",
    "    plot_mkt = (1.0 + ff_factors['Mkt-RF'].loc[plot_bf.index]).cumprod()\n",
    "    plot_df = pd.concat([plot_bf, plot_mkt], axis = 1).dropna()\n",
    "    ax = plot_df.plot()\n",
    "    ax.figure.savefig(os.path.join(image_write, data_type + '_Logistic_' + 's' + '.pdf'))\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(\"KNN\")\n",
    "    print(\"Long and Short\")\n",
    "    backtest, weights, scaled_backtest = bt.run_backtest(returns, lookback_backtest, input_classifiers= [\"Nearest Neighbors\"], inputs = feature_set, outputs = binary_returns,  ensemble = 'no', trade_type = 'both')\n",
    "    results, fm_sum = bt.factor_results(backtest, ff_factors_compare)\n",
    "    results_capm, fm_sum_capm = bt.factor_results_capm(backtest, ff_factors_compare)\n",
    "    results_umd, fm_sum_umd = bt.factor_results_umd(backtest, umd)\n",
    "    results_q, fm_sum_q = bt.factor_results_qfactor(backtest, q_factor)\n",
    "    results_tm = bt.treynor_mazuy(backtest, ff_factors_compare)\n",
    "    results_hm = bt.henriksson_merton(backtest, ff_factors_compare)\n",
    "\n",
    "    results_scaled, fm_sum = bt.factor_results(scaled_backtest, ff_factors_compare)\n",
    "    results_capm_scaled, fm_sum_capm = bt.factor_results_capm(scaled_backtest, ff_factors_compare)\n",
    "    results_umd_scaled, fm_sum_umd = bt.factor_results_umd(scaled_backtest, umd)\n",
    "    results_q_scaled, fm_sum_q = bt.factor_results_qfactor(scaled_backtest, q_factor)\n",
    "    results_tm_scaled = bt.treynor_mazuy(scaled_backtest, ff_factors_compare)\n",
    "    results_hm_scaled = bt.henriksson_merton(scaled_backtest, ff_factors_compare)\n",
    "\n",
    "    output_reg.append(results)\n",
    "    output_capm.append(results_capm)\n",
    "    output_umd.append(results_umd)\n",
    "    output_qfactor.append(results_q)\n",
    "    output_tm.append(results_tm)\n",
    "    output_hm.append(results_hm)\n",
    "\n",
    "    output_reg_scaled.append(results_scaled)\n",
    "    output_capm_scaled.append(results_capm_scaled)\n",
    "    output_umd_scaled.append(results_umd_scaled)\n",
    "    output_qfactor_scaled.append(results_q_scaled)\n",
    "    output_tm_scaled.append(results_tm_scaled)\n",
    "    output_hm_scaled.append(results_hm_scaled)\n",
    "\n",
    "    backtest.to_csv(os.path.join(backtest_write, data_type + '_KNN_' + 'ls' + '.csv'))\n",
    "    scaled_backtest.to_csv(os.path.join(backtest_write, data_type + '_KNN_scaled_' + 'ls' + '.csv'))\n",
    "    plot_bf = pd.DataFrame(1.0 + backtest.mean(axis = 1), columns = ['backtest']).cumprod()\n",
    "    plot_mkt = (1.0 + ff_factors['Mkt-RF'].loc[plot_bf.index]).cumprod()\n",
    "    plot_df = pd.concat([plot_bf, plot_mkt], axis = 1).dropna()\n",
    "    ax = plot_df.plot()\n",
    "    ax.figure.savefig(os.path.join(image_write, data_type + '_KNN_' + 'ls' + '.pdf'))\n",
    "\n",
    "    \n",
    "\n",
    "    print(\"Long Only\")\n",
    "    backtest, weights, scaled_backtest = bt.run_backtest(returns, lookback_backtest, input_classifiers= [\"Nearest Neighbors\"], inputs = feature_set, outputs = binary_returns,  ensemble = 'no', trade_type = 'long')\n",
    "    results, fm_sum = bt.factor_results(backtest, ff_factors_compare)\n",
    "    results_capm, fm_sum_capm = bt.factor_results_capm(backtest, ff_factors_compare)\n",
    "    results_umd, fm_sum_umd = bt.factor_results_umd(backtest, umd)\n",
    "    results_q, fm_sum_q = bt.factor_results_qfactor(backtest, q_factor)\n",
    "    results_tm = bt.treynor_mazuy(backtest, ff_factors_compare)\n",
    "    results_hm = bt.henriksson_merton(backtest, ff_factors_compare)\n",
    "\n",
    "    results_scaled, fm_sum = bt.factor_results(scaled_backtest, ff_factors_compare)\n",
    "    results_capm_scaled, fm_sum_capm = bt.factor_results_capm(scaled_backtest, ff_factors_compare)\n",
    "    results_umd_scaled, fm_sum_umd = bt.factor_results_umd(scaled_backtest, umd)\n",
    "    results_q_scaled, fm_sum_q = bt.factor_results_qfactor(scaled_backtest, q_factor)\n",
    "    results_tm_scaled = bt.treynor_mazuy(scaled_backtest, ff_factors_compare)\n",
    "    results_hm_scaled = bt.henriksson_merton(scaled_backtest, ff_factors_compare)\n",
    "\n",
    "    output_reg.append(results)\n",
    "    output_capm.append(results_capm)\n",
    "    output_umd.append(results_umd)\n",
    "    output_qfactor.append(results_q)\n",
    "    output_tm.append(results_tm)\n",
    "    output_hm.append(results_hm)\n",
    "\n",
    "    output_reg_scaled.append(results_scaled)\n",
    "    output_capm_scaled.append(results_capm_scaled)\n",
    "    output_umd_scaled.append(results_umd_scaled)\n",
    "    output_qfactor_scaled.append(results_q_scaled)\n",
    "    output_tm_scaled.append(results_tm_scaled)\n",
    "    output_hm_scaled.append(results_hm_scaled)\n",
    "\n",
    "    backtest.to_csv(os.path.join(backtest_write, data_type + '_KNN_' + 'l' + '.csv'))\n",
    "    scaled_backtest.to_csv(os.path.join(backtest_write, data_type + '_KNN_scaled_' + 'l' + '.csv'))\n",
    "    plot_bf = pd.DataFrame(1.0 + backtest.mean(axis = 1), columns = ['backtest']).cumprod()\n",
    "    plot_mkt = (1.0 + ff_factors['Mkt-RF'].loc[plot_bf.index]).cumprod()\n",
    "    plot_df = pd.concat([plot_bf, plot_mkt], axis = 1).dropna()\n",
    "    ax = plot_df.plot()\n",
    "    ax.figure.savefig(os.path.join(image_write, data_type + '_KNN_' + 'l' + '.pdf'))\n",
    "\n",
    "\n",
    "    print(\"Short Only\")\n",
    "    backtest, weights, scaled_backtest = bt.run_backtest(returns, lookback_backtest, input_classifiers= [\"Nearest Neighbors\"], inputs = feature_set, outputs = binary_returns,  ensemble = 'no', trade_type = 'short')\n",
    "    results, fm_sum = bt.factor_results(backtest, ff_factors_compare)\n",
    "    results_capm, fm_sum_capm = bt.factor_results_capm(backtest, ff_factors_compare)\n",
    "    results_umd, fm_sum_umd = bt.factor_results_umd(backtest, umd)\n",
    "    results_q, fm_sum_q = bt.factor_results_qfactor(backtest, q_factor)\n",
    "    results_tm = bt.treynor_mazuy(backtest, ff_factors_compare)\n",
    "    results_hm = bt.henriksson_merton(backtest, ff_factors_compare)\n",
    "\n",
    "    results_scaled, fm_sum = bt.factor_results(scaled_backtest, ff_factors_compare)\n",
    "    results_capm_scaled, fm_sum_capm = bt.factor_results_capm(scaled_backtest, ff_factors_compare)\n",
    "    results_umd_scaled, fm_sum_umd = bt.factor_results_umd(scaled_backtest, umd)\n",
    "    results_q_scaled, fm_sum_q = bt.factor_results_qfactor(scaled_backtest, q_factor)\n",
    "    results_tm_scaled = bt.treynor_mazuy(scaled_backtest, ff_factors_compare)\n",
    "    results_hm_scaled = bt.henriksson_merton(scaled_backtest, ff_factors_compare)\n",
    "\n",
    "    output_reg.append(results)\n",
    "    output_capm.append(results_capm)\n",
    "    output_umd.append(results_umd)\n",
    "    output_qfactor.append(results_q)\n",
    "    output_tm.append(results_tm)\n",
    "    output_hm.append(results_hm)\n",
    "\n",
    "    output_reg_scaled.append(results_scaled)\n",
    "    output_capm_scaled.append(results_capm_scaled)\n",
    "    output_umd_scaled.append(results_umd_scaled)\n",
    "    output_qfactor_scaled.append(results_q_scaled)\n",
    "    output_tm_scaled.append(results_tm_scaled)\n",
    "    output_hm_scaled.append(results_hm_scaled)\n",
    "\n",
    "    backtest.to_csv(os.path.join(backtest_write, data_type + '_KNN_' + 's' + '.csv'))\n",
    "    scaled_backtest.to_csv(os.path.join(backtest_write, data_type + '_KNN_scaled_' + 's' + '.csv'))\n",
    "    plot_bf = pd.DataFrame(1.0 + backtest.mean(axis = 1), columns = ['backtest']).cumprod()\n",
    "    plot_mkt = (1.0 + ff_factors['Mkt-RF'].loc[plot_bf.index]).cumprod()\n",
    "    plot_df = pd.concat([plot_bf, plot_mkt], axis = 1).dropna()\n",
    "    ax = plot_df.plot()\n",
    "    ax.figure.savefig(os.path.join(image_write, data_type + '_KNN_' + 's' + '.pdf'))\n",
    "\n",
    "\n",
    "    print(\"Neural Net\")\n",
    "    print(\"Long and Short\")\n",
    "    backtest, weights, scaled_backtest = bt.run_backtest(returns, lookback_backtest, input_classifiers= [\"Neural Net\"], inputs = feature_set, outputs = binary_returns,  ensemble = 'no', trade_type = 'both')\n",
    "    results, fm_sum = bt.factor_results(backtest, ff_factors_compare)\n",
    "    results_capm, fm_sum_capm = bt.factor_results_capm(backtest, ff_factors_compare)\n",
    "    results_umd, fm_sum_umd = bt.factor_results_umd(backtest, umd)\n",
    "    results_q, fm_sum_q = bt.factor_results_qfactor(backtest, q_factor)\n",
    "    results_tm = bt.treynor_mazuy(backtest, ff_factors_compare)\n",
    "    results_hm = bt.henriksson_merton(backtest, ff_factors_compare)\n",
    "\n",
    "    results_scaled, fm_sum = bt.factor_results(scaled_backtest, ff_factors_compare)\n",
    "    results_capm_scaled, fm_sum_capm = bt.factor_results_capm(scaled_backtest, ff_factors_compare)\n",
    "    results_umd_scaled, fm_sum_umd = bt.factor_results_umd(scaled_backtest, umd)\n",
    "    results_q_scaled, fm_sum_q = bt.factor_results_qfactor(scaled_backtest, q_factor)\n",
    "    results_tm_scaled = bt.treynor_mazuy(scaled_backtest, ff_factors_compare)\n",
    "    results_hm_scaled = bt.henriksson_merton(scaled_backtest, ff_factors_compare)\n",
    "\n",
    "    output_reg.append(results)\n",
    "    output_capm.append(results_capm)\n",
    "    output_umd.append(results_umd)\n",
    "    output_qfactor.append(results_q)\n",
    "    output_tm.append(results_tm)\n",
    "    output_hm.append(results_hm)\n",
    "\n",
    "    output_reg_scaled.append(results_scaled)\n",
    "    output_capm_scaled.append(results_capm_scaled)\n",
    "    output_umd_scaled.append(results_umd_scaled)\n",
    "    output_qfactor_scaled.append(results_q_scaled)\n",
    "    output_tm_scaled.append(results_tm_scaled)\n",
    "    output_hm_scaled.append(results_hm_scaled)\n",
    "\n",
    "    backtest.to_csv(os.path.join(backtest_write, data_type + '_NN_' + 'ls' + '.csv'))\n",
    "    scaled_backtest.to_csv(os.path.join(backtest_write, data_type + '_NN_scaled_' + 'ls' + '.csv'))\n",
    "    plot_bf = pd.DataFrame(1.0 + backtest.mean(axis = 1), columns = ['backtest']).cumprod()\n",
    "    plot_mkt = (1.0 + ff_factors['Mkt-RF'].loc[plot_bf.index]).cumprod()\n",
    "    plot_df = pd.concat([plot_bf, plot_mkt], axis = 1).dropna()\n",
    "    ax = plot_df.plot()\n",
    "    ax.figure.savefig(os.path.join(image_write, data_type + '_NN_' + 'ls' + '.pdf'))\n",
    "\n",
    "\n",
    "    print(\"Long Only\")\n",
    "    backtest, weights, scaled_backtest = bt.run_backtest(returns, lookback_backtest, input_classifiers= [\"Neural Net\"], inputs = feature_set, outputs = binary_returns,  ensemble = 'no', trade_type = 'long')\n",
    "    results, fm_sum = bt.factor_results(backtest, ff_factors_compare)\n",
    "    results_capm, fm_sum_capm = bt.factor_results_capm(backtest, ff_factors_compare)\n",
    "    results_umd, fm_sum_umd = bt.factor_results_umd(backtest, umd)\n",
    "    results_q, fm_sum_q = bt.factor_results_qfactor(backtest, q_factor)\n",
    "    results_tm = bt.treynor_mazuy(backtest, ff_factors_compare)\n",
    "    results_hm = bt.henriksson_merton(backtest, ff_factors_compare)\n",
    "\n",
    "    results_scaled, fm_sum = bt.factor_results(scaled_backtest, ff_factors_compare)\n",
    "    results_capm_scaled, fm_sum_capm = bt.factor_results_capm(scaled_backtest, ff_factors_compare)\n",
    "    results_umd_scaled, fm_sum_umd = bt.factor_results_umd(scaled_backtest, umd)\n",
    "    results_q_scaled, fm_sum_q = bt.factor_results_qfactor(scaled_backtest, q_factor)\n",
    "    results_tm_scaled = bt.treynor_mazuy(scaled_backtest, ff_factors_compare)\n",
    "    results_hm_scaled = bt.henriksson_merton(scaled_backtest, ff_factors_compare)\n",
    "\n",
    "    output_reg.append(results)\n",
    "    output_capm.append(results_capm)\n",
    "    output_umd.append(results_umd)\n",
    "    output_qfactor.append(results_q)\n",
    "    output_tm.append(results_tm)\n",
    "    output_hm.append(results_hm)\n",
    "\n",
    "    output_reg_scaled.append(results_scaled)\n",
    "    output_capm_scaled.append(results_capm_scaled)\n",
    "    output_umd_scaled.append(results_umd_scaled)\n",
    "    output_qfactor_scaled.append(results_q_scaled)\n",
    "    output_tm_scaled.append(results_tm_scaled)\n",
    "    output_hm_scaled.append(results_hm_scaled)\n",
    "\n",
    "    backtest.to_csv(os.path.join(backtest_write, data_type + '_NN_' + 'l' + '.csv'))\n",
    "    scaled_backtest.to_csv(os.path.join(backtest_write, data_type + '_NN_scaled_' + 'l' + '.csv'))\n",
    "    plot_bf = pd.DataFrame(1.0 + backtest.mean(axis = 1), columns = ['backtest']).cumprod()\n",
    "    plot_mkt = (1.0 + ff_factors['Mkt-RF'].loc[plot_bf.index]).cumprod()\n",
    "    plot_df = pd.concat([plot_bf, plot_mkt], axis = 1).dropna()\n",
    "    ax = plot_df.plot()\n",
    "    ax.figure.savefig(os.path.join(image_write, data_type + '_NN_' + 'l' + '.pdf'))\n",
    "\n",
    "    print(\"Short Only\")\n",
    "    backtest, weights, scaled_backtest = bt.run_backtest(returns,lookback_backtest, input_classifiers= [\"Neural Net\"], inputs = feature_set, outputs = binary_returns,  ensemble = 'no', trade_type = 'short')\n",
    "    results, fm_sum = bt.factor_results(backtest, ff_factors_compare)\n",
    "    results_capm, fm_sum_capm = bt.factor_results_capm(backtest, ff_factors_compare)\n",
    "    results_umd, fm_sum_umd = bt.factor_results_umd(backtest, umd)\n",
    "    results_q, fm_sum_q = bt.factor_results_qfactor(backtest, q_factor)\n",
    "    results_tm = bt.treynor_mazuy(backtest, ff_factors_compare)\n",
    "    results_hm = bt.henriksson_merton(backtest, ff_factors_compare)\n",
    "\n",
    "    results_scaled, fm_sum = bt.factor_results(scaled_backtest, ff_factors_compare)\n",
    "    results_capm_scaled, fm_sum_capm = bt.factor_results_capm(scaled_backtest, ff_factors_compare)\n",
    "    results_umd_scaled, fm_sum_umd = bt.factor_results_umd(scaled_backtest, umd)\n",
    "    results_q_scaled, fm_sum_q = bt.factor_results_qfactor(scaled_backtest, q_factor)\n",
    "    results_tm_scaled = bt.treynor_mazuy(scaled_backtest, ff_factors_compare)\n",
    "    results_hm_scaled = bt.henriksson_merton(scaled_backtest, ff_factors_compare)\n",
    "\n",
    "    output_reg.append(results)\n",
    "    output_capm.append(results_capm)\n",
    "    output_umd.append(results_umd)\n",
    "    output_qfactor.append(results_q)\n",
    "    output_tm.append(results_tm)\n",
    "    output_hm.append(results_hm)\n",
    "\n",
    "    output_reg_scaled.append(results_scaled)\n",
    "    output_capm_scaled.append(results_capm_scaled)\n",
    "    output_umd_scaled.append(results_umd_scaled)\n",
    "    output_qfactor_scaled.append(results_q_scaled)\n",
    "    output_tm_scaled.append(results_tm_scaled)\n",
    "    output_hm_scaled.append(results_hm_scaled)\n",
    "\n",
    "    backtest.to_csv(os.path.join(backtest_write, data_type + '_NN_' + 's' + '.csv'))\n",
    "    scaled_backtest.to_csv(os.path.join(backtest_write, data_type + '_NN_scaled_' + 's' + '.csv'))\n",
    "    plot_bf = pd.DataFrame(1.0 + backtest.mean(axis = 1), columns = ['backtest']).cumprod()\n",
    "    plot_mkt = (1.0 + ff_factors['Mkt-RF'].loc[plot_bf.index]).cumprod()\n",
    "    plot_df = pd.concat([plot_bf, plot_mkt], axis = 1).dropna()\n",
    "    ax = plot_df.plot()\n",
    "    ax.figure.savefig(os.path.join(image_write, data_type + '_NN_' + 's' + '.pdf'))\n",
    "\n",
    "\n",
    "    print(\"Random Forest\")\n",
    "    print(\"Long and Short\")\n",
    "    backtest, weights, scaled_backtest = bt.run_backtest(returns, lookback_backtest, input_classifiers= [\"Random Forest\"], inputs = feature_set, outputs = binary_returns,  ensemble = 'no', trade_type = 'both')\n",
    "    results, fm_sum = bt.factor_results(backtest, ff_factors_compare)\n",
    "    results_capm, fm_sum_capm = bt.factor_results_capm(backtest, ff_factors_compare)\n",
    "    results_umd, fm_sum_umd = bt.factor_results_umd(backtest, umd)\n",
    "    results_q, fm_sum_q = bt.factor_results_qfactor(backtest, q_factor)\n",
    "    results_tm = bt.treynor_mazuy(backtest, ff_factors_compare)\n",
    "    results_hm = bt.henriksson_merton(backtest, ff_factors_compare)\n",
    "\n",
    "    results_scaled, fm_sum = bt.factor_results(scaled_backtest, ff_factors_compare)\n",
    "    results_capm_scaled, fm_sum_capm = bt.factor_results_capm(scaled_backtest, ff_factors_compare)\n",
    "    results_umd_scaled, fm_sum_umd = bt.factor_results_umd(scaled_backtest, umd)\n",
    "    results_q_scaled, fm_sum_q = bt.factor_results_qfactor(scaled_backtest, q_factor)\n",
    "    results_tm_scaled = bt.treynor_mazuy(scaled_backtest, ff_factors_compare)\n",
    "    results_hm_scaled = bt.henriksson_merton(scaled_backtest, ff_factors_compare)\n",
    "\n",
    "    output_reg.append(results)\n",
    "    output_capm.append(results_capm)\n",
    "    output_umd.append(results_umd)\n",
    "    output_qfactor.append(results_q)\n",
    "    output_tm.append(results_tm)\n",
    "    output_hm.append(results_hm)\n",
    "\n",
    "    output_reg_scaled.append(results_scaled)\n",
    "    output_capm_scaled.append(results_capm_scaled)\n",
    "    output_umd_scaled.append(results_umd_scaled)\n",
    "    output_qfactor_scaled.append(results_q_scaled)\n",
    "    output_tm_scaled.append(results_tm_scaled)\n",
    "    output_hm_scaled.append(results_hm_scaled)\n",
    "\n",
    "    backtest.to_csv(os.path.join(backtest_write, data_type + '_RF_' + 'ls' + '.csv'))\n",
    "    scaled_backtest.to_csv(os.path.join(backtest_write, data_type + '_RF_scaled_' + 'ls' + '.csv'))\n",
    "    plot_bf = pd.DataFrame(1.0 + backtest.mean(axis = 1), columns = ['backtest']).cumprod()\n",
    "    plot_mkt = (1.0 + ff_factors['Mkt-RF'].loc[plot_bf.index]).cumprod()\n",
    "    plot_df = pd.concat([plot_bf, plot_mkt], axis = 1).dropna()\n",
    "    ax = plot_df.plot()\n",
    "    ax.figure.savefig(os.path.join(image_write, data_type + '_RF_' + 'ls' + '.pdf'))\n",
    "\n",
    "\n",
    "    print(\"Long Only\")\n",
    "    backtest, weights, scaled_backtest = bt.run_backtest(returns, lookback_backtest, input_classifiers= [\"Random Forest\"], inputs = feature_set, outputs = binary_returns,  ensemble = 'no', trade_type = 'long')\n",
    "    results, fm_sum = bt.factor_results(backtest, ff_factors_compare)\n",
    "    results_capm, fm_sum_capm = bt.factor_results_capm(backtest, ff_factors_compare)\n",
    "    results_umd, fm_sum_umd = bt.factor_results_umd(backtest, umd)\n",
    "    results_q, fm_sum_q = bt.factor_results_qfactor(backtest, q_factor)\n",
    "    results_tm = bt.treynor_mazuy(backtest, ff_factors_compare)\n",
    "    results_hm = bt.henriksson_merton(backtest, ff_factors_compare)\n",
    "\n",
    "    results_scaled, fm_sum = bt.factor_results(scaled_backtest, ff_factors_compare)\n",
    "    results_capm_scaled, fm_sum_capm = bt.factor_results_capm(scaled_backtest, ff_factors_compare)\n",
    "    results_umd_scaled, fm_sum_umd = bt.factor_results_umd(scaled_backtest, umd)\n",
    "    results_q_scaled, fm_sum_q = bt.factor_results_qfactor(scaled_backtest, q_factor)\n",
    "    results_tm_scaled = bt.treynor_mazuy(scaled_backtest, ff_factors_compare)\n",
    "    results_hm_scaled = bt.henriksson_merton(scaled_backtest, ff_factors_compare)\n",
    "\n",
    "    output_reg.append(results)\n",
    "    output_capm.append(results_capm)\n",
    "    output_umd.append(results_umd)\n",
    "    output_qfactor.append(results_q)\n",
    "    output_tm.append(results_tm)\n",
    "    output_hm.append(results_hm)\n",
    "\n",
    "    output_reg_scaled.append(results_scaled)\n",
    "    output_capm_scaled.append(results_capm_scaled)\n",
    "    output_umd_scaled.append(results_umd_scaled)\n",
    "    output_qfactor_scaled.append(results_q_scaled)\n",
    "    output_tm_scaled.append(results_tm_scaled)\n",
    "    output_hm_scaled.append(results_hm_scaled)\n",
    "\n",
    "    backtest.to_csv(os.path.join(backtest_write, data_type + '_RF_' + 'l' + '.csv'))\n",
    "    backtest.to_csv(os.path.join(backtest_write, data_type + '_RF_scaled_' + 'l' + '.csv'))\n",
    "    plot_bf = pd.DataFrame(1.0 + backtest.mean(axis = 1), columns = ['backtest']).cumprod()\n",
    "    plot_mkt = (1.0 + ff_factors['Mkt-RF'].loc[plot_bf.index]).cumprod()\n",
    "    plot_df = pd.concat([plot_bf, plot_mkt], axis = 1).dropna()\n",
    "    ax = plot_df.plot()\n",
    "    ax.figure.savefig(os.path.join(image_write, data_type + '_RF_' + 'l' + '.pdf'))\n",
    "\n",
    "\n",
    "    print(\"Short Only\")\n",
    "    backtest, weights, scaled_backtest = bt.run_backtest(returns, lookback_backtest, input_classifiers= [\"Random Forest\"], inputs = feature_set, outputs = binary_returns,  ensemble = 'no', trade_type = 'short')\n",
    "    results, fm_sum = bt.factor_results(backtest, ff_factors_compare)\n",
    "    results_capm, fm_sum_capm = bt.factor_results_capm(backtest, ff_factors_compare)\n",
    "    results_umd, fm_sum_umd = bt.factor_results_umd(backtest, umd)\n",
    "    results_q, fm_sum_q = bt.factor_results_qfactor(backtest, q_factor)\n",
    "    results_tm = bt.treynor_mazuy(backtest, ff_factors_compare)\n",
    "    results_hm = bt.henriksson_merton(backtest, ff_factors_compare)\n",
    "\n",
    "    results_scaled, fm_sum = bt.factor_results(scaled_backtest, ff_factors_compare)\n",
    "    results_capm_scaled, fm_sum_capm = bt.factor_results_capm(scaled_backtest, ff_factors_compare)\n",
    "    results_umd_scaled, fm_sum_umd = bt.factor_results_umd(scaled_backtest, umd)\n",
    "    results_q_scaled, fm_sum_q = bt.factor_results_qfactor(scaled_backtest, q_factor)\n",
    "    results_tm_scaled = bt.treynor_mazuy(scaled_backtest, ff_factors_compare)\n",
    "    results_hm_scaled = bt.henriksson_merton(scaled_backtest, ff_factors_compare)\n",
    "\n",
    "    output_reg.append(results)\n",
    "    output_capm.append(results_capm)\n",
    "    output_umd.append(results_umd)\n",
    "    output_qfactor.append(results_q)\n",
    "    output_tm.append(results_tm)\n",
    "    output_hm.append(results_hm)\n",
    "\n",
    "    output_reg_scaled.append(results_scaled)\n",
    "    output_capm_scaled.append(results_capm_scaled)\n",
    "    output_umd_scaled.append(results_umd_scaled)\n",
    "    output_qfactor_scaled.append(results_q_scaled)\n",
    "    output_tm_scaled.append(results_tm_scaled)\n",
    "    output_hm_scaled.append(results_hm_scaled)\n",
    "\n",
    "    backtest.to_csv(os.path.join(backtest_write, data_type + '_RF_' + 's' + '.csv'))\n",
    "    scaled_backtest.to_csv(os.path.join(backtest_write, data_type + '_RF_scaled' + 's' + '.csv'))\n",
    "    plot_bf = pd.DataFrame(1.0 + backtest.mean(axis = 1), columns = ['backtest']).cumprod()\n",
    "    plot_mkt = (1.0 + ff_factors['Mkt-RF'].loc[plot_bf.index]).cumprod()\n",
    "    plot_df = pd.concat([plot_bf, plot_mkt], axis = 1).dropna()\n",
    "    ax = plot_df.plot()\n",
    "    ax.figure.savefig(os.path.join(image_write, data_type + '_RF_' + 's' + '.pdf'))\n",
    "\n",
    "        \n",
    "        \n",
    "    print(\"AdaBooost\")\n",
    "    print(\"Long and Short\")\n",
    "    backtest, weights, scaled_backtest = bt.run_backtest(returns, lookback_backtest, input_classifiers= [\"AdaBoost\"], inputs = feature_set, outputs = binary_returns,  ensemble = 'no', trade_type = 'both')\n",
    "    results, fm_sum = bt.factor_results(backtest, ff_factors_compare)\n",
    "    results_capm, fm_sum_capm = bt.factor_results_capm(backtest, ff_factors_compare)\n",
    "    results_umd, fm_sum_umd = bt.factor_results_umd(backtest, umd)\n",
    "    results_q, fm_sum_q = bt.factor_results_qfactor(backtest, q_factor)\n",
    "    results_tm = bt.treynor_mazuy(backtest, ff_factors_compare)\n",
    "    results_hm = bt.henriksson_merton(backtest, ff_factors_compare)\n",
    "\n",
    "    results_scaled, fm_sum = bt.factor_results(scaled_backtest, ff_factors_compare)\n",
    "    results_capm_scaled, fm_sum_capm = bt.factor_results_capm(scaled_backtest, ff_factors_compare)\n",
    "    results_umd_scaled, fm_sum_umd = bt.factor_results_umd(scaled_backtest, umd)\n",
    "    results_q_scaled, fm_sum_q = bt.factor_results_qfactor(scaled_backtest, q_factor)\n",
    "    results_tm_scaled = bt.treynor_mazuy(scaled_backtest, ff_factors_compare)\n",
    "    results_hm_scaled = bt.henriksson_merton(scaled_backtest, ff_factors_compare)\n",
    "\n",
    "    output_reg.append(results)\n",
    "    output_capm.append(results_capm)\n",
    "    output_umd.append(results_umd)\n",
    "    output_qfactor.append(results_q)\n",
    "    output_tm.append(results_tm)\n",
    "    output_hm.append(results_hm)\n",
    "\n",
    "    output_reg_scaled.append(results_scaled)\n",
    "    output_capm_scaled.append(results_capm_scaled)\n",
    "    output_umd_scaled.append(results_umd_scaled)\n",
    "    output_qfactor_scaled.append(results_q_scaled)\n",
    "    output_tm_scaled.append(results_tm_scaled)\n",
    "    output_hm_scaled.append(results_hm_scaled)\n",
    "\n",
    "    backtest.to_csv(os.path.join(backtest_write, data_type + '_AdaBoost_' + 'ls' + '.csv'))\n",
    "    scaled_backtest.to_csv(os.path.join(backtest_write, data_type + '_AdaBoost_scaled_' + 'ls' + '.csv'))\n",
    "    plot_bf = pd.DataFrame(1.0 + backtest.mean(axis = 1), columns = ['backtest']).cumprod()\n",
    "    plot_mkt = (1.0 + ff_factors['Mkt-RF'].loc[plot_bf.index]).cumprod()\n",
    "    plot_df = pd.concat([plot_bf, plot_mkt], axis = 1).dropna()\n",
    "    ax = plot_df.plot()\n",
    "    ax.figure.savefig(os.path.join(image_write, data_type + '_AdaBoost_' + 'ls' + '.pdf'))\n",
    "\n",
    "\n",
    "    print(\"Long Only\")\n",
    "    backtest, weights, scaled_backtest = bt.run_backtest(returns, lookback_backtest, input_classifiers= [\"AdaBoost\"], inputs = feature_set, outputs = binary_returns,  ensemble = 'no', trade_type = 'long')\n",
    "    results, fm_sum = bt.factor_results(backtest, ff_factors_compare)\n",
    "    results_capm, fm_sum_capm = bt.factor_results_capm(backtest, ff_factors_compare)\n",
    "    results_umd, fm_sum_umd = bt.factor_results_umd(backtest, umd)\n",
    "    results_q, fm_sum_q = bt.factor_results_qfactor(backtest, q_factor)\n",
    "    results_tm = bt.treynor_mazuy(backtest, ff_factors_compare)\n",
    "    results_hm = bt.henriksson_merton(backtest, ff_factors_compare)\n",
    "\n",
    "    results_scaled, fm_sum = bt.factor_results(scaled_backtest, ff_factors_compare)\n",
    "    results_capm_scaled, fm_sum_capm = bt.factor_results_capm(scaled_backtest, ff_factors_compare)\n",
    "    results_umd_scaled, fm_sum_umd = bt.factor_results_umd(scaled_backtest, umd)\n",
    "    results_q_scaled, fm_sum_q = bt.factor_results_qfactor(scaled_backtest, q_factor)\n",
    "    results_tm_scaled = bt.treynor_mazuy(scaled_backtest, ff_factors_compare)\n",
    "    results_hm_scaled = bt.henriksson_merton(scaled_backtest, ff_factors_compare)\n",
    "\n",
    "    output_reg.append(results)\n",
    "    output_capm.append(results_capm)\n",
    "    output_umd.append(results_umd)\n",
    "    output_qfactor.append(results_q)\n",
    "    output_tm.append(results_tm)\n",
    "    output_hm.append(results_hm)\n",
    "\n",
    "    output_reg_scaled.append(results_scaled)\n",
    "    output_capm_scaled.append(results_capm_scaled)\n",
    "    output_umd_scaled.append(results_umd_scaled)\n",
    "    output_qfactor_scaled.append(results_q_scaled)\n",
    "    output_tm_scaled.append(results_tm_scaled)\n",
    "    output_hm_scaled.append(results_hm_scaled)\n",
    "\n",
    "    backtest.to_csv(os.path.join(backtest_write, data_type + '_AdaBoost_' + 'l' + '.csv'))\n",
    "    backtest.to_csv(os.path.join(backtest_write, data_type + '_AdaBoost_scaled_' + 'l' + '.csv'))\n",
    "    plot_bf = pd.DataFrame(1.0 + backtest.mean(axis = 1), columns = ['backtest']).cumprod()\n",
    "    plot_mkt = (1.0 + ff_factors['Mkt-RF'].loc[plot_bf.index]).cumprod()\n",
    "    plot_df = pd.concat([plot_bf, plot_mkt], axis = 1).dropna()\n",
    "    ax = plot_df.plot()\n",
    "    ax.figure.savefig(os.path.join(image_write, data_type + '_AdaBoost_' + 'l' + '.pdf'))\n",
    "\n",
    "\n",
    "    print(\"Short Only\")\n",
    "    backtest, weights, scaled_backtest = bt.run_backtest(returns, lookback_backtest, input_classifiers= [\"AdaBoost\"], inputs = feature_set, outputs = binary_returns,  ensemble = 'no', trade_type = 'short')\n",
    "    results, fm_sum = bt.factor_results(backtest, ff_factors_compare)\n",
    "    results_capm, fm_sum_capm = bt.factor_results_capm(backtest, ff_factors_compare)\n",
    "    results_umd, fm_sum_umd = bt.factor_results_umd(backtest, umd)\n",
    "    results_q, fm_sum_q = bt.factor_results_qfactor(backtest, q_factor)\n",
    "    results_tm = bt.treynor_mazuy(backtest, ff_factors_compare)\n",
    "    results_hm = bt.henriksson_merton(backtest, ff_factors_compare)\n",
    "\n",
    "    results_scaled, fm_sum = bt.factor_results(scaled_backtest, ff_factors_compare)\n",
    "    results_capm_scaled, fm_sum_capm = bt.factor_results_capm(scaled_backtest, ff_factors_compare)\n",
    "    results_umd_scaled, fm_sum_umd = bt.factor_results_umd(scaled_backtest, umd)\n",
    "    results_q_scaled, fm_sum_q = bt.factor_results_qfactor(scaled_backtest, q_factor)\n",
    "    results_tm_scaled = bt.treynor_mazuy(scaled_backtest, ff_factors_compare)\n",
    "    results_hm_scaled = bt.henriksson_merton(scaled_backtest, ff_factors_compare)\n",
    "\n",
    "    output_reg.append(results)\n",
    "    output_capm.append(results_capm)\n",
    "    output_umd.append(results_umd)\n",
    "    output_qfactor.append(results_q)\n",
    "    output_tm.append(results_tm)\n",
    "    output_hm.append(results_hm)\n",
    "\n",
    "    output_reg_scaled.append(results_scaled)\n",
    "    output_capm_scaled.append(results_capm_scaled)\n",
    "    output_umd_scaled.append(results_umd_scaled)\n",
    "    output_qfactor_scaled.append(results_q_scaled)\n",
    "    output_tm_scaled.append(results_tm_scaled)\n",
    "    output_hm_scaled.append(results_hm_scaled)\n",
    "\n",
    "    backtest.to_csv(os.path.join(backtest_write, data_type + '_AdaBoost_' + 's' + '.csv'))\n",
    "    scaled_backtest.to_csv(os.path.join(backtest_write, data_type + '_AdaBoost_scaled' + 's' + '.csv'))\n",
    "    plot_bf = pd.DataFrame(1.0 + backtest.mean(axis = 1), columns = ['backtest']).cumprod()\n",
    "    plot_mkt = (1.0 + ff_factors['Mkt-RF'].loc[plot_bf.index]).cumprod()\n",
    "    plot_df = pd.concat([plot_bf, plot_mkt], axis = 1).dropna()\n",
    "    ax = plot_df.plot()\n",
    "    ax.figure.savefig(os.path.join(image_write, data_type + '_AdaBoost_' + 's' + '.pdf'))\n",
    "\n",
    "\n",
    "    print(\"Ensemble System\")\n",
    "    backtest, weights, scaled_backtest = bt.run_backtest(returns, lookback_backtest, input_classifiers= [\"Voter\"], inputs = feature_set, outputs = binary_returns,  ensemble = 'no')  \n",
    "    results, fm_sum = bt.factor_results(backtest, ff_factors_compare)\n",
    "    results_capm, fm_sum_capm = bt.factor_results_capm(backtest, ff_factors_compare)\n",
    "    results_umd, fm_sum_umd = bt.factor_results_umd(backtest, umd)\n",
    "    results_q, fm_sum_q = bt.factor_results_qfactor(backtest, q_factor)\n",
    "    results_tm = bt.treynor_mazuy(backtest, ff_factors_compare)\n",
    "    results_hm = bt.henriksson_merton(backtest, ff_factors_compare)\n",
    "\n",
    "    results_scaled, fm_sum = bt.factor_results(scaled_backtest, ff_factors_compare)\n",
    "    results_capm_scaled, fm_sum_capm = bt.factor_results_capm(scaled_backtest, ff_factors_compare)\n",
    "    results_umd_scaled, fm_sum_umd = bt.factor_results_umd(scaled_backtest, umd)\n",
    "    results_q_scaled, fm_sum_q = bt.factor_results_qfactor(scaled_backtest, q_factor)\n",
    "    results_tm_scaled = bt.treynor_mazuy(scaled_backtest, ff_factors_compare)\n",
    "    results_hm_scaled = bt.henriksson_merton(scaled_backtest, ff_factors_compare)\n",
    "\n",
    "    output_reg.append(results)\n",
    "    output_capm.append(results_capm)\n",
    "    output_umd.append(results_umd)\n",
    "    output_qfactor.append(results_q)\n",
    "    output_tm.append(results_tm)\n",
    "    output_hm.append(results_hm)\n",
    "\n",
    "    output_reg_scaled.append(results_scaled)\n",
    "    output_capm_scaled.append(results_capm_scaled)\n",
    "    output_umd_scaled.append(results_umd_scaled)\n",
    "    output_qfactor_scaled.append(results_q_scaled)\n",
    "    output_tm_scaled.append(results_tm_scaled)\n",
    "    output_hm_scaled.append(results_hm_scaled)\n",
    "\n",
    "    backtest.to_csv(os.path.join(backtest_write, data_type + '_ensemble_' + 'ls' + '.csv'))\n",
    "    scaled_backtest.to_csv(os.path.join(backtest_write, data_type + '_ensemble_scaled_' + 'ls' + '.csv'))\n",
    "    plot_bf = pd.DataFrame(1.0 + backtest.mean(axis = 1), columns = ['backtest']).cumprod()\n",
    "    plot_mkt = (1.0 + ff_factors['Mkt-RF'].loc[plot_bf.index]).cumprod()\n",
    "    plot_df = pd.concat([plot_bf, plot_mkt], axis = 1).dropna()\n",
    "    ax = plot_df.plot()\n",
    "    ax.figure.savefig(os.path.join(image_write, data_type + '_ensemble_' + 'ls' + '.pdf'))\n",
    "\n",
    "\n",
    "    print(\"Simple Binary Decision\")\n",
    "    scaled_backtest = returns.dropna(axis = 1, how = 'all').dropna(axis = 0, how = 'all')\n",
    "    weights = returns.dropna(axis = 1, how = 'all').dropna(axis = 0, how = 'all')\n",
    "    backtest.values[:] = 0\n",
    "    scaled_backtest.values[:] = 0\n",
    "    weights.values[:] = 0\n",
    "    signal = returns.rolling(lookback_backtest).mean().shift(1).dropna(axis = 1, how = 'all').dropna(axis = 0, how = 'all')\n",
    "    std = returns.rolling(lookback_backtest).std().shift(1).dropna(axis = 1, how = 'all').dropna(axis = 0, how = 'all')\n",
    "    std = .05/std\n",
    "    signal[signal >= 0] = 1.0\n",
    "    signal[signal < 0] = -1.0\n",
    "    backtest = (signal * returns).dropna()\n",
    "    weights = signal\n",
    "    scaled_backtest = (signal * returns * std).dropna()\n",
    "\n",
    "\n",
    "\n",
    "    results, fm_sum = bt.factor_results(backtest, ff_factors_compare)\n",
    "    results_capm, fm_sum_capm = bt.factor_results_capm(backtest, ff_factors_compare)\n",
    "    results_umd, fm_sum_umd = bt.factor_results_umd(backtest, umd)\n",
    "    results_q, fm_sum_q = bt.factor_results_qfactor(backtest, q_factor)\n",
    "    results_tm = bt.treynor_mazuy(backtest, ff_factors_compare)\n",
    "    results_hm = bt.henriksson_merton(backtest, ff_factors_compare)\n",
    "\n",
    "    results_scaled, fm_sum = bt.factor_results(scaled_backtest, ff_factors_compare)\n",
    "    results_capm_scaled, fm_sum_capm = bt.factor_results_capm(scaled_backtest, ff_factors_compare)\n",
    "    results_umd_scaled, fm_sum_umd = bt.factor_results_umd(scaled_backtest, umd)\n",
    "    results_q_scaled, fm_sum_q = bt.factor_results_qfactor(scaled_backtest, q_factor)\n",
    "    results_tm_scaled = bt.treynor_mazuy(scaled_backtest, ff_factors_compare)\n",
    "    results_hm_scaled = bt.henriksson_merton(scaled_backtest, ff_factors_compare)\n",
    "\n",
    "    output_reg.append(results)\n",
    "    output_capm.append(results_capm)\n",
    "    output_umd.append(results_umd)\n",
    "    output_qfactor.append(results_q)\n",
    "    output_tm.append(results_tm)\n",
    "    output_hm.append(results_hm)\n",
    "\n",
    "    output_reg_scaled.append(results_scaled)\n",
    "    output_capm_scaled.append(results_capm_scaled)\n",
    "    output_umd_scaled.append(results_umd_scaled)\n",
    "    output_qfactor_scaled.append(results_q_scaled)\n",
    "    output_tm_scaled.append(results_tm_scaled)\n",
    "    output_hm_scaled.append(results_hm_scaled)\n",
    "\n",
    "\n",
    "    backtest.to_csv(os.path.join(backtest_write, data_type + '_simp_binary_' + 'ls' + '.csv'))\n",
    "    scaled_backtest.to_csv(os.path.join(backtest_write, data_type + '_simp_binary_scaled_' '_' + 'ls' + '.csv'))\n",
    "    plot_bf = pd.DataFrame(1.0 + backtest.mean(axis = 1), columns = ['backtest']).cumprod()\n",
    "    plot_mkt = (1.0 + ff_factors['Mkt-RF'].loc[plot_bf.index]).cumprod()\n",
    "    plot_df = pd.concat([plot_bf, plot_mkt], axis = 1).dropna()\n",
    "    ax = plot_df.plot()\n",
    "    ax.figure.savefig(os.path.join(image_write, data_type + '_sim_binary_scaled_' + 'ls' + '.pdf'))\n",
    "\n",
    "    plt.close('all')\n",
    "    \n",
    "    factor_results = summary_col(output_reg,stars=True,float_format='%0.3f',model_names=names)\n",
    "    factor_results.title = data_type\n",
    "    factor_results_capm = summary_col(output_capm,stars=True,float_format='%0.3f',model_names=names)\n",
    "    factor_results_capm.title = data_type + ' capm'\n",
    "    \n",
    "    factor_results_umd = summary_col(output_umd,stars=True,float_format='%0.3f',model_names=names)\n",
    "    factor_results_umd.title = data_type + ' umd'\n",
    "    \n",
    "    factor_results_q = summary_col(output_qfactor,stars=True,float_format='%0.3f',model_names=names)\n",
    "    factor_results_q.title = data_type + ' q factor'\n",
    "                                                        \n",
    "    factor_results_tm = summary_col(output_tm,stars=True,float_format='%0.3f',model_names=names)\n",
    "    factor_results_tm.title = data_type + ' Treynor and Mazuy'\n",
    "                               \n",
    "    factor_results_hm = summary_col(output_hm,stars=True,float_format='%0.3f',model_names=names)\n",
    "    factor_results_hm.title = data_type + ' Henriksson and Merton'\n",
    "    \n",
    "    resultFile = open(os.path.join(backtest_wd,'outputs' ,outname),'a')\n",
    "    resultFile.write(factor_results.as_text())\n",
    "    resultFile.write('\\n')\n",
    "    resultFile.write(factor_results_capm.as_text())\n",
    "    resultFile.write('\\n')\n",
    "    resultFile.write(factor_results_umd.as_text())\n",
    "    resultFile.write('\\n')\n",
    "    resultFile.write(factor_results_q.as_text())\n",
    "    resultFile.write('\\n')\n",
    "    resultFile.write(factor_results_tm.as_text())\n",
    "    resultFile.write('\\n')\n",
    "    resultFile.write(factor_results_hm.as_text())\n",
    "    resultFile.write('\\n')\n",
    "    resultFile.close()\n",
    "    \n",
    "    resultFile = open(os.path.join(backtest_wd,'outputs' ,outnamelatex),'a')\n",
    "    resultFile.write(factor_results.as_latex())\n",
    "    resultFile.write('\\n')\n",
    "    resultFile.write(factor_results_capm.as_latex())\n",
    "    resultFile.write('\\n')\n",
    "    resultFile.write(factor_results_umd.as_latex())\n",
    "    resultFile.write('\\n')\n",
    "    resultFile.write(factor_results_q.as_latex())\n",
    "    resultFile.write('\\n')\n",
    "    resultFile.write(factor_results_tm.as_latex())\n",
    "    resultFile.write('\\n')\n",
    "    resultFile.write(factor_results_hm.as_latex())\n",
    "    resultFile.write('\\n')\n",
    "    resultFile.close()\n",
    "    \n",
    "    add_factor = pd.DataFrame(factor_results.tables[0]).T['intercept']\n",
    "    add_factor.columns = [data_type + '_' +  str(lookback) + '_' + str(lookback_backtest) ]\n",
    "    add_factor.name = data_type + '_' +  str(lookback) + '_' + str(lookback_backtest) \n",
    "    add_factor_capm = pd.DataFrame(factor_results_capm.tables[0]).T['intercept']\n",
    "    add_factor_capm.columns = [data_type + '_' + str(lookback) + '_' + str(lookback_backtest) ]\n",
    "    add_factor_capm.name = data_type + '_' + str(lookback) + '_' + str(lookback_backtest)\n",
    "                               \n",
    "    factor_results_scaled = summary_col(output_reg_scaled,stars=True,float_format='%0.3f',model_names=names)\n",
    "    factor_results_scaled.title = data_type\n",
    "                               \n",
    "    factor_results_capm_scaled = summary_col(output_capm_scaled,stars=True,float_format='%0.3f',model_names=names)\n",
    "    factor_results_capm_scaled.title = data_type + ' capm'\n",
    "    \n",
    "    factor_results_umd_scaled = summary_col(output_umd_scaled,stars=True,float_format='%0.3f',model_names=names)\n",
    "    factor_results_umd_scaled.title = data_type + ' umd'\n",
    "    \n",
    "    factor_results_q_scaled = summary_col(output_qfactor_scaled,stars=True,float_format='%0.3f',model_names=names)\n",
    "    factor_results_q_scaled.title = data_type + ' q factor'\n",
    "                                                        \n",
    "    factor_results_tm_scaled = summary_col(output_tm_scaled,stars=True,float_format='%0.3f',model_names=names)\n",
    "    factor_results_tm_scaled.title = data_type + ' Treynor and Mazuy'\n",
    "                    \n",
    "    factor_results_hm_scaled = summary_col(output_hm_scaled,stars=True,float_format='%0.3f',model_names=names)\n",
    "    factor_results_hm_scaled.title = data_type + ' Henriksson and Merton'\n",
    "    \n",
    "    resultFile = open(os.path.join(backtest_wd,'outputs' ,outname_scaled),'a')\n",
    "    resultFile.write(factor_results_scaled.as_text())\n",
    "    resultFile.write('\\n')\n",
    "    resultFile.write(factor_results_capm_scaled.as_text())\n",
    "    resultFile.write('\\n')\n",
    "    resultFile.write(factor_results_umd_scaled.as_text())\n",
    "    resultFile.write('\\n')\n",
    "    resultFile.write(factor_results_q_scaled.as_text())\n",
    "    resultFile.write('\\n')\n",
    "    resultFile.write(factor_results_tm_scaled.as_text())\n",
    "    resultFile.write('\\n')\n",
    "    resultFile.write(factor_results_hm_scaled.as_text())\n",
    "    resultFile.write('\\n')\n",
    "    resultFile.close()\n",
    "    \n",
    "    resultFile = open(os.path.join(backtest_wd,'outputs' ,outnamelatex_scaled),'a')\n",
    "    resultFile.write(factor_results_scaled.as_latex())\n",
    "    resultFile.write('\\n')\n",
    "    resultFile.write(factor_results_capm_scaled.as_latex())\n",
    "    resultFile.write('\\n')\n",
    "    resultFile.write(factor_results_umd_scaled.as_latex())\n",
    "    resultFile.write('\\n')\n",
    "    resultFile.write(factor_results_q_scaled.as_latex())\n",
    "    resultFile.write('\\n')\n",
    "    resultFile.write(factor_results_tm_scaled.as_latex())\n",
    "    resultFile.write('\\n')\n",
    "    resultFile.write(factor_results_hm_scaled.as_latex())\n",
    "    resultFile.write('\\n')\n",
    "    resultFile.close()\n",
    "    \n",
    "    add_factor_scaled = pd.DataFrame(factor_results_scaled.tables[0]).T['intercept']\n",
    "    add_factor_scaled.columns = [data_type + '_' +  str(lookback) + '_' + str(lookback_backtest) ]\n",
    "    add_factor_scaled.name = data_type + '_' +  str(lookback) + '_' + str(lookback_backtest) \n",
    "    add_factor_capm_scaled = pd.DataFrame(factor_results_capm_scaled.tables[0]).T['intercept']\n",
    "    add_factor_capm_scaled.columns = [data_type + '_' + str(lookback) + '_' + str(lookback_backtest) ]\n",
    "    add_factor_capm_scaled.name = data_type + '_' + str(lookback) + '_' + str(lookback_backtest)\n",
    "       \n",
    "                    \n",
    "    \n",
    "    return add_factor, add_factor_capm, factor_results, add_factor_scaled, add_factor_capm_scaled\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the famma french factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_factors = pd.read_csv('F-F_Research_Data_5_Factors_2x3.csv', index_col = 0)\n",
    "ff_factors.index = pd.to_datetime(ff_factors.index, format= '%Y%m')\n",
    "ff_factors = ff_factors / 100.0\n",
    "ff_factors[ff_factors == -999] = np.NaN\n",
    "ff_factors[ff_factors == -99.9] = np.NaN\n",
    "ff_factors = ff_factors.dropna(how = 'any', axis = 0)\n",
    "ff_factors = ff_factors[['Mkt-RF', 'SMB', 'HML', 'RMW', 'CMA']]\n",
    "ff_factors_compare = ff_factors.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the momentum model\n",
    "mom = pd.read_csv('Mom.csv', index_col = 0)\n",
    "mom.index = pd.to_datetime(mom.index, format= '%Y%m')\n",
    "mom = mom / 100.0\n",
    "mom[mom == -999] = np.NaN\n",
    "mom[mom == -99.9] = np.NaN\n",
    "mom = mom.dropna(how = 'any', axis = 0)\n",
    "umd = pd.concat([ff_factors[['Mkt-RF', 'SMB', 'HML']], mom], axis = 1)\n",
    "umd = umd.dropna()\n",
    "cols_add = []\n",
    "for c in umd.columns:\n",
    "    cols_add.append(c.replace(' ', ''))\n",
    "umd.columns  = cols_add\n",
    "print(umd.head(5))\n",
    "print(umd.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_factor = pd.read_csv('q5_factors.csv', index_col = 0)\n",
    "q_factor.index = pd.to_datetime(q_factor.index, format= '%Y%m')\n",
    "q_factor = q_factor / 100.0\n",
    "print(q_factor.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "industry = pd.read_csv('48_Industry_Portfolios.csv', index_col = 0)\n",
    "industry.index = pd.to_datetime(industry.index, format= '%Y%m')\n",
    "industry[industry <= -99.] = np.NaN\n",
    "industry = industry / 100.0\n",
    "\n",
    "ff_factors = pd.read_csv('F-F_Research_Data_5_Factors_2x3.csv', index_col = 0)\n",
    "ff_factors.index = pd.to_datetime(ff_factors.index, format= '%Y%m')\n",
    "ff_factors = ff_factors / 100.0\n",
    "ff_factors[ff_factors == -999] = np.NaN\n",
    "ff_factors[ff_factors == -99.9] = np.NaN\n",
    "ff_factors = ff_factors.dropna(how = 'any', axis = 0)\n",
    "\n",
    "factor_model = ff_factors.copy().dropna(axis = 0, how = 'any')\n",
    "rf = factor_model[['RF']]\n",
    "rf_subtract = rf.copy()\n",
    "factor_model = factor_model[['Mkt-RF', 'SMB', 'HML', 'RMW', 'CMA']]\n",
    "ff_factors_compare = factor_model.copy()\n",
    "\n",
    "rf_subtract.index = pd.to_datetime(rf_subtract.index, format = 'Y%-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get policy uncertainty\n",
    "policy_uncertainty = pd.read_csv('US_Policy_Uncertainty_Data.csv', index_col = 0)\n",
    "policy_uncertainty.index = pd.to_datetime(policy_uncertainty.index, format= '%Y%m')\n",
    "print(policy_uncertainty.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookback_list = [24,12]\n",
    "lookback_backtest_list = [12,24,60]\n",
    "\n",
    "for lookback in lookback_list:\n",
    "    for lookback_backtest in lookback_backtest_list:\n",
    "\n",
    "        if not os.path.exists(os.path.join(image_wd, str(lookback) + '_' + str(lookback_backtest))):\n",
    "            os.makedirs(os.path.join(image_wd, str(lookback) + '_' + str(lookback_backtest)))\n",
    "\n",
    "        image_write = os.path.join(image_wd, str(lookback) + '_' + str(lookback_backtest))\n",
    "\n",
    "        if not os.path.exists(os.path.join(backtest_wd, str(lookback) + '_' + str(lookback_backtest))):\n",
    "            os.makedirs(os.path.join(backtest_wd, str(lookback) + '_' + str(lookback_backtest)))\n",
    "        if not os.path.exists(os.path.join(backtest_wd, 'outputs')):\n",
    "            os.makedirs(os.path.join(backtest_wd, 'outputs'))\n",
    "\n",
    "        outname = 'output_results_' + str(lookback) + '_' + str(lookback_backtest) +  '.txt'\n",
    "        outnamelatex = 'output_results_latex_' + str(lookback) + '_' + str(lookback_backtest) +  '.txt'\n",
    "    \n",
    "        backtest_write = os.path.join(backtest_wd, str(lookback) + '_' + str(lookback_backtest))\n",
    "\n",
    "        resultFile = open(os.path.join(backtest_wd,'outputs' ,outname),'w')\n",
    "        resultFile.close()\n",
    "        \n",
    "        resultFile = open(os.path.join(backtest_wd,'outputs' ,outnamelatex),'w')\n",
    "        resultFile.close()\n",
    "        \n",
    "        outname = 'output_results_scaled_' + str(lookback) + '_' + str(lookback_backtest) +  '.txt'\n",
    "        outnamelatex = 'output_results_latex_scaled_' + str(lookback) + '_' + str(lookback_backtest) +  '.txt'\n",
    "    \n",
    "        backtest_write = os.path.join(backtest_wd, str(lookback) + '_' + str(lookback_backtest))\n",
    "\n",
    "        resultFile = open(os.path.join(backtest_wd,'outputs' ,outname),'w')\n",
    "        resultFile.close()\n",
    "        \n",
    "        resultFile = open(os.path.join(backtest_wd,'outputs' ,outnamelatex),'w')\n",
    "        resultFile.close()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        ####################################################\n",
    "        ####################################################\n",
    "        ####################################################\n",
    "        #get the q factor model\n",
    "        ####################################################\n",
    "        ####################################################\n",
    "        ####################################################\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        q_factor = pd.read_csv('q5_factors.csv', index_col = 0)\n",
    "        q_factor.index = pd.to_datetime(q_factor.index, format= '%Y%m')\n",
    "        q_factor = q_factor / 100.0\n",
    "\n",
    "        factor_model = q_factor.copy()\n",
    "        factor_model = factor_model[['R_MKT', 'R_ME', 'R_IA', 'R_ROE', 'R_EG']]\n",
    "        rf = q_factor[['R_F']]\n",
    "        q_factor = factor_model.copy()\n",
    "\n",
    "        index_symbol = 'R_MKT'\n",
    "\n",
    "        print(q_factor.head(5))\n",
    "        \n",
    "        #start date for data\n",
    "        start = '2004-01-01'\n",
    "        #end date for data\n",
    "        end = '2020-01-01'\n",
    "\n",
    "\n",
    "        #control the leverage\n",
    "        lev = 1.\n",
    "        stk_symbols  = factor_model.columns\n",
    "        returns = factor_model.copy()\n",
    "\n",
    "\n",
    "        #set the backtest parameters\n",
    "        bp = BacktestParameters(stk_symbols, start, end, lookback, lev  )\n",
    "        bt = Backtest(bp, returns, mode = 'insample')\n",
    "        data = Data(bp)\n",
    "\n",
    "        df_alpha, idiosyncratic_vol, binary_alpha = data.get_alpha(returns, factor_model, rf)\n",
    "        rolling_mean_12, rolling_mean_6, rolling_std_12, binary_mean_12,binary_mean_6,binary_returns, rolling_sum_12s = data.get_rolling_features(returns, 0.0)\n",
    "\n",
    "\n",
    "        #creates a dictionary of different features for analysis based on ticker keys\n",
    "        feature_list = [binary_alpha, idiosyncratic_vol,rolling_mean_12, rolling_std_12]\n",
    "\n",
    "        feature_set = data.get_features(feature_list)\n",
    "        for k in feature_set.keys():\n",
    "            feature_set[k] = pd.concat([feature_set[k], policy_uncertainty],axis = 1)\n",
    "            feature_set[k] =  feature_set[k][(feature_set[k].index > binary_alpha.index.min()) & (feature_set[k].index < binary_alpha.index.max())]\n",
    "        \n",
    "        data_type = 'q_factor'\n",
    "\n",
    "        factor_results, factor_results_capm, factor_reg,factor_results_scaled, factor_results_capm_scaled = write_outputs(bt, ff_factors_compare, umd, q_factor, returns, lookback_backtest, feature_set, binary_returns,backtest_write, image_write, cur_wd, data_type )\n",
    "        \n",
    "        if alpha_ff5.empty:\n",
    "            alpha_ff5 = factor_results.copy()\n",
    "            alpha_capm = factor_results_capm.copy()\n",
    "        else:\n",
    "            alpha_ff5 = pd.concat( [alpha_ff5, factor_results], axis = 1)\n",
    "            alpha_capm = pd.concat( [alpha_capm, factor_results_capm], axis = 1)\n",
    "            \n",
    "        if alpha_ff5_scaled.empty:\n",
    "            alpha_ff5_scaled = factor_results_scaled.copy()\n",
    "            alpha_capm_scaled = factor_results_capm_scaled.copy()\n",
    "        else:\n",
    "            alpha_ff5_scaled = pd.concat( [alpha_ff5_scaled, factor_results_scaled], axis = 1)\n",
    "            alpha_capm_scaled = pd.concat( [alpha_capm_scaled, factor_results_capm_scaled], axis = 1)      \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        ####################################################\n",
    "        ####################################################    \n",
    "        #get the ff5 results\n",
    "        ####################################################\n",
    "        ####################################################\n",
    "        ####################################################\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        ff_factors = pd.read_csv('F-F_Research_Data_5_Factors_2x3.csv', index_col = 0)\n",
    "        ff_factors.index = pd.to_datetime(ff_factors.index, format= '%Y%m')\n",
    "        ff_factors = ff_factors / 100.0\n",
    "        ff_factors[ff_factors == -999] = np.NaN\n",
    "        ff_factors[ff_factors == -99.9] = np.NaN\n",
    "        ff_factors = ff_factors.dropna(how = 'any', axis = 0)\n",
    "\n",
    "        factor_model = ff_factors.copy()\n",
    "        factor_model = factor_model[['Mkt-RF', 'SMB', 'HML', 'RMW', 'CMA']]\n",
    "        rf = ff_factors[['RF']]\n",
    "\n",
    "        index_symbol = 'Mkt-RF'\n",
    "\n",
    "        print(ff_factors.head(5))\n",
    "\n",
    "        #start date for data\n",
    "        start = '2004-01-01'\n",
    "        #end date for data\n",
    "        end = '2020-01-01'\n",
    "\n",
    "        #control the leverage\n",
    "        lev = 1.\n",
    "        stk_symbols  = factor_model.columns\n",
    "        returns = factor_model.copy()\n",
    "\n",
    "\n",
    "        #set the backtest parameters\n",
    "        bp = BacktestParameters(stk_symbols, start, end, lookback, lev  )\n",
    "        bt = Backtest(bp, returns, mode = 'insample')\n",
    "        data = Data(bp)\n",
    "\n",
    "        df_alpha, idiosyncratic_vol, binary_alpha = data.get_alpha(returns, factor_model, rf)\n",
    "        rolling_mean_12, rolling_mean_6, rolling_std_12, binary_mean_12,binary_mean_6,binary_returns, rolling_sum_12s = data.get_rolling_features(returns, 0.0)\n",
    "\n",
    "\n",
    "        #creates a dictionary of different features for analysis based on ticker keys\n",
    "        feature_list = [binary_alpha, binary_returns]\n",
    "        \n",
    "        feature_set = data.get_features(feature_list)\n",
    "        \n",
    "\n",
    " \n",
    "        for k in feature_set.keys():\n",
    "            feature_set[k] = pd.concat([feature_set[k], policy_uncertainty],axis = 1)\n",
    "            feature_set[k] = feature_set[k][(feature_set[k].index > binary_alpha.index.min()) & (feature_set[k].index < binary_alpha.index.max())]\n",
    "               \n",
    "                \n",
    "        data_type = 'ff5'\n",
    "        \n",
    "        factor_results, factor_results_capm, factor_reg,factor_results_scaled, factor_results_capm_scaled = write_outputs(bt, ff_factors_compare, umd, q_factor, returns, lookback_backtest, feature_set, binary_returns,backtest_write, image_write, cur_wd, data_type )\n",
    "        \n",
    "        if alpha_ff5.empty:\n",
    "            alpha_ff5 = factor_results.copy()\n",
    "            alpha_capm = factor_results_capm.copy()\n",
    "        else:\n",
    "            alpha_ff5 = pd.concat( [alpha_ff5, factor_results], axis = 1)\n",
    "            alpha_capm = pd.concat( [alpha_capm, factor_results_capm], axis = 1)\n",
    "            \n",
    "        if alpha_ff5_scaled.empty:\n",
    "            alpha_ff5_scaled = factor_results_scaled.copy()\n",
    "            alpha_capm_scaled = factor_results_capm_scaled.copy()\n",
    "        else:\n",
    "            alpha_ff5_scaled = pd.concat( [alpha_ff5_scaled, factor_results_scaled], axis = 1)\n",
    "            alpha_capm_scaled = pd.concat( [alpha_capm_scaled, factor_results_capm_scaled], axis = 1)      \n",
    "        \n",
    "            \n",
    "        ####################################################\n",
    "        ####################################################\n",
    "        #Industry\n",
    "        ####################################################\n",
    "        ####################################################\n",
    "        ####################################################\n",
    "\n",
    "        industry = pd.read_csv('48_Industry_Portfolios.csv', index_col = 0)\n",
    "        industry.index = pd.to_datetime(industry.index, format= '%Y%m')\n",
    "        industry[industry <= -99.] = np.nan\n",
    "        industry = industry / 100.0\n",
    "\n",
    "        ff_factors = pd.read_csv('F-F_Research_Data_5_Factors_2x3.csv', index_col = 0)\n",
    "        ff_factors.index = pd.to_datetime(ff_factors.index, format= '%Y%m')\n",
    "        ff_factors = ff_factors / 100.0\n",
    "        ff_factors[ff_factors == -999] = np.NaN\n",
    "        ff_factors[ff_factors == -99.9] = np.NaN\n",
    "        ff_factors = ff_factors.dropna(how = 'any', axis = 0)\n",
    "\n",
    "        factor_model = ff_factors.copy().dropna(axis = 0, how = 'any')\n",
    "        rf = factor_model[['RF']]\n",
    "        rf_subtract = rf.copy()\n",
    "        factor_model = factor_model[['Mkt-RF', 'SMB', 'HML', 'RMW', 'CMA']]\n",
    "        ff_factors_compare = factor_model[['Mkt-RF']].copy().copy()\n",
    "\n",
    "        rf_subtract.index = pd.to_datetime(rf_subtract.index, format = 'Y%-%m-%d')\n",
    "\n",
    "        union_index = industry.index.intersection(rf_subtract.index)\n",
    "        industry = industry.loc[union_index,:]\n",
    "        rf_subtract = rf_subtract.loc[union_index, :]\n",
    "\n",
    "        for i in industry.columns:\n",
    "            industry.loc[:,i] = industry.loc[:,i] - rf_subtract['RF']\n",
    "\n",
    "        index_symbol = 'Mkt-RF'\n",
    "\n",
    "        print(ff_factors.head(5))\n",
    "\n",
    "        print(industry.head(5))\n",
    "\n",
    "\n",
    "        #start date for data\n",
    "        start = '2004-01-01'\n",
    "        #end date for data\n",
    "        end = '2020-01-01'\n",
    "\n",
    "        #control the leverage\n",
    "        lev = 1.\n",
    "        stk_symbols  = industry.columns\n",
    "        returns = industry.copy()\n",
    "\n",
    "\n",
    "        #set the backtest parameters\n",
    "        bp = BacktestParameters(stk_symbols, start, end, lookback, lev  )\n",
    "        bt = Backtest(bp, returns, mode = 'insample')\n",
    "        data = Data(bp)\n",
    "\n",
    "        df_alpha, idiosyncratic_vol, binary_alpha = data.get_alpha(returns, factor_model, rf)\n",
    "        rolling_mean_12, rolling_mean_6, rolling_std_12, binary_mean_12,binary_mean_6,binary_returns, rolling_sum_12s = data.get_rolling_features(returns, 0.0)\n",
    "\n",
    "\n",
    "        #creates a dictionary of different features for analysis based on ticker keys\n",
    "        feature_list = [binary_alpha, binary_returns]\n",
    "        \n",
    "        feature_set = data.get_features(feature_list)\n",
    "        \n",
    "\n",
    " \n",
    "        for k in feature_set.keys():\n",
    "            feature_set[k] = pd.concat([feature_set[k], policy_uncertainty],axis = 1)\n",
    "            feature_set[k] = feature_set[k][(feature_set[k].index > binary_alpha.index.min()) & (feature_set[k].index < binary_alpha.index.max())]\n",
    "               \n",
    "                \n",
    "        data_type = 'industry'\n",
    "\n",
    "        factor_results, factor_results_capm, factor_reg,factor_results_scaled, factor_results_capm_scaled = write_outputs(bt, ff_factors_compare, umd, q_factor, returns, lookback_backtest, feature_set, binary_returns,backtest_write, image_write, cur_wd, data_type )\n",
    "        \n",
    "        if alpha_ff5.empty:\n",
    "            alpha_ff5 = factor_results.copy()\n",
    "            alpha_capm = factor_results_capm.copy()\n",
    "        else:\n",
    "            alpha_ff5 = pd.concat( [alpha_ff5, factor_results], axis = 1)\n",
    "            alpha_capm = pd.concat( [alpha_capm, factor_results_capm], axis = 1)\n",
    "            \n",
    "        if alpha_ff5_scaled.empty:\n",
    "            alpha_ff5_scaled = factor_results_scaled.copy()\n",
    "            alpha_capm_scaled = factor_results_capm_scaled.copy()\n",
    "        else:\n",
    "            alpha_ff5_scaled = pd.concat( [alpha_ff5_scaled, factor_results_scaled], axis = 1)\n",
    "            alpha_capm_scaled = pd.concat( [alpha_capm_scaled, factor_results_capm_scaled], axis = 1)      \n",
    "   \n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "        ####################################################    \n",
    "        ####################################################    \n",
    "        #stambaugh_yuan model\n",
    "        ####################################################\n",
    "        ####################################################\n",
    "        ####################################################\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        m4 = pd.read_csv('M4-1.csv', index_col = 0)\n",
    "        m4.index = pd.to_datetime(m4.index, format= '%Y%m')\n",
    "\n",
    "        factor_model = m4.copy()\n",
    "        factor_model = factor_model[['MKTRF','SMB','MGMT','PERF']]\n",
    "        rf = m4[['RF']]\n",
    "\n",
    "        index_symbol = 'MKTRF'\n",
    "\n",
    "        print(m4.head(5))\n",
    "\n",
    "        #start date for data\n",
    "        start = '2004-01-01'\n",
    "        #end date for data\n",
    "        end = '2020-01-01'\n",
    "\n",
    "\n",
    "        #control the leverage\n",
    "        lev = 1.\n",
    "        stk_symbols  = factor_model.columns\n",
    "        returns = factor_model.copy()\n",
    "\n",
    "\n",
    "        #set the backtest parameters\n",
    "        bp = BacktestParameters(stk_symbols, start, end, lookback, lev  )\n",
    "        bt = Backtest(bp, returns, mode = 'insample')\n",
    "        data = Data(bp)\n",
    "\n",
    "        feature_list = [binary_alpha, binary_returns]\n",
    "        \n",
    "        feature_set = data.get_features(feature_list)\n",
    "        \n",
    "\n",
    " \n",
    "        for k in feature_set.keys():\n",
    "            feature_set[k] = pd.concat([feature_set[k], policy_uncertainty],axis = 1)\n",
    "            feature_set[k] = feature_set[k][(feature_set[k].index > binary_alpha.index.min()) & (feature_set[k].index < binary_alpha.index.max())]\n",
    "               \n",
    "                \n",
    "        data_type = 'stambaugh_yuan'\n",
    "        \n",
    "        factor_results, factor_results_capm, factor_reg,factor_results_scaled, factor_results_capm_scaled = write_outputs(bt, ff_factors_compare, umd, q_factor, returns, lookback_backtest, feature_set, binary_returns,backtest_write, image_write, cur_wd, data_type )\n",
    "        \n",
    "        if alpha_ff5.empty:\n",
    "            alpha_ff5 = factor_results.copy()\n",
    "            alpha_capm = factor_results_capm.copy()\n",
    "        else:\n",
    "            alpha_ff5 = pd.concat( [alpha_ff5, factor_results], axis = 1)\n",
    "            alpha_capm = pd.concat( [alpha_capm, factor_results_capm], axis = 1)\n",
    "            \n",
    "        if alpha_ff5_scaled.empty:\n",
    "            alpha_ff5_scaled = factor_results_scaled.copy()\n",
    "            alpha_capm_scaled = factor_results_capm_scaled.copy()\n",
    "        else:\n",
    "            alpha_ff5_scaled = pd.concat( [alpha_ff5_scaled, factor_results_scaled], axis = 1)\n",
    "            alpha_capm_scaled = pd.concat( [alpha_capm_scaled, factor_results_capm_scaled], axis = 1)      \n",
    "          \n",
    "        \n",
    "        \n",
    "        ####################################################\n",
    "        ####################################################   \n",
    "        #Testing Portfolios- Hou, Kewei, Chen Xue, and Lu Zhang####################################################\n",
    "        ####################################################\n",
    "        ####################################################\n",
    "        ####################################################\n",
    "        \n",
    "        \n",
    "        ###############size control#######################################\n",
    "        anomaly = pd.read_csv('anomalies_size_control.csv', index_col = 0)\n",
    "        anomaly.index = pd.to_datetime(anomaly.index, format= '%Y-%m-%d')\n",
    "\n",
    "        ff_factors = pd.read_csv('F-F_Research_Data_5_Factors_2x3.csv', index_col = 0)\n",
    "        ff_factors.index = pd.to_datetime(ff_factors.index, format= '%Y%m')\n",
    "        ff_factors = ff_factors / 100.0\n",
    "        ff_factors[ff_factors == -999] = np.NaN\n",
    "        ff_factors[ff_factors == -99.9] = np.NaN\n",
    "        ff_factors = ff_factors.dropna(how = 'any', axis = 0)\n",
    "\n",
    "        factor_model = ff_factors.copy().dropna(axis = 0, how = 'any')\n",
    "        rf = factor_model[['RF']]\n",
    "        factor_model = factor_model[['Mkt-RF', 'SMB', 'HML', 'RMW', 'CMA']]\n",
    "        ff_factors_compare = factor_model.copy()\n",
    "\n",
    "        index_symbol = 'Mkt-RF'\n",
    "\n",
    "        print(anomaly.head(5))\n",
    "\n",
    "        #start date for data\n",
    "        start = '2004-01-01'\n",
    "        #end date for data\n",
    "        end = '2020-01-01'\n",
    "\n",
    "\n",
    "        #control the leverage\n",
    "        lev = 1.\n",
    "        stk_symbols  = anomaly.columns\n",
    "        returns = anomaly.copy()\n",
    "\n",
    "\n",
    "        #set the backtest parameters\n",
    "        bp = BacktestParameters(stk_symbols, start, end, lookback, lev  )\n",
    "        bt = Backtest(bp, returns, mode = 'insample')\n",
    "        data = Data(bp)\n",
    "\n",
    "        df_alpha, idiosyncratic_vol, binary_alpha = data.get_alpha(returns, factor_model, rf)\n",
    "        rolling_mean_12, rolling_mean_6, rolling_std_12, binary_mean_12,binary_mean_6,binary_returns, rolling_sum_12s = data.get_rolling_features(returns, 0.0)\n",
    "\n",
    "\n",
    "        feature_list = [binary_alpha, binary_returns]\n",
    "        \n",
    "        feature_set = data.get_features(feature_list)\n",
    "        \n",
    "\n",
    " \n",
    "        for k in feature_set.keys():\n",
    "            feature_set[k] = pd.concat([feature_set[k], policy_uncertainty],axis = 1)\n",
    "            feature_set[k] = feature_set[k][(feature_set[k].index > binary_alpha.index.min()) & (feature_set[k].index < binary_alpha.index.max())]\n",
    "               \n",
    "        data_type = 'anomaly_size_control'\n",
    "        \n",
    "        factor_results, factor_results_capm, factor_reg,factor_results_scaled, factor_results_capm_scaled = write_outputs(bt, ff_factors_compare, umd, q_factor, returns, lookback_backtest, feature_set, binary_returns,backtest_write, image_write, cur_wd, data_type )\n",
    "        \n",
    "        if alpha_ff5.empty:\n",
    "            alpha_ff5 = factor_results.copy()\n",
    "            alpha_capm = factor_results_capm.copy()\n",
    "        else:\n",
    "            alpha_ff5 = pd.concat( [alpha_ff5, factor_results], axis = 1)\n",
    "            alpha_capm = pd.concat( [alpha_capm, factor_results_capm], axis = 1)\n",
    "            \n",
    "        if alpha_ff5_scaled.empty:\n",
    "            alpha_ff5_scaled = factor_results_scaled.copy()\n",
    "            alpha_capm_scaled = factor_results_capm_scaled.copy()\n",
    "        else:\n",
    "            alpha_ff5_scaled = pd.concat( [alpha_ff5_scaled, factor_results_scaled], axis = 1)\n",
    "            alpha_capm_scaled = pd.concat( [alpha_capm_scaled, factor_results_capm_scaled], axis = 1)      \n",
    "       \n",
    "        \n",
    "        \n",
    "        \n",
    "        ##########################################################################\n",
    "        ##########################################################################\n",
    "        ##########################################################################\n",
    "        ###############deciles #######################################\n",
    "        ##########################################################################\n",
    "        ##########################################################################\n",
    "        ##########################################################################\n",
    "        ##########################################################################\n",
    "        \n",
    "        anomaly = pd.read_csv('anomalies_decile.csv', index_col = 0)\n",
    "        anomaly.index = pd.to_datetime(anomaly.index, format= '%Y-%m-%d')\n",
    "\n",
    "        ff_factors = pd.read_csv('F-F_Research_Data_5_Factors_2x3.csv', index_col = 0)\n",
    "        ff_factors.index = pd.to_datetime(ff_factors.index, format= '%Y%m')\n",
    "        ff_factors = ff_factors / 100.0\n",
    "        ff_factors[ff_factors == -999] = np.NaN\n",
    "        ff_factors[ff_factors == -99.9] = np.NaN\n",
    "        ff_factors = ff_factors.dropna(how = 'any', axis = 0)\n",
    "\n",
    "        factor_model = ff_factors.copy().dropna(axis = 0, how = 'any')\n",
    "        rf = factor_model[['RF']]\n",
    "        factor_model = factor_model[['Mkt-RF', 'SMB', 'HML', 'RMW', 'CMA']]\n",
    "        ff_factors_compare = factor_model.copy()\n",
    "\n",
    "        index_symbol = 'Mkt-RF'\n",
    "\n",
    "        print(anomaly.head(5))\n",
    "\n",
    "        #start date for data\n",
    "        start = '2004-01-01'\n",
    "        #end date for data\n",
    "        end = '2020-01-01'\n",
    "\n",
    "\n",
    "        #control the leverage\n",
    "        lev = 1.\n",
    "        stk_symbols  = anomaly.columns\n",
    "        returns = anomaly.copy()\n",
    "\n",
    "\n",
    "        #set the backtest parameters\n",
    "        bp = BacktestParameters(stk_symbols, start, end, lookback, lev  )\n",
    "        bt = Backtest(bp, returns, mode = 'insample')\n",
    "        data = Data(bp)\n",
    "\n",
    "        df_alpha, idiosyncratic_vol, binary_alpha = data.get_alpha(returns, factor_model, rf)\n",
    "        rolling_mean_12, rolling_mean_6, rolling_std_12, binary_mean_12,binary_mean_6,binary_returns, rolling_sum_12s = data.get_rolling_features(returns, 0.0)\n",
    "\n",
    "\n",
    "        #creates a dictionary of different features for analysis based on ticker keys\n",
    "        feature_list = [binary_alpha, binary_returns]\n",
    "        \n",
    "        feature_set = data.get_features(feature_list)\n",
    "        \n",
    "\n",
    " \n",
    "        for k in feature_set.keys():\n",
    "            feature_set[k] = pd.concat([feature_set[k], policy_uncertainty],axis = 1)\n",
    "            feature_set[k] = feature_set[k][(feature_set[k].index > binary_alpha.index.min()) & (feature_set[k].index < binary_alpha.index.max())]\n",
    "               \n",
    "        data_type = 'anomaly_decile'\n",
    "        \n",
    "        factor_results, factor_results_capm, factor_reg,factor_results_scaled, factor_results_capm_scaled = write_outputs(bt, ff_factors_compare, umd, q_factor, returns, lookback_backtest, feature_set, binary_returns,backtest_write, image_write, cur_wd, data_type )\n",
    "        \n",
    "        if alpha_ff5.empty:\n",
    "            alpha_ff5 = factor_results.copy()\n",
    "            alpha_capm = factor_results_capm.copy()\n",
    "        else:\n",
    "            alpha_ff5 = pd.concat( [alpha_ff5, factor_results], axis = 1)\n",
    "            alpha_capm = pd.concat( [alpha_capm, factor_results_capm], axis = 1)\n",
    "            \n",
    "        if alpha_ff5_scaled.empty:\n",
    "            alpha_ff5_scaled = factor_results_scaled.copy()\n",
    "            alpha_capm_scaled = factor_results_capm_scaled.copy()\n",
    "        else:\n",
    "            alpha_ff5_scaled = pd.concat( [alpha_ff5_scaled, factor_results_scaled], axis = 1)\n",
    "            alpha_capm_scaled = pd.concat( [alpha_capm_scaled, factor_results_capm_scaled], axis = 1)      \n",
    "   \n",
    "        \n",
    "        ################    Sector_ETF Sectors           ######################################\n",
    "        etf = pd.read_csv('Sector_ETF.csv', index_col = 0)\n",
    "        etf.index = pd.to_datetime(etf.index, format= '%Y%m%d')\n",
    "        etf.index = etf.index.to_period('M').astype(str)\n",
    "        etf.index = pd.to_datetime(etf.index, format= '%Y-%m')\n",
    "\n",
    "        ff_factors = pd.read_csv('F-F_Research_Data_5_Factors_2x3.csv', index_col = 0)\n",
    "        ff_factors.index = pd.to_datetime(ff_factors.index, format= '%Y%m')\n",
    "        ff_factors = ff_factors / 100.0\n",
    "        ff_factors[ff_factors == -999] = np.NaN\n",
    "        ff_factors[ff_factors == -99.9] = np.NaN\n",
    "        ff_factors = ff_factors.dropna(how = 'any', axis = 0)\n",
    "\n",
    "        factor_model = ff_factors.copy().dropna(axis = 0, how = 'any')\n",
    "        rf = factor_model[['RF']]\n",
    "        factor_model = factor_model[['Mkt-RF', 'SMB', 'HML', 'RMW', 'CMA']]\n",
    "        ff_factors_compare = factor_model.copy()\n",
    "        rf_subtract = rf.copy()\n",
    "        rf_subtract.index = pd.to_datetime(rf_subtract.index, format = 'Y%-%m-%d')\n",
    "\n",
    "        union_index = etf.index.intersection(rf_subtract.index)\n",
    "        etf = etf.loc[union_index,:]\n",
    "        rf_subtract = rf_subtract.loc[union_index, :]\n",
    "        for e in etf.columns:\n",
    "            etf.loc[:,e] = etf.loc[:,e] - rf_subtract['RF']\n",
    "\n",
    "        index_symbol = 'Mkt-RF'\n",
    "        \n",
    "        print(etf.head(5))\n",
    "\n",
    "        #start date for data\n",
    "        start = '2004-01-01'\n",
    "        #end date for data\n",
    "        end = '2020-01-01'\n",
    "\n",
    "\n",
    "        #control the leverage\n",
    "        lev = 1.\n",
    "        stk_symbols  = etf.columns\n",
    "        returns = etf.copy()\n",
    "\n",
    "\n",
    "        #set the backtest parameters\n",
    "        bp = BacktestParameters(stk_symbols, start, end, lookback, lev  )\n",
    "        bt = Backtest(bp, returns, mode = 'insample')\n",
    "        data = Data(bp)\n",
    "\n",
    "        p = data.get_probability(returns, factor_model, rf)\n",
    "        df_alpha, idiosyncratic_vol, binary_alpha = data.get_alpha(returns, factor_model, rf)\n",
    "        rolling_mean_12, rolling_mean_6, rolling_std_12, binary_mean_12,binary_mean_6,binary_returns, rolling_sum_12s = data.get_rolling_features(returns, 0.0)\n",
    "\n",
    "\n",
    "        #creates a dictionary of different features for analysis based on ticker keys\n",
    "        feature_list = [binary_alpha, binary_returns]\n",
    "        \n",
    "        feature_set = data.get_features(feature_list)\n",
    "        \n",
    "\n",
    " \n",
    "        for k in feature_set.keys():\n",
    "            feature_set[k] = pd.concat([feature_set[k], policy_uncertainty],axis = 1)\n",
    "            feature_set[k] = feature_set[k][(feature_set[k].index > binary_alpha.index.min()) & (feature_set[k].index < binary_alpha.index.max())]\n",
    "               \n",
    "                \n",
    "        data_type = 'sector_etf_universe'\n",
    "\n",
    "        factor_results, factor_results_capm, factor_reg,factor_results_scaled, factor_results_capm_scaled = write_outputs(bt, ff_factors_compare, umd, q_factor, returns, lookback_backtest, feature_set, binary_returns,backtest_write, image_write, cur_wd, data_type )\n",
    "        \n",
    "        if alpha_ff5.empty:\n",
    "            alpha_ff5 = factor_results.copy()\n",
    "            alpha_capm = factor_results_capm.copy()\n",
    "        else:\n",
    "            alpha_ff5 = pd.concat( [alpha_ff5, factor_results], axis = 1)\n",
    "            alpha_capm = pd.concat( [alpha_capm, factor_results_capm], axis = 1)\n",
    "            \n",
    "        if alpha_ff5_scaled.empty:\n",
    "            alpha_ff5_scaled = factor_results_scaled.copy()\n",
    "            alpha_capm_scaled = factor_results_capm_scaled.copy()\n",
    "        else:\n",
    "            alpha_ff5_scaled = pd.concat( [alpha_ff5_scaled, factor_results_scaled], axis = 1)\n",
    "            alpha_capm_scaled = pd.concat( [alpha_capm_scaled, factor_results_capm_scaled], axis = 1)      \n",
    "   \n",
    "        ######################################################\n",
    "        ######################################################\n",
    "        ######################################################\n",
    "        ############## Factor Model ETF   ####################\n",
    "        ######################################################\n",
    "        ######################################################\n",
    "        ######################################################\n",
    "        \n",
    "        etf = pd.read_csv('Three_Factor_ETF.csv', index_col = 0)\n",
    "        etf.index = pd.to_datetime(etf.index, format= '%Y%m%d')\n",
    "        etf.index = etf.index.to_period('M').astype(str)\n",
    "        etf.index = pd.to_datetime(etf.index, format= '%Y-%m')\n",
    "\n",
    "        ff_factors = pd.read_csv('F-F_Research_Data_5_Factors_2x3.csv', index_col = 0)\n",
    "        ff_factors.index = pd.to_datetime(ff_factors.index, format= '%Y%m')\n",
    "        ff_factors = ff_factors / 100.0\n",
    "        ff_factors[ff_factors == -999] = np.NaN\n",
    "        ff_factors[ff_factors == -99.9] = np.NaN\n",
    "        ff_factors = ff_factors.dropna(how = 'any', axis = 0)\n",
    "\n",
    "        factor_model = ff_factors.copy().dropna(axis = 0, how = 'any')\n",
    "        rf = factor_model[['RF']]\n",
    "        factor_model = factor_model[['Mkt-RF', 'SMB', 'HML']]\n",
    "        ff_factors_compare = factor_model.copy()\n",
    "        rf_subtract = rf.copy()\n",
    "        rf_subtract.index = pd.to_datetime(rf_subtract.index, format = 'Y%-%m-%d')\n",
    "\n",
    "        union_index = etf.index.intersection(rf_subtract.index)\n",
    "        etf = etf.loc[union_index,:]\n",
    "        rf_subtract = rf_subtract.loc[union_index, :]\n",
    "        for e in etf.columns:\n",
    "            if e == 'SPY':\n",
    "                etf.loc[:,e] = etf.loc[:,e] - rf_subtract['RF']\n",
    "\n",
    "        index_symbol = 'Mkt-RF'\n",
    "        #end date for data\n",
    "        end = '2020-01-01'\n",
    "\n",
    "\n",
    "        #control the leverage\n",
    "        lev = 1.\n",
    "        stk_symbols  = etf.columns\n",
    "        returns = etf.copy()\n",
    "\n",
    "\n",
    "        #set the backtest parameters\n",
    "        bp = BacktestParameters(stk_symbols, start, end, lookback, lev  )\n",
    "        bt = Backtest(bp, returns, mode = 'insample')\n",
    "        data = Data(bp)\n",
    "\n",
    "        p = data.get_probability(returns, factor_model, rf)\n",
    "        df_alpha, idiosyncratic_vol, binary_alpha = data.get_alpha(returns, factor_model, rf)\n",
    "        rolling_mean_12, rolling_mean_6, rolling_std_12, binary_mean_12,binary_mean_6,binary_returns, rolling_sum_12s = data.get_rolling_features(returns, 0.0)\n",
    "\n",
    "\n",
    "        #creates a dictionary of different features for analysis based on ticker keys\n",
    "        feature_list = [binary_alpha, binary_returns]\n",
    "        \n",
    "        feature_set = data.get_features(feature_list)\n",
    "        \n",
    "\n",
    " \n",
    "        for k in feature_set.keys():\n",
    "            feature_set[k] = pd.concat([feature_set[k], policy_uncertainty],axis = 1)\n",
    "            feature_set[k] = feature_set[k][(feature_set[k].index > binary_alpha.index.min()) & (feature_set[k].index < binary_alpha.index.max())]\n",
    "               \n",
    "        data_type = 'three_factor_etf'\n",
    "\n",
    "        factor_results, factor_results_capm, factor_reg = write_outputs(bt, ff_factors_compare, umd, q_factor, returns, lookback_backtest, feature_set, binary_returns,backtest_write, image_write, cur_wd, data_type )\n",
    "\n",
    "        if alpha_ff5.empty:\n",
    "            alpha_ff5 = factor_results.copy()\n",
    "            alpha_capm = factor_results_capm.copy()\n",
    "        else:\n",
    "            alpha_ff5 = pd.concat( [alpha_ff5, factor_results], axis = 1)\n",
    "            alpha_capm = pd.concat( [alpha_capm, factor_results_capm], axis = 1)     \n",
    "        print(etf.head(5))\n",
    "\n",
    "        \n",
    "        etf = pd.read_csv('Five_Factor_ETF.csv', index_col = 0)\n",
    "        etf.index = pd.to_datetime(etf.index, format= '%Y%m%d')\n",
    "        etf.index = etf.index.to_period('M').astype(str)\n",
    "        etf.index = pd.to_datetime(etf.index, format= '%Y-%m')\n",
    "\n",
    "        ff_factors = pd.read_csv('F-F_Research_Data_5_Factors_2x3.csv', index_col = 0)\n",
    "        ff_factors.index = pd.to_datetime(ff_factors.index, format= '%Y%m')\n",
    "        ff_factors = ff_factors / 100.0\n",
    "        ff_factors[ff_factors == -999] = np.NaN\n",
    "        ff_factors[ff_factors == -99.9] = np.NaN\n",
    "        ff_factors = ff_factors.dropna(how = 'any', axis = 0)\n",
    "\n",
    "        factor_model = ff_factors.copy().dropna(axis = 0, how = 'any')\n",
    "        rf = factor_model[['RF']]\n",
    "        factor_model = factor_model[['Mkt-RF', 'SMB', 'HML']]\n",
    "        ff_factors_compare = factor_model.copy()\n",
    "        rf_subtract = rf.copy()\n",
    "        rf_subtract.index = pd.to_datetime(rf_subtract.index, format = 'Y%-%m-%d')\n",
    "\n",
    "        union_index = etf.index.intersection(rf_subtract.index)\n",
    "        etf = etf.loc[union_index,:]\n",
    "        rf_subtract = rf_subtract.loc[union_index, :]\n",
    "        for e in etf.columns:\n",
    "            if e == 'SPY':\n",
    "                etf.loc[:,e] = etf.loc[:,e] - rf_subtract['RF']\n",
    "\n",
    "        index_symbol = 'Mkt-RF'\n",
    "        #end date for data\n",
    "        end = '2020-01-01'\n",
    "\n",
    "\n",
    "        #control the leverage\n",
    "        lev = 1.\n",
    "        stk_symbols  = etf.columns\n",
    "        returns = etf.copy()\n",
    "\n",
    "\n",
    "        #set the backtest parameters\n",
    "        bp = BacktestParameters(stk_symbols, start, end, lookback, lev  )\n",
    "        bt = Backtest(bp, returns, mode = 'insample')\n",
    "        data = Data(bp)\n",
    "\n",
    "        p = data.get_probability(returns, factor_model, rf)\n",
    "        df_alpha, idiosyncratic_vol, binary_alpha = data.get_alpha(returns, factor_model, rf)\n",
    "        rolling_mean_12, rolling_mean_6, rolling_std_12, binary_mean_12,binary_mean_6,binary_returns, rolling_sum_12s = data.get_rolling_features(returns, 0.0)\n",
    "\n",
    "\n",
    "        #creates a dictionary of different features for analysis based on ticker keys\n",
    "        feature_list = [binary_alpha, binary_returns]\n",
    "        \n",
    "        feature_set = data.get_features(feature_list)\n",
    "        \n",
    "\n",
    " \n",
    "        for k in feature_set.keys():\n",
    "            feature_set[k] = pd.concat([feature_set[k], policy_uncertainty],axis = 1)\n",
    "            feature_set[k] = feature_set[k][(feature_set[k].index > binary_alpha.index.min()) & (feature_set[k].index < binary_alpha.index.max())]\n",
    "               \n",
    "                \n",
    "        data_type = 'five_factor_etf'\n",
    "\n",
    "        factor_results, factor_results_capm, factor_reg,factor_results_scaled, factor_results_capm_scaled = write_outputs(bt, ff_factors_compare, umd, q_factor, returns, lookback_backtest, feature_set, binary_returns,backtest_write, image_write, cur_wd, data_type )\n",
    "        \n",
    "        if alpha_ff5.empty:\n",
    "            alpha_ff5 = factor_results.copy()\n",
    "            alpha_capm = factor_results_capm.copy()\n",
    "        else:\n",
    "            alpha_ff5 = pd.concat( [alpha_ff5, factor_results], axis = 1)\n",
    "            alpha_capm = pd.concat( [alpha_capm, factor_results_capm], axis = 1)\n",
    "            \n",
    "        if alpha_ff5_scaled.empty:\n",
    "            alpha_ff5_scaled = factor_results_scaled.copy()\n",
    "            alpha_capm_scaled = factor_results_capm_scaled.copy()\n",
    "        else:\n",
    "            alpha_ff5_scaled = pd.concat( [alpha_ff5_scaled, factor_results_scaled], axis = 1)\n",
    "            alpha_capm_scaled = pd.concat( [alpha_capm_scaled, factor_results_capm_scaled], axis = 1)      \n",
    "   \n",
    "        \n",
    "        plt.close('all')\n",
    "        \n",
    "        ################    Sector_ETF Sectors           ######################################\n",
    "        etf = pd.read_csv('Sector_ETF.csv', index_col = 0)\n",
    "        etf.index = pd.to_datetime(etf.index, format= '%Y%m%d')\n",
    "        etf.index = etf.index.to_period('M').astype(str)\n",
    "        etf.index = pd.to_datetime(etf.index, format= '%Y-%m')\n",
    "\n",
    "        ff_factors = pd.read_csv('F-F_Research_Data_5_Factors_2x3.csv', index_col = 0)\n",
    "        ff_factors.index = pd.to_datetime(ff_factors.index, format= '%Y%m')\n",
    "        ff_factors = ff_factors / 100.0\n",
    "        ff_factors[ff_factors == -999] = np.NaN\n",
    "        ff_factors[ff_factors == -99.9] = np.NaN\n",
    "        ff_factors = ff_factors.dropna(how = 'any', axis = 0)\n",
    "\n",
    "        factor_model = ff_factors.copy().dropna(axis = 0, how = 'any')\n",
    "        rf = factor_model[['RF']]\n",
    "        factor_model = factor_model[['Mkt-RF', 'SMB', 'HML', 'RMW', 'CMA']]\n",
    "        ff_factors_compare = factor_model.copy()\n",
    "        rf_subtract = rf.copy()\n",
    "        rf_subtract.index = pd.to_datetime(rf_subtract.index, format = 'Y%-%m-%d')\n",
    "\n",
    "        union_index = etf.index.intersection(rf_subtract.index)\n",
    "        etf = etf.loc[union_index,:]\n",
    "        rf_subtract = rf_subtract.loc[union_index, :]\n",
    "        for e in etf.columns:\n",
    "            etf.loc[:,e] = etf.loc[:,e] - rf_subtract['RF']\n",
    "\n",
    "        index_symbol = 'Mkt-RF'\n",
    "        \n",
    "        print(etf.head(5))\n",
    "\n",
    "        #start date for data\n",
    "        start = '2004-01-01'\n",
    "        #end date for data\n",
    "        end = '2020-01-01'\n",
    "\n",
    "\n",
    "        #control the leverage\n",
    "        lev = 1.\n",
    "        stk_symbols  = etf.columns\n",
    "        returns = etf.copy()\n",
    "\n",
    "\n",
    "        #set the backtest parameters\n",
    "        bp = BacktestParameters(stk_symbols, start, end, lookback, lev  )\n",
    "        bt = Backtest(bp, returns, mode = 'insample')\n",
    "        data = Data(bp)\n",
    "\n",
    "        p = data.get_probability(returns, factor_model, rf)\n",
    "        df_alpha, idiosyncratic_vol, binary_alpha = data.get_alpha(returns, factor_model, rf)\n",
    "        rolling_mean_12, rolling_mean_6, rolling_std_12, binary_mean_12,binary_mean_6,binary_returns, rolling_sum_12s = data.get_rolling_features(returns, 0.0)\n",
    "\n",
    "\n",
    "        #creates a dictionary of different features for analysis based on ticker keys\n",
    "        feature_list = [binary_alpha, binary_returns]\n",
    "        \n",
    "        feature_set = data.get_features(feature_list)\n",
    "        \n",
    "\n",
    " \n",
    "        for k in feature_set.keys():\n",
    "            feature_set[k] = pd.concat([feature_set[k], policy_uncertainty],axis = 1)\n",
    "            feature_set[k] = feature_set[k][(feature_set[k].index > binary_alpha.index.min()) & (feature_set[k].index < binary_alpha.index.max())]\n",
    "               \n",
    "        data_type = 'sector_etf_universe'\n",
    "\n",
    "        factor_results, factor_results_capm, factor_reg,factor_results_scaled, factor_results_capm_scaled = write_outputs(bt, ff_factors_compare, umd, q_factor, returns, lookback_backtest, feature_set, binary_returns,backtest_write, image_write, cur_wd, data_type )\n",
    "        \n",
    "        if alpha_ff5.empty:\n",
    "            alpha_ff5 = factor_results.copy()\n",
    "            alpha_capm = factor_results_capm.copy()\n",
    "        else:\n",
    "            alpha_ff5 = pd.concat( [alpha_ff5, factor_results], axis = 1)\n",
    "            alpha_capm = pd.concat( [alpha_capm, factor_results_capm], axis = 1)\n",
    "            \n",
    "        if alpha_ff5_scaled.empty:\n",
    "            alpha_ff5_scaled = factor_results_scaled.copy()\n",
    "            alpha_capm_scaled = factor_results_capm_scaled.copy()\n",
    "        else:\n",
    "            alpha_ff5_scaled = pd.concat( [alpha_ff5_scaled, factor_results_scaled], axis = 1)\n",
    "            alpha_capm_scaled = pd.concat( [alpha_capm_scaled, factor_results_capm_scaled], axis = 1)      \n",
    "   \n",
    "        ######################################################\n",
    "        ######################################################\n",
    "        ######################################################\n",
    "        ############## Factor Model ETF   ####################\n",
    "        ######################################################\n",
    "        ######################################################\n",
    "        ######################################################\n",
    "        \n",
    "        etf = pd.read_csv('Three_Factor_ETF.csv', index_col = 0)\n",
    "        etf.index = pd.to_datetime(etf.index, format= '%Y%m%d')\n",
    "        etf.index = etf.index.to_period('M').astype(str)\n",
    "        etf.index = pd.to_datetime(etf.index, format= '%Y-%m')\n",
    "\n",
    "        ff_factors = pd.read_csv('F-F_Research_Data_5_Factors_2x3.csv', index_col = 0)\n",
    "        ff_factors.index = pd.to_datetime(ff_factors.index, format= '%Y%m')\n",
    "        ff_factors = ff_factors / 100.0\n",
    "        ff_factors[ff_factors == -999] = np.NaN\n",
    "        ff_factors[ff_factors == -99.9] = np.NaN\n",
    "        ff_factors = ff_factors.dropna(how = 'any', axis = 0)\n",
    "\n",
    "        factor_model = ff_factors.copy().dropna(axis = 0, how = 'any')\n",
    "        rf = factor_model[['RF']]\n",
    "        factor_model = factor_model[['Mkt-RF', 'SMB', 'HML']]\n",
    "        ff_factors_compare = factor_model.copy()\n",
    "        rf_subtract = rf.copy()\n",
    "        rf_subtract.index = pd.to_datetime(rf_subtract.index, format = 'Y%-%m-%d')\n",
    "\n",
    "        union_index = etf.index.intersection(rf_subtract.index)\n",
    "        etf = etf.loc[union_index,:]\n",
    "        rf_subtract = rf_subtract.loc[union_index, :]\n",
    "        for e in etf.columns:\n",
    "            if e == 'SPY':\n",
    "                etf.loc[:,e] = etf.loc[:,e] - rf_subtract['RF']\n",
    "\n",
    "        index_symbol = 'Mkt-RF'\n",
    "        #end date for data\n",
    "        end = '2020-01-01'\n",
    "\n",
    "\n",
    "        #control the leverage\n",
    "        lev = 1.\n",
    "        stk_symbols  = etf.columns\n",
    "        returns = etf.copy()\n",
    "\n",
    "\n",
    "        #set the backtest parameters\n",
    "        bp = BacktestParameters(stk_symbols, start, end, lookback, lev  )\n",
    "        bt = Backtest(bp, returns, mode = 'insample')\n",
    "        data = Data(bp)\n",
    "\n",
    "        p = data.get_probability(returns, factor_model, rf)\n",
    "        df_alpha, idiosyncratic_vol, binary_alpha = data.get_alpha(returns, factor_model, rf)\n",
    "        rolling_mean_12, rolling_mean_6, rolling_std_12, binary_mean_12,binary_mean_6,binary_returns, rolling_sum_12s = data.get_rolling_features(returns, 0.0)\n",
    "\n",
    "\n",
    "        #creates a dictionary of different features for analysis based on ticker keys\n",
    "        feature_list = [binary_alpha, binary_returns]\n",
    "        \n",
    "        feature_set = data.get_features(feature_list)\n",
    "        \n",
    "\n",
    " \n",
    "        for k in feature_set.keys():\n",
    "            feature_set[k] = pd.concat([feature_set[k], policy_uncertainty],axis = 1)\n",
    "            feature_set[k] = feature_set[k][(feature_set[k].index > binary_alpha.index.min()) & (feature_set[k].index < binary_alpha.index.max())]\n",
    "               \n",
    "                \n",
    "        data_type = 'three_factor_etf'\n",
    "\n",
    "        factor_results, factor_results_capm, factor_reg,factor_results_scaled, factor_results_capm_scaled = write_outputs(bt, ff_factors_compare, umd, q_factor, returns, lookback_backtest, feature_set, binary_returns,backtest_write, image_write, cur_wd, data_type )\n",
    "        \n",
    "        if alpha_ff5.empty:\n",
    "            alpha_ff5 = factor_results.copy()\n",
    "            alpha_capm = factor_results_capm.copy()\n",
    "        else:\n",
    "            alpha_ff5 = pd.concat( [alpha_ff5, factor_results], axis = 1)\n",
    "            alpha_capm = pd.concat( [alpha_capm, factor_results_capm], axis = 1)\n",
    "            \n",
    "        if alpha_ff5_scaled.empty:\n",
    "            alpha_ff5_scaled = factor_results_scaled.copy()\n",
    "            alpha_capm_scaled = factor_results_capm_scaled.copy()\n",
    "        else:\n",
    "            alpha_ff5_scaled = pd.concat( [alpha_ff5_scaled, factor_results_scaled], axis = 1)\n",
    "            alpha_capm_scaled = pd.concat( [alpha_capm_scaled, factor_results_capm_scaled], axis = 1)      \n",
    "      \n",
    "        print(etf.head(5))\n",
    "        \n",
    "        ############################ Five Factor ETF Model ################################\n",
    "\n",
    "        \n",
    "        etf = pd.read_csv('Five_Factor_ETF.csv', index_col = 0)\n",
    "        etf.index = pd.to_datetime(etf.index, format= '%Y%m%d')\n",
    "        etf.index = etf.index.to_period('M').astype(str)\n",
    "        etf.index = pd.to_datetime(etf.index, format= '%Y-%m')\n",
    "\n",
    "        ff_factors = pd.read_csv('F-F_Research_Data_5_Factors_2x3.csv', index_col = 0)\n",
    "        ff_factors.index = pd.to_datetime(ff_factors.index, format= '%Y%m')\n",
    "        ff_factors = ff_factors / 100.0\n",
    "        ff_factors[ff_factors == -999] = np.NaN\n",
    "        ff_factors[ff_factors == -99.9] = np.NaN\n",
    "        ff_factors = ff_factors.dropna(how = 'any', axis = 0)\n",
    "\n",
    "        factor_model = ff_factors.copy().dropna(axis = 0, how = 'any')\n",
    "        rf = factor_model[['RF']]\n",
    "        factor_model = factor_model[['Mkt-RF', 'SMB', 'HML']]\n",
    "        ff_factors_compare = factor_model.copy()\n",
    "        rf_subtract = rf.copy()\n",
    "        rf_subtract.index = pd.to_datetime(rf_subtract.index, format = 'Y%-%m-%d')\n",
    "\n",
    "        union_index = etf.index.intersection(rf_subtract.index)\n",
    "        etf = etf.loc[union_index,:]\n",
    "        rf_subtract = rf_subtract.loc[union_index, :]\n",
    "        for e in etf.columns:\n",
    "            if e == 'SPY':\n",
    "                etf.loc[:,e] = etf.loc[:,e] - rf_subtract['RF']\n",
    "\n",
    "        index_symbol = 'Mkt-RF'\n",
    "        #end date for data\n",
    "        end = '2020-01-01'\n",
    "\n",
    "\n",
    "        #control the leverage\n",
    "        lev = 1.\n",
    "        stk_symbols  = etf.columns\n",
    "        returns = etf.copy()\n",
    "\n",
    "\n",
    "        #set the backtest parameters\n",
    "        bp = BacktestParameters(stk_symbols, start, end, lookback, lev  )\n",
    "        bt = Backtest(bp, returns, mode = 'insample')\n",
    "        data = Data(bp)\n",
    "\n",
    "        p = data.get_probability(returns, factor_model, rf)\n",
    "        df_alpha, idiosyncratic_vol, binary_alpha = data.get_alpha(returns, factor_model, rf)\n",
    "        rolling_mean_12, rolling_mean_6, rolling_std_12, binary_mean_12,binary_mean_6,binary_returns, rolling_sum_12s = data.get_rolling_features(returns, 0.0)\n",
    "\n",
    "\n",
    "        #creates a dictionary of different features for analysis based on ticker keys\n",
    "        feature_list = [binary_alpha, binary_returns]\n",
    "        \n",
    "        feature_set = data.get_features(feature_list)\n",
    "        \n",
    "\n",
    " \n",
    "        for k in feature_set.keys():\n",
    "            feature_set[k] = pd.concat([feature_set[k], policy_uncertainty],axis = 1)\n",
    "            feature_set[k] = feature_set[k][(feature_set[k].index > binary_alpha.index.min()) & (feature_set[k].index < binary_alpha.index.max())]\n",
    "               \n",
    "        data_type = 'five_factor_etf'\n",
    "\n",
    "        factor_results, factor_results_capm, factor_reg,factor_results_scaled, factor_results_capm_scaled = write_outputs(bt, ff_factors_compare, umd, q_factor, returns, lookback_backtest, feature_set, binary_returns,backtest_write, image_write, cur_wd, data_type )\n",
    "        \n",
    "        if alpha_ff5.empty:\n",
    "            alpha_ff5 = factor_results.copy()\n",
    "            alpha_capm = factor_results_capm.copy()\n",
    "        else:\n",
    "            alpha_ff5 = pd.concat( [alpha_ff5, factor_results], axis = 1)\n",
    "            alpha_capm = pd.concat( [alpha_capm, factor_results_capm], axis = 1)\n",
    "            \n",
    "        if alpha_ff5_scaled.empty:\n",
    "            alpha_ff5_scaled = factor_results_scaled.copy()\n",
    "            alpha_capm_scaled = factor_results_capm_scaled.copy()\n",
    "        else:\n",
    "            alpha_ff5_scaled = pd.concat( [alpha_ff5_scaled, factor_results_scaled], axis = 1)\n",
    "            alpha_capm_scaled = pd.concat( [alpha_capm_scaled, factor_results_capm_scaled], axis = 1)      \n",
    "   \n",
    "        \n",
    "        plt.close('all')\n",
    "\n",
    "        ######################################################\n",
    "        #write the alphas to a csv\n",
    "        #annualize the alphas\n",
    "        \n",
    "        \n",
    "        alpha_ff5.to_csv('alphaff5.csv')\n",
    "        alpha_capm.to_csv('alphacapm.csv')\n",
    "        alpha_ff5_scaled.to_csv('alphaff5_scaled.csv')\n",
    "        alpha_capm_scaled.to_csv('alphacapm_scaled.csv')\n",
    "        \n",
    "        plt.close('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets clean up the input of the alpha parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_ff5 = pd.read_csv('alphaff5.csv', index_col = 0).T\n",
    "alpha_capm = pd.read_csv('alphacapm.csv', index_col = 0).T\n",
    "\n",
    "alpha_ff5_clean  = alpha_ff5.copy()\n",
    "alpha_capm_clean = alpha_capm.copy()\n",
    "\n",
    "alpha_ff5_clean.loc[:, 'lookback feature'] = ''\n",
    "alpha_ff5_clean.loc[:, 'lookback ml model'] = ''\n",
    "alpha_ff5_clean.loc[:, 'new index'] = ''\n",
    "\n",
    "alpha_capm_clean.loc[:, 'lookback feature'] = ''\n",
    "alpha_capm_clean.loc[:, 'lookback ml model'] = ''\n",
    "alpha_capm_clean.loc[:, 'new index'] = ''\n",
    "\n",
    "for count, row in alpha_ff5.iterrows():\n",
    "    alpha_ff5_clean.loc[count, 'lookback feature'] = count.split('_')[-2]\n",
    "    alpha_ff5_clean.loc[count, 'lookback ml model'] = count.split('_')[-1]\n",
    "    alpha_ff5_clean.loc[count, 'new index'] = \"_\".join(count.split('_')[0:-2])\n",
    "   \n",
    "for count, row in alpha_capm.iterrows():\n",
    "    alpha_capm_clean.loc[count, 'lookback feature'] = count.split('_')[-2]\n",
    "    alpha_capm_clean.loc[count, 'lookback ml model'] = count.split('_')[-1]\n",
    "    alpha_capm_clean.loc[count, 'new index'] = \"_\".join(count.split('_')[0:-2])\n",
    "    \n",
    "alpha_capm_clean.index = alpha_capm_clean.loc[:, 'new index']\n",
    "alpha_ff5_clean.index = alpha_ff5_clean.loc[:, 'new index']\n",
    "\n",
    "alpha_capm_clean.drop('new index', axis = 1, inplace = True)\n",
    "alpha_capm.index.name = 'strategy'\n",
    "alpha_ff5_clean.drop('new index', axis = 1, inplace = True)\n",
    "alpha_ff5_clean.index.name = 'strategy'\n",
    "\n",
    "alpha_ff5_clean.to_csv('alphaff5_cleaned.csv')\n",
    "alpha_capm_clean.to_csv('alphacapm_cleaned.csv')\n",
    "alpha_ff5_clean.to_latex('alphaff5_latex.txt')\n",
    "alpha_capm_clean.to_latex('alphacapm_latex.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the alphas for a backtest across individual portfolios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_ff5_scaled"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
