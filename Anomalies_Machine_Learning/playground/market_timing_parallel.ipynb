{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\janko_machine_learning\\playground\n"
     ]
    }
   ],
   "source": [
    "from regime_module import *\n",
    "from statsmodels.iolib.summary2 import summary_col\n",
    "import csv\n",
    "import os\n",
    "import matplotlib.backends.backend_pdf\n",
    "from multiprocessing import Pool\n",
    "\n",
    "cur_wd = os.getcwd()\n",
    "print(cur_wd)\n",
    "image_wd = os.path.join(cur_wd, 'backtest_images_market_timing')\n",
    "backtest_wd = os.path.join(cur_wd, 'backtests_market_timing')\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "from mpl_toolkits import mplot3d\n",
    "%matplotlib inline\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_outputs(bt, ff_factors_compare, umd, q_factor,stambaugh_yuan, returns, lookback_backtest, feature_set, binary_returns, backtest_write, image_write, cur_wd, data_type ):\n",
    "    \n",
    "    backtest_dict = {}\n",
    "    \n",
    "    outname = 'output_results_' + str(lookback) + '_' + str(lookback_backtest) +  '.txt'\n",
    "    outnamelatex = 'output_results_latex_' + str(lookback) + '_' + str(lookback_backtest) +  '.txt'\n",
    "    \n",
    "    outname_scaled = 'output_results_scaled_' + str(lookback) + '_' + str(lookback_backtest) +  '.txt'\n",
    "    outnamelatex_scaled = 'output_results_latex_scaled_' + str(lookback) + '_' + str(lookback_backtest) +  '.txt'\n",
    "    \n",
    "    \n",
    "    output_reg = []\n",
    "    output_capm = []\n",
    "    output_umd = []\n",
    "    output_qfactor = []\n",
    "    output_tm = []\n",
    "    output_hm = []\n",
    "    output_sy = []\n",
    "    \n",
    "    output_reg_scaled = []\n",
    "    output_capm_scaled = []\n",
    "    output_umd_scaled = []\n",
    "    output_qfactor_scaled = []\n",
    "    output_tm_scaled = []\n",
    "    output_hm_scaled = []\n",
    "    output_sy_scaled = []\n",
    "    \n",
    "    #names = ['Logistic L/S', 'Logistic L', 'Logistic S', 'KNN L/S', 'KNN L', 'KNN S', 'Neural Net L/S', 'Neural Net L', 'Neural Net S', 'RF L/S', 'RF L', 'RF S','AdaBoost L/S', 'AdaBoost L', 'AdaBoost S', 'Ensemble', 'Binary']\n",
    "\n",
    "    names = ['KNN L/S', 'KNN L', 'KNN S', 'Neural Net L/S', 'Neural Net L', 'Neural Net S', 'RF L/S', 'RF L', 'RF S','AdaBoost L/S', 'AdaBoost L', 'AdaBoost S', 'Ensemble', 'Binary']\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    print(\"Logistic\")\n",
    "    print(\"Long and Short\")\n",
    "    backtest, weights, scaled_backtest = bt.run_backtest(returns, lookback_backtest, input_classifiers= [\"Logistic\"], inputs = feature_set, outputs = binary_returns,  ensemble = 'no', trade_type = 'both')\n",
    "    results, fm_sum = bt.factor_results(backtest, ff_factors_compare)\n",
    "    results_capm, fm_sum_capm = bt.factor_results_capm(backtest, ff_factors_compare)\n",
    "    results_umd, fm_sum_umd = bt.factor_results_umd(backtest, umd)\n",
    "    results_q, fm_sum_q = bt.factor_results_qfactor(backtest, q_factor)\n",
    "    results_tm = bt.treynor_mazuy(backtest, ff_factors_compare)\n",
    "    results_hm = bt.henriksson_merton(backtest, ff_factors_compare)\n",
    "\n",
    "    results_scaled, fm_sum = bt.factor_results(scaled_backtest, ff_factors_compare)\n",
    "    results_capm_scaled, fm_sum_capm = bt.factor_results_capm(scaled_backtest, ff_factors_compare)\n",
    "    results_umd_scaled, fm_sum_umd = bt.factor_results_umd(scaled_backtest, umd)\n",
    "    results_q_scaled, fm_sum_q = bt.factor_results_qfactor(scaled_backtest, q_factor)\n",
    "    results_tm_scaled = bt.treynor_mazuy(scaled_backtest, ff_factors_compare)\n",
    "    results_hm_scaled = bt.henriksson_merton(scaled_backtest, ff_factors_compare)\n",
    "\n",
    "    output_reg.append(results)\n",
    "    output_capm.append(results_capm)\n",
    "    output_umd.append(results_umd)\n",
    "    output_qfactor.append(results_q)\n",
    "    output_tm.append(results_tm)\n",
    "    output_hm.append(results_hm)\n",
    "\n",
    "    output_reg_scaled.append(results_scaled)\n",
    "    output_capm_scaled.append(results_capm_scaled)\n",
    "    output_umd_scaled.append(results_umd_scaled)\n",
    "    output_qfactor_scaled.append(results_q_scaled)\n",
    "    output_tm_scaled.append(results_tm_scaled)\n",
    "    output_hm_scaled.append(results_hm_scaled)\n",
    "\n",
    "    backtest.to_csv(os.path.join(backtest_write, data_type + '_Logistic_' + 'ls' + '.csv'))\n",
    "    scaled_backtest.to_csv(os.path.join(backtest_write, data_type + '_Logistic_scaled_' + 'ls' + '.csv'))\n",
    "    plot_bf = pd.DataFrame(1.0 + backtest.mean(axis = 1), columns = ['backtest']).cumprod()\n",
    "    plot_mkt = (1.0 + ff_factors['Mkt-RF'].loc[plot_bf.index]).cumprod()\n",
    "    plot_df = pd.concat([plot_bf, plot_mkt], axis = 1).dropna()\n",
    "    ax = plot_df.plot()\n",
    "    ax.figure.savefig(os.path.join(image_write, data_type + '_Logistic_' + 'ls' + '.pdf'))\n",
    "\n",
    "\n",
    "    print(\"Long Only\")\n",
    "    backtest, weights, scaled_backtest = bt.run_backtest(returns, lookback_backtest, input_classifiers= [\"Logistic\"], inputs = feature_set, outputs = binary_returns,  ensemble = 'no', trade_type = 'long')\n",
    "    results, fm_sum = bt.factor_results(backtest, ff_factors_compare)\n",
    "    results_capm, fm_sum_capm = bt.factor_results_capm(backtest, ff_factors_compare)\n",
    "    results_umd, fm_sum_umd = bt.factor_results_umd(backtest, umd)\n",
    "    results_q, fm_sum_q = bt.factor_results_qfactor(backtest, q_factor)\n",
    "    results_tm = bt.treynor_mazuy(backtest, ff_factors_compare)\n",
    "    results_hm = bt.henriksson_merton(backtest, ff_factors_compare)\n",
    "\n",
    "    results_scaled, fm_sum = bt.factor_results(scaled_backtest, ff_factors_compare)\n",
    "    results_capm_scaled, fm_sum_capm = bt.factor_results_capm(scaled_backtest, ff_factors_compare)\n",
    "    results_umd_scaled, fm_sum_umd = bt.factor_results_umd(scaled_backtest, umd)\n",
    "    results_q_scaled, fm_sum_q = bt.factor_results_qfactor(scaled_backtest, q_factor)\n",
    "    results_tm_scaled = bt.treynor_mazuy(scaled_backtest, ff_factors_compare)\n",
    "    results_hm_scaled = bt.henriksson_merton(scaled_backtest, ff_factors_compare)\n",
    "\n",
    "    output_reg.append(results)\n",
    "    output_capm.append(results_capm)\n",
    "    output_umd.append(results_umd)\n",
    "    output_qfactor.append(results_q)\n",
    "    output_tm.append(results_tm)\n",
    "    output_hm.append(results_hm)\n",
    "\n",
    "    output_reg_scaled.append(results_scaled)\n",
    "    output_capm_scaled.append(results_capm_scaled)\n",
    "    output_umd_scaled.append(results_umd_scaled)\n",
    "    output_qfactor_scaled.append(results_q_scaled)\n",
    "    output_tm_scaled.append(results_tm_scaled)\n",
    "    output_hm_scaled.append(results_hm_scaled)\n",
    "\n",
    "    backtest.to_csv(os.path.join(backtest_write, data_type + '_Logistic_' + 'l' + '.csv'))\n",
    "    scaled_backtest.to_csv(os.path.join(backtest_write, data_type + '_Logistic_scaled_' + 'l' + '.csv'))\n",
    "    plot_bf = pd.DataFrame(1.0 + backtest.mean(axis = 1), columns = ['backtest']).cumprod()\n",
    "    plot_mkt = (1.0 + ff_factors['Mkt-RF'].loc[plot_bf.index]).cumprod()\n",
    "    plot_df = pd.concat([plot_bf, plot_mkt], axis = 1).dropna()\n",
    "    ax = plot_df.plot()\n",
    "    ax.figure.savefig(os.path.join(image_write, data_type + '_Logistic_' + 'l' + '.pdf'))\n",
    "\n",
    "\n",
    "    print(\"Short Only\")\n",
    "    backtest, weights, scaled_backtest = bt.run_backtest(returns, lookback_backtest, input_classifiers= [\"Logistic\"], inputs = feature_set, outputs = binary_returns,  ensemble = 'no', trade_type = 'short')\n",
    "    results, fm_sum = bt.factor_results(backtest, ff_factors_compare)\n",
    "    results_capm, fm_sum_capm = bt.factor_results_capm(backtest, ff_factors_compare)\n",
    "    results_umd, fm_sum_umd = bt.factor_results_umd(backtest, umd)\n",
    "    results_q, fm_sum_q = bt.factor_results_qfactor(backtest, q_factor)\n",
    "    results_tm = bt.treynor_mazuy(backtest, ff_factors_compare)\n",
    "    results_hm = bt.henriksson_merton(backtest, ff_factors_compare)\n",
    "\n",
    "    results_scaled, fm_sum = bt.factor_results(scaled_backtest, ff_factors_compare)\n",
    "    results_capm_scaled, fm_sum_capm = bt.factor_results_capm(scaled_backtest, ff_factors_compare)\n",
    "    results_umd_scaled, fm_sum_umd = bt.factor_results_umd(scaled_backtest, umd)\n",
    "    results_q_scaled, fm_sum_q = bt.factor_results_qfactor(scaled_backtest, q_factor)\n",
    "    results_tm_scaled = bt.treynor_mazuy(scaled_backtest, ff_factors_compare)\n",
    "    results_hm_scaled = bt.henriksson_merton(scaled_backtest, ff_factors_compare)\n",
    "\n",
    "    output_reg.append(results)\n",
    "    output_capm.append(results_capm)\n",
    "    output_umd.append(results_umd)\n",
    "    output_qfactor.append(results_q)\n",
    "    output_tm.append(results_tm)\n",
    "    output_hm.append(results_hm)\n",
    "\n",
    "    output_reg_scaled.append(results_scaled)\n",
    "    output_capm_scaled.append(results_capm_scaled)\n",
    "    output_umd_scaled.append(results_umd_scaled)\n",
    "    output_qfactor_scaled.append(results_q_scaled)\n",
    "    output_tm_scaled.append(results_tm_scaled)\n",
    "    output_hm_scaled.append(results_hm_scaled)\n",
    "\n",
    "    backtest.to_csv(os.path.join(backtest_write, data_type + '_Logistic_' + 's' + '.csv'))\n",
    "    scaled_backtest.to_csv(os.path.join(backtest_write, data_type + '_Logistic_scaled_' + 's' + '.csv'))\n",
    "    plot_bf = pd.DataFrame(1.0 + backtest.mean(axis = 1), columns = ['backtest']).cumprod()\n",
    "    plot_mkt = (1.0 + ff_factors['Mkt-RF'].loc[plot_bf.index]).cumprod()\n",
    "    plot_df = pd.concat([plot_bf, plot_mkt], axis = 1).dropna()\n",
    "    ax = plot_df.plot()\n",
    "    ax.figure.savefig(os.path.join(image_write, data_type + '_Logistic_' + 's' + '.pdf'))\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(\"KNN\")\n",
    "    print(\"Long and Short\")\n",
    "    backtest, weights, scaled_backtest = bt.run_backtest(returns, lookback_backtest, input_classifiers= [\"Nearest Neighbors\"], inputs = feature_set, outputs = binary_returns,  ensemble = 'no', trade_type = 'both')\n",
    "    backtest_dict['KNN_LS'] = backtest.copy()\n",
    "    results, fm_sum = bt.factor_results(backtest, ff_factors_compare)\n",
    "    results_capm, fm_sum_capm = bt.factor_results_capm(backtest, ff_factors_compare)\n",
    "    results_umd, fm_sum_umd = bt.factor_results_umd(backtest, umd)\n",
    "    results_q, fm_sum_q = bt.factor_results_qfactor(backtest, q_factor)\n",
    "    results_sy, fm_sum_sy = bt.factor_results_stambaugh_yuan(backtest, stambaugh_yuan)\n",
    "    results_tm = bt.treynor_mazuy(backtest, ff_factors_compare)\n",
    "    results_hm = bt.henriksson_merton(backtest, ff_factors_compare)\n",
    "\n",
    "    results_scaled, fm_sum = bt.factor_results(scaled_backtest, ff_factors_compare)\n",
    "    results_capm_scaled, fm_sum_capm = bt.factor_results_capm(scaled_backtest, ff_factors_compare)\n",
    "    results_umd_scaled, fm_sum_umd = bt.factor_results_umd(scaled_backtest, umd)\n",
    "    results_q_scaled, fm_sum_q = bt.factor_results_qfactor(scaled_backtest, q_factor)\n",
    "    results_sy_scaled, fm_sum_sy = bt.factor_results_stambaugh_yuan(scaled_backtest, stambaugh_yuan)\n",
    "    results_tm_scaled = bt.treynor_mazuy(scaled_backtest, ff_factors_compare)\n",
    "    results_hm_scaled = bt.henriksson_merton(scaled_backtest, ff_factors_compare)\n",
    "\n",
    "    output_reg.append(results)\n",
    "    output_capm.append(results_capm)\n",
    "    output_umd.append(results_umd)\n",
    "    output_qfactor.append(results_q)\n",
    "    output_tm.append(results_tm)\n",
    "    output_hm.append(results_hm)\n",
    "    output_sy.append(results_sy)\n",
    "\n",
    "    output_reg_scaled.append(results_scaled)\n",
    "    output_capm_scaled.append(results_capm_scaled)\n",
    "    output_umd_scaled.append(results_umd_scaled)\n",
    "    output_qfactor_scaled.append(results_q_scaled)\n",
    "    output_tm_scaled.append(results_tm_scaled)\n",
    "    output_hm_scaled.append(results_hm_scaled)\n",
    "    output_sy_scaled.append(results_sy_scaled)\n",
    "\n",
    "    backtest.to_csv(os.path.join(backtest_write, data_type + '_KNN_' + 'ls' + '.csv'))\n",
    "    scaled_backtest.to_csv(os.path.join(backtest_write, data_type + '_KNN_scaled_' + 'ls' + '.csv'))\n",
    "    plot_bf = pd.DataFrame(1.0 + backtest.mean(axis = 1), columns = ['backtest']).cumprod()\n",
    "    plot_mkt = (1.0 + ff_factors['Mkt-RF'].loc[plot_bf.index]).cumprod()\n",
    "    plot_df = pd.concat([plot_bf, plot_mkt], axis = 1).dropna()\n",
    "    ax = plot_df.plot()\n",
    "    ax.figure.savefig(os.path.join(image_write, data_type + '_KNN_' + 'ls' + '.pdf'))\n",
    "\n",
    "    \n",
    "\n",
    "    print(\"Long Only\")\n",
    "    backtest, weights, scaled_backtest = bt.run_backtest(returns, lookback_backtest, input_classifiers= [\"Nearest Neighbors\"], inputs = feature_set, outputs = binary_returns,  ensemble = 'no', trade_type = 'long')\n",
    "    backtest_dict['KNN_L'] = backtest.copy()\n",
    "    results, fm_sum = bt.factor_results(backtest, ff_factors_compare)\n",
    "    results_capm, fm_sum_capm = bt.factor_results_capm(backtest, ff_factors_compare)\n",
    "    results_umd, fm_sum_umd = bt.factor_results_umd(backtest, umd)\n",
    "    results_q, fm_sum_q = bt.factor_results_qfactor(backtest, q_factor)\n",
    "    results_sy, fm_sum_sy = bt.factor_results_stambaugh_yuan(backtest, stambaugh_yuan)\n",
    "    results_tm = bt.treynor_mazuy(backtest, ff_factors_compare)\n",
    "    results_hm = bt.henriksson_merton(backtest, ff_factors_compare)\n",
    "\n",
    "    results_scaled, fm_sum = bt.factor_results(scaled_backtest, ff_factors_compare)\n",
    "    results_capm_scaled, fm_sum_capm = bt.factor_results_capm(scaled_backtest, ff_factors_compare)\n",
    "    results_umd_scaled, fm_sum_umd = bt.factor_results_umd(scaled_backtest, umd)\n",
    "    results_q_scaled, fm_sum_q = bt.factor_results_qfactor(scaled_backtest, q_factor)\n",
    "    results_sy_scaled, fm_sum_sy = bt.factor_results_stambaugh_yuan(scaled_backtest, stambaugh_yuan)\n",
    "    results_tm_scaled = bt.treynor_mazuy(scaled_backtest, ff_factors_compare)\n",
    "    results_hm_scaled = bt.henriksson_merton(scaled_backtest, ff_factors_compare)\n",
    "\n",
    "    output_reg.append(results)\n",
    "    output_capm.append(results_capm)\n",
    "    output_umd.append(results_umd)\n",
    "    output_qfactor.append(results_q)\n",
    "    output_tm.append(results_tm)\n",
    "    output_hm.append(results_hm)\n",
    "    output_sy.append(results_sy)\n",
    "\n",
    "    output_reg_scaled.append(results_scaled)\n",
    "    output_capm_scaled.append(results_capm_scaled)\n",
    "    output_umd_scaled.append(results_umd_scaled)\n",
    "    output_qfactor_scaled.append(results_q_scaled)\n",
    "    output_tm_scaled.append(results_tm_scaled)\n",
    "    output_hm_scaled.append(results_hm_scaled)\n",
    "    output_sy_scaled.append(results_sy_scaled)\n",
    "\n",
    "    backtest.to_csv(os.path.join(backtest_write, data_type + '_KNN_' + 'l' + '.csv'))\n",
    "    scaled_backtest.to_csv(os.path.join(backtest_write, data_type + '_KNN_scaled_' + 'l' + '.csv'))\n",
    "    plot_bf = pd.DataFrame(1.0 + backtest.mean(axis = 1), columns = ['backtest']).cumprod()\n",
    "    plot_mkt = (1.0 + ff_factors['Mkt-RF'].loc[plot_bf.index]).cumprod()\n",
    "    plot_df = pd.concat([plot_bf, plot_mkt], axis = 1).dropna()\n",
    "    ax = plot_df.plot()\n",
    "    ax.figure.savefig(os.path.join(image_write, data_type + '_KNN_' + 'l' + '.pdf'))\n",
    "\n",
    "\n",
    "    print(\"Short Only\")\n",
    "    backtest, weights, scaled_backtest = bt.run_backtest(returns, lookback_backtest, input_classifiers= [\"Nearest Neighbors\"], inputs = feature_set, outputs = binary_returns,  ensemble = 'no', trade_type = 'short')\n",
    "    backtest_dict['KNN_S'] = backtest.copy()\n",
    "    results, fm_sum = bt.factor_results(backtest, ff_factors_compare)\n",
    "    results_capm, fm_sum_capm = bt.factor_results_capm(backtest, ff_factors_compare)\n",
    "    results_umd, fm_sum_umd = bt.factor_results_umd(backtest, umd)\n",
    "    results_q, fm_sum_q = bt.factor_results_qfactor(backtest, q_factor)\n",
    "    results_sy, fm_sum_sy = bt.factor_results_stambaugh_yuan(backtest, stambaugh_yuan)\n",
    "    results_tm = bt.treynor_mazuy(backtest, ff_factors_compare)\n",
    "    results_hm = bt.henriksson_merton(backtest, ff_factors_compare)\n",
    "\n",
    "    results_scaled, fm_sum = bt.factor_results(scaled_backtest, ff_factors_compare)\n",
    "    results_capm_scaled, fm_sum_capm = bt.factor_results_capm(scaled_backtest, ff_factors_compare)\n",
    "    results_umd_scaled, fm_sum_umd = bt.factor_results_umd(scaled_backtest, umd)\n",
    "    results_q_scaled, fm_sum_q = bt.factor_results_qfactor(scaled_backtest, q_factor)\n",
    "    results_sy_scaled, fm_sum_sy = bt.factor_results_stambaugh_yuan(scaled_backtest, stambaugh_yuan)\n",
    "    results_tm_scaled = bt.treynor_mazuy(scaled_backtest, ff_factors_compare)\n",
    "    results_hm_scaled = bt.henriksson_merton(scaled_backtest, ff_factors_compare)\n",
    "\n",
    "    output_reg.append(results)\n",
    "    output_capm.append(results_capm)\n",
    "    output_umd.append(results_umd)\n",
    "    output_qfactor.append(results_q)\n",
    "    output_tm.append(results_tm)\n",
    "    output_hm.append(results_hm)\n",
    "    output_sy.append(results_sy)\n",
    "\n",
    "    output_reg_scaled.append(results_scaled)\n",
    "    output_capm_scaled.append(results_capm_scaled)\n",
    "    output_umd_scaled.append(results_umd_scaled)\n",
    "    output_qfactor_scaled.append(results_q_scaled)\n",
    "    output_tm_scaled.append(results_tm_scaled)\n",
    "    output_hm_scaled.append(results_hm_scaled)\n",
    "    output_sy_scaled.append(results_sy_scaled)\n",
    "\n",
    "    backtest.to_csv(os.path.join(backtest_write, data_type + '_KNN_' + 's' + '.csv'))\n",
    "    scaled_backtest.to_csv(os.path.join(backtest_write, data_type + '_KNN_scaled_' + 's' + '.csv'))\n",
    "    plot_bf = pd.DataFrame(1.0 + backtest.mean(axis = 1), columns = ['backtest']).cumprod()\n",
    "    plot_mkt = (1.0 + ff_factors['Mkt-RF'].loc[plot_bf.index]).cumprod()\n",
    "    plot_df = pd.concat([plot_bf, plot_mkt], axis = 1).dropna()\n",
    "    ax = plot_df.plot()\n",
    "    ax.figure.savefig(os.path.join(image_write, data_type + '_KNN_' + 's' + '.pdf'))\n",
    "\n",
    "\n",
    "    print(\"Neural Net\")\n",
    "    print(\"Long and Short\")\n",
    "    backtest, weights, scaled_backtest = bt.run_backtest(returns, lookback_backtest, input_classifiers= [\"Neural Net\"], inputs = feature_set, outputs = binary_returns,  ensemble = 'no', trade_type = 'both')\n",
    "    backtest_dict['NN_LS'] = backtest.copy()\n",
    "    results, fm_sum = bt.factor_results(backtest, ff_factors_compare)\n",
    "    results_capm, fm_sum_capm = bt.factor_results_capm(backtest, ff_factors_compare)\n",
    "    results_umd, fm_sum_umd = bt.factor_results_umd(backtest, umd)\n",
    "    results_q, fm_sum_q = bt.factor_results_qfactor(backtest, q_factor)\n",
    "    results_sy, fm_sum_sy = bt.factor_results_stambaugh_yuan(backtest, stambaugh_yuan)\n",
    "    results_tm = bt.treynor_mazuy(backtest, ff_factors_compare)\n",
    "    results_hm = bt.henriksson_merton(backtest, ff_factors_compare)\n",
    "\n",
    "    results_scaled, fm_sum = bt.factor_results(scaled_backtest, ff_factors_compare)\n",
    "    results_capm_scaled, fm_sum_capm = bt.factor_results_capm(scaled_backtest, ff_factors_compare)\n",
    "    results_umd_scaled, fm_sum_umd = bt.factor_results_umd(scaled_backtest, umd)\n",
    "    results_q_scaled, fm_sum_q = bt.factor_results_qfactor(scaled_backtest, q_factor)\n",
    "    results_sy_scaled, fm_sum_sy = bt.factor_results_stambaugh_yuan(scaled_backtest, stambaugh_yuan)\n",
    "    results_tm_scaled = bt.treynor_mazuy(scaled_backtest, ff_factors_compare)\n",
    "    results_hm_scaled = bt.henriksson_merton(scaled_backtest, ff_factors_compare)\n",
    "\n",
    "    output_reg.append(results)\n",
    "    output_capm.append(results_capm)\n",
    "    output_umd.append(results_umd)\n",
    "    output_qfactor.append(results_q)\n",
    "    output_tm.append(results_tm)\n",
    "    output_hm.append(results_hm)\n",
    "    output_sy.append(results_sy)\n",
    "\n",
    "    output_reg_scaled.append(results_scaled)\n",
    "    output_capm_scaled.append(results_capm_scaled)\n",
    "    output_umd_scaled.append(results_umd_scaled)\n",
    "    output_qfactor_scaled.append(results_q_scaled)\n",
    "    output_tm_scaled.append(results_tm_scaled)\n",
    "    output_hm_scaled.append(results_hm_scaled)\n",
    "    output_sy_scaled.append(results_sy_scaled)\n",
    "\n",
    "    backtest.to_csv(os.path.join(backtest_write, data_type + '_NN_' + 'ls' + '.csv'))\n",
    "    scaled_backtest.to_csv(os.path.join(backtest_write, data_type + '_NN_scaled_' + 'ls' + '.csv'))\n",
    "    plot_bf = pd.DataFrame(1.0 + backtest.mean(axis = 1), columns = ['backtest']).cumprod()\n",
    "    plot_mkt = (1.0 + ff_factors['Mkt-RF'].loc[plot_bf.index]).cumprod()\n",
    "    plot_df = pd.concat([plot_bf, plot_mkt], axis = 1).dropna()\n",
    "    ax = plot_df.plot()\n",
    "    ax.figure.savefig(os.path.join(image_write, data_type + '_NN_' + 'ls' + '.pdf'))\n",
    "\n",
    "\n",
    "    print(\"Long Only\")\n",
    "    backtest, weights, scaled_backtest = bt.run_backtest(returns, lookback_backtest, input_classifiers= [\"Neural Net\"], inputs = feature_set, outputs = binary_returns,  ensemble = 'no', trade_type = 'long')\n",
    "    backtest_dict['KNN_L'] = backtest.copy()\n",
    "    results, fm_sum = bt.factor_results(backtest, ff_factors_compare)\n",
    "    results_capm, fm_sum_capm = bt.factor_results_capm(backtest, ff_factors_compare)\n",
    "    results_umd, fm_sum_umd = bt.factor_results_umd(backtest, umd)\n",
    "    results_q, fm_sum_q = bt.factor_results_qfactor(backtest, q_factor)\n",
    "    results_sy, fm_sum_sy = bt.factor_results_stambaugh_yuan(backtest, stambaugh_yuan)\n",
    "    results_tm = bt.treynor_mazuy(backtest, ff_factors_compare)\n",
    "    results_hm = bt.henriksson_merton(backtest, ff_factors_compare)\n",
    "\n",
    "    results_scaled, fm_sum = bt.factor_results(scaled_backtest, ff_factors_compare)\n",
    "    results_capm_scaled, fm_sum_capm = bt.factor_results_capm(scaled_backtest, ff_factors_compare)\n",
    "    results_umd_scaled, fm_sum_umd = bt.factor_results_umd(scaled_backtest, umd)\n",
    "    results_q_scaled, fm_sum_q = bt.factor_results_qfactor(scaled_backtest, q_factor)\n",
    "    results_sy_scaled, fm_sum_sy = bt.factor_results_stambaugh_yuan(scaled_backtest, stambaugh_yuan)\n",
    "    results_tm_scaled = bt.treynor_mazuy(scaled_backtest, ff_factors_compare)\n",
    "    results_hm_scaled = bt.henriksson_merton(scaled_backtest, ff_factors_compare)\n",
    "\n",
    "    output_reg.append(results)\n",
    "    output_capm.append(results_capm)\n",
    "    output_umd.append(results_umd)\n",
    "    output_qfactor.append(results_q)\n",
    "    output_tm.append(results_tm)\n",
    "    output_hm.append(results_hm)\n",
    "    output_sy.append(results_sy)\n",
    "\n",
    "    output_reg_scaled.append(results_scaled)\n",
    "    output_capm_scaled.append(results_capm_scaled)\n",
    "    output_umd_scaled.append(results_umd_scaled)\n",
    "    output_qfactor_scaled.append(results_q_scaled)\n",
    "    output_tm_scaled.append(results_tm_scaled)\n",
    "    output_hm_scaled.append(results_hm_scaled)\n",
    "    output_sy_scaled.append(results_sy_scaled)\n",
    "\n",
    "    backtest.to_csv(os.path.join(backtest_write, data_type + '_NN_' + 'l' + '.csv'))\n",
    "    scaled_backtest.to_csv(os.path.join(backtest_write, data_type + '_NN_scaled_' + 'l' + '.csv'))\n",
    "    plot_bf = pd.DataFrame(1.0 + backtest.mean(axis = 1), columns = ['backtest']).cumprod()\n",
    "    plot_mkt = (1.0 + ff_factors['Mkt-RF'].loc[plot_bf.index]).cumprod()\n",
    "    plot_df = pd.concat([plot_bf, plot_mkt], axis = 1).dropna()\n",
    "    ax = plot_df.plot()\n",
    "    ax.figure.savefig(os.path.join(image_write, data_type + '_NN_' + 'l' + '.pdf'))\n",
    "\n",
    "    print(\"Short Only\")\n",
    "    backtest, weights, scaled_backtest = bt.run_backtest(returns,lookback_backtest, input_classifiers= [\"Neural Net\"], inputs = feature_set, outputs = binary_returns,  ensemble = 'no', trade_type = 'short')\n",
    "    backtest_dict['KNN_S'] = backtest.copy()\n",
    "    results, fm_sum = bt.factor_results(backtest, ff_factors_compare)\n",
    "    results_capm, fm_sum_capm = bt.factor_results_capm(backtest, ff_factors_compare)\n",
    "    results_umd, fm_sum_umd = bt.factor_results_umd(backtest, umd)\n",
    "    results_q, fm_sum_q = bt.factor_results_qfactor(backtest, q_factor)\n",
    "    results_sy, fm_sum_sy = bt.factor_results_stambaugh_yuan(backtest, stambaugh_yuan)\n",
    "    results_tm = bt.treynor_mazuy(backtest, ff_factors_compare)\n",
    "    results_hm = bt.henriksson_merton(backtest, ff_factors_compare)\n",
    "\n",
    "    results_scaled, fm_sum = bt.factor_results(scaled_backtest, ff_factors_compare)\n",
    "    results_capm_scaled, fm_sum_capm = bt.factor_results_capm(scaled_backtest, ff_factors_compare)\n",
    "    results_umd_scaled, fm_sum_umd = bt.factor_results_umd(scaled_backtest, umd)\n",
    "    results_q_scaled, fm_sum_q = bt.factor_results_qfactor(scaled_backtest, q_factor)\n",
    "    results_sy_scaled, fm_sum_sy = bt.factor_results_stambaugh_yuan(scaled_backtest, stambaugh_yuan)\n",
    "    results_tm_scaled = bt.treynor_mazuy(scaled_backtest, ff_factors_compare)\n",
    "    results_hm_scaled = bt.henriksson_merton(scaled_backtest, ff_factors_compare)\n",
    "\n",
    "    output_reg.append(results)\n",
    "    output_capm.append(results_capm)\n",
    "    output_umd.append(results_umd)\n",
    "    output_qfactor.append(results_q)\n",
    "    output_tm.append(results_tm)\n",
    "    output_hm.append(results_hm)\n",
    "    output_sy.append(results_sy)\n",
    "\n",
    "    output_reg_scaled.append(results_scaled)\n",
    "    output_capm_scaled.append(results_capm_scaled)\n",
    "    output_umd_scaled.append(results_umd_scaled)\n",
    "    output_qfactor_scaled.append(results_q_scaled)\n",
    "    output_tm_scaled.append(results_tm_scaled)\n",
    "    output_hm_scaled.append(results_hm_scaled)\n",
    "    output_sy_scaled.append(results_sy_scaled)\n",
    "\n",
    "    backtest.to_csv(os.path.join(backtest_write, data_type + '_NN_' + 's' + '.csv'))\n",
    "    scaled_backtest.to_csv(os.path.join(backtest_write, data_type + '_NN_scaled_' + 's' + '.csv'))\n",
    "    plot_bf = pd.DataFrame(1.0 + backtest.mean(axis = 1), columns = ['backtest']).cumprod()\n",
    "    plot_mkt = (1.0 + ff_factors['Mkt-RF'].loc[plot_bf.index]).cumprod()\n",
    "    plot_df = pd.concat([plot_bf, plot_mkt], axis = 1).dropna()\n",
    "    ax = plot_df.plot()\n",
    "    ax.figure.savefig(os.path.join(image_write, data_type + '_NN_' + 's' + '.pdf'))\n",
    "\n",
    "\n",
    "    print(\"Random Forest\")\n",
    "    print(\"Long and Short\")\n",
    "    backtest, weights, scaled_backtest = bt.run_backtest(returns, lookback_backtest, input_classifiers= [\"Random Forest\"], inputs = feature_set, outputs = binary_returns,  ensemble = 'no', trade_type = 'both')\n",
    "    backtest_dict['RF_LS'] = backtest.copy()\n",
    "    results, fm_sum = bt.factor_results(backtest, ff_factors_compare)\n",
    "    results_capm, fm_sum_capm = bt.factor_results_capm(backtest, ff_factors_compare)\n",
    "    results_umd, fm_sum_umd = bt.factor_results_umd(backtest, umd)\n",
    "    results_q, fm_sum_q = bt.factor_results_qfactor(backtest, q_factor)\n",
    "    results_sy, fm_sum_sy = bt.factor_results_stambaugh_yuan(backtest, stambaugh_yuan)\n",
    "    results_tm = bt.treynor_mazuy(backtest, ff_factors_compare)\n",
    "    results_hm = bt.henriksson_merton(backtest, ff_factors_compare)\n",
    "\n",
    "    results_scaled, fm_sum = bt.factor_results(scaled_backtest, ff_factors_compare)\n",
    "    results_capm_scaled, fm_sum_capm = bt.factor_results_capm(scaled_backtest, ff_factors_compare)\n",
    "    results_umd_scaled, fm_sum_umd = bt.factor_results_umd(scaled_backtest, umd)\n",
    "    results_q_scaled, fm_sum_q = bt.factor_results_qfactor(scaled_backtest, q_factor)\n",
    "    results_sy_scaled, fm_sum_sy = bt.factor_results_stambaugh_yuan(scaled_backtest, stambaugh_yuan)\n",
    "    results_tm_scaled = bt.treynor_mazuy(scaled_backtest, ff_factors_compare)\n",
    "    results_hm_scaled = bt.henriksson_merton(scaled_backtest, ff_factors_compare)\n",
    "\n",
    "    output_reg.append(results)\n",
    "    output_capm.append(results_capm)\n",
    "    output_umd.append(results_umd)\n",
    "    output_qfactor.append(results_q)\n",
    "    output_tm.append(results_tm)\n",
    "    output_hm.append(results_hm)\n",
    "    output_sy.append(results_sy)\n",
    "\n",
    "    output_reg_scaled.append(results_scaled)\n",
    "    output_capm_scaled.append(results_capm_scaled)\n",
    "    output_umd_scaled.append(results_umd_scaled)\n",
    "    output_qfactor_scaled.append(results_q_scaled)\n",
    "    output_tm_scaled.append(results_tm_scaled)\n",
    "    output_hm_scaled.append(results_hm_scaled)\n",
    "    output_sy_scaled.append(results_sy_scaled)\n",
    "\n",
    "    backtest.to_csv(os.path.join(backtest_write, data_type + '_RF_' + 'ls' + '.csv'))\n",
    "    scaled_backtest.to_csv(os.path.join(backtest_write, data_type + '_RF_scaled_' + 'ls' + '.csv'))\n",
    "    plot_bf = pd.DataFrame(1.0 + backtest.mean(axis = 1), columns = ['backtest']).cumprod()\n",
    "    plot_mkt = (1.0 + ff_factors['Mkt-RF'].loc[plot_bf.index]).cumprod()\n",
    "    plot_df = pd.concat([plot_bf, plot_mkt], axis = 1).dropna()\n",
    "    ax = plot_df.plot()\n",
    "    ax.figure.savefig(os.path.join(image_write, data_type + '_RF_' + 'ls' + '.pdf'))\n",
    "\n",
    "\n",
    "    print(\"Long Only\")\n",
    "    backtest, weights, scaled_backtest = bt.run_backtest(returns, lookback_backtest, input_classifiers= [\"Random Forest\"], inputs = feature_set, outputs = binary_returns,  ensemble = 'no', trade_type = 'long')\n",
    "    backtest_dict['RF_L'] = backtest.copy()\n",
    "    results, fm_sum = bt.factor_results(backtest, ff_factors_compare)\n",
    "    results_capm, fm_sum_capm = bt.factor_results_capm(backtest, ff_factors_compare)\n",
    "    results_umd, fm_sum_umd = bt.factor_results_umd(backtest, umd)\n",
    "    results_q, fm_sum_q = bt.factor_results_qfactor(backtest, q_factor)\n",
    "    results_sy, fm_sum_sy = bt.factor_results_stambaugh_yuan(backtest, stambaugh_yuan)\n",
    "    results_tm = bt.treynor_mazuy(backtest, ff_factors_compare)\n",
    "    results_hm = bt.henriksson_merton(backtest, ff_factors_compare)\n",
    "\n",
    "    results_scaled, fm_sum = bt.factor_results(scaled_backtest, ff_factors_compare)\n",
    "    results_capm_scaled, fm_sum_capm = bt.factor_results_capm(scaled_backtest, ff_factors_compare)\n",
    "    results_umd_scaled, fm_sum_umd = bt.factor_results_umd(scaled_backtest, umd)\n",
    "    results_q_scaled, fm_sum_q = bt.factor_results_qfactor(scaled_backtest, q_factor)\n",
    "    results_sy_scaled, fm_sum_sy = bt.factor_results_stambaugh_yuan(scaled_backtest, stambaugh_yuan)\n",
    "    results_tm_scaled = bt.treynor_mazuy(scaled_backtest, ff_factors_compare)\n",
    "    results_hm_scaled = bt.henriksson_merton(scaled_backtest, ff_factors_compare)\n",
    "\n",
    "    output_reg.append(results)\n",
    "    output_capm.append(results_capm)\n",
    "    output_umd.append(results_umd)\n",
    "    output_qfactor.append(results_q)\n",
    "    output_tm.append(results_tm)\n",
    "    output_hm.append(results_hm)\n",
    "    output_sy.append(results_sy)\n",
    "\n",
    "    output_reg_scaled.append(results_scaled)\n",
    "    output_capm_scaled.append(results_capm_scaled)\n",
    "    output_umd_scaled.append(results_umd_scaled)\n",
    "    output_qfactor_scaled.append(results_q_scaled)\n",
    "    output_tm_scaled.append(results_tm_scaled)\n",
    "    output_hm_scaled.append(results_hm_scaled)\n",
    "    output_sy_scaled.append(results_sy_scaled)\n",
    "\n",
    "    backtest.to_csv(os.path.join(backtest_write, data_type + '_RF_' + 'l' + '.csv'))\n",
    "    backtest.to_csv(os.path.join(backtest_write, data_type + '_RF_scaled_' + 'l' + '.csv'))\n",
    "    plot_bf = pd.DataFrame(1.0 + backtest.mean(axis = 1), columns = ['backtest']).cumprod()\n",
    "    plot_mkt = (1.0 + ff_factors['Mkt-RF'].loc[plot_bf.index]).cumprod()\n",
    "    plot_df = pd.concat([plot_bf, plot_mkt], axis = 1).dropna()\n",
    "    ax = plot_df.plot()\n",
    "    ax.figure.savefig(os.path.join(image_write, data_type + '_RF_' + 'l' + '.pdf'))\n",
    "\n",
    "\n",
    "    print(\"Short Only\")\n",
    "    backtest, weights, scaled_backtest = bt.run_backtest(returns, lookback_backtest, input_classifiers= [\"Random Forest\"], inputs = feature_set, outputs = binary_returns,  ensemble = 'no', trade_type = 'short')\n",
    "    backtest_dict['RF_S'] = backtest.copy()\n",
    "    results, fm_sum = bt.factor_results(backtest, ff_factors_compare)\n",
    "    results_capm, fm_sum_capm = bt.factor_results_capm(backtest, ff_factors_compare)\n",
    "    results_umd, fm_sum_umd = bt.factor_results_umd(backtest, umd)\n",
    "    results_q, fm_sum_q = bt.factor_results_qfactor(backtest, q_factor)\n",
    "    results_sy, fm_sum_sy = bt.factor_results_stambaugh_yuan(backtest, stambaugh_yuan)\n",
    "    results_tm = bt.treynor_mazuy(backtest, ff_factors_compare)\n",
    "    results_hm = bt.henriksson_merton(backtest, ff_factors_compare)\n",
    "\n",
    "    results_scaled, fm_sum = bt.factor_results(scaled_backtest, ff_factors_compare)\n",
    "    results_capm_scaled, fm_sum_capm = bt.factor_results_capm(scaled_backtest, ff_factors_compare)\n",
    "    results_umd_scaled, fm_sum_umd = bt.factor_results_umd(scaled_backtest, umd)\n",
    "    results_q_scaled, fm_sum_q = bt.factor_results_qfactor(scaled_backtest, q_factor)\n",
    "    results_sy_scaled, fm_sum_sy = bt.factor_results_stambaugh_yuan(scaled_backtest, stambaugh_yuan)\n",
    "    results_tm_scaled = bt.treynor_mazuy(scaled_backtest, ff_factors_compare)\n",
    "    results_hm_scaled = bt.henriksson_merton(scaled_backtest, ff_factors_compare)\n",
    "\n",
    "    output_reg.append(results)\n",
    "    output_capm.append(results_capm)\n",
    "    output_umd.append(results_umd)\n",
    "    output_qfactor.append(results_q)\n",
    "    output_tm.append(results_tm)\n",
    "    output_hm.append(results_hm)\n",
    "    output_sy.append(results_sy)\n",
    "\n",
    "    output_reg_scaled.append(results_scaled)\n",
    "    output_capm_scaled.append(results_capm_scaled)\n",
    "    output_umd_scaled.append(results_umd_scaled)\n",
    "    output_qfactor_scaled.append(results_q_scaled)\n",
    "    output_tm_scaled.append(results_tm_scaled)\n",
    "    output_hm_scaled.append(results_hm_scaled)\n",
    "    output_sy_scaled.append(results_sy_scaled)\n",
    "\n",
    "    backtest.to_csv(os.path.join(backtest_write, data_type + '_RF_' + 's' + '.csv'))\n",
    "    scaled_backtest.to_csv(os.path.join(backtest_write, data_type + '_RF_scaled' + 's' + '.csv'))\n",
    "    plot_bf = pd.DataFrame(1.0 + backtest.mean(axis = 1), columns = ['backtest']).cumprod()\n",
    "    plot_mkt = (1.0 + ff_factors['Mkt-RF'].loc[plot_bf.index]).cumprod()\n",
    "    plot_df = pd.concat([plot_bf, plot_mkt], axis = 1).dropna()\n",
    "    ax = plot_df.plot()\n",
    "    ax.figure.savefig(os.path.join(image_write, data_type + '_RF_' + 's' + '.pdf'))\n",
    "\n",
    "        \n",
    "        \n",
    "    print(\"AdaBooost\")\n",
    "    print(\"Long and Short\")\n",
    "    backtest, weights, scaled_backtest = bt.run_backtest(returns, lookback_backtest, input_classifiers= [\"AdaBoost\"], inputs = feature_set, outputs = binary_returns,  ensemble = 'no', trade_type = 'both')\n",
    "    backtest_dict['AdaBoost_LS'] = backtest.copy()\n",
    "    results, fm_sum = bt.factor_results(backtest, ff_factors_compare)\n",
    "    results_capm, fm_sum_capm = bt.factor_results_capm(backtest, ff_factors_compare)\n",
    "    results_umd, fm_sum_umd = bt.factor_results_umd(backtest, umd)\n",
    "    results_q, fm_sum_q = bt.factor_results_qfactor(backtest, q_factor)\n",
    "    results_sy, fm_sum_sy = bt.factor_results_stambaugh_yuan(backtest, stambaugh_yuan)\n",
    "    results_tm = bt.treynor_mazuy(backtest, ff_factors_compare)\n",
    "    results_hm = bt.henriksson_merton(backtest, ff_factors_compare)\n",
    "\n",
    "    results_scaled, fm_sum = bt.factor_results(scaled_backtest, ff_factors_compare)\n",
    "    results_capm_scaled, fm_sum_capm = bt.factor_results_capm(scaled_backtest, ff_factors_compare)\n",
    "    results_umd_scaled, fm_sum_umd = bt.factor_results_umd(scaled_backtest, umd)\n",
    "    results_q_scaled, fm_sum_q = bt.factor_results_qfactor(scaled_backtest, q_factor)\n",
    "    results_sy_scaled, fm_sum_sy = bt.factor_results_stambaugh_yuan(scaled_backtest, stambaugh_yuan)\n",
    "    results_tm_scaled = bt.treynor_mazuy(scaled_backtest, ff_factors_compare)\n",
    "    results_hm_scaled = bt.henriksson_merton(scaled_backtest, ff_factors_compare)\n",
    "\n",
    "    output_reg.append(results)\n",
    "    output_capm.append(results_capm)\n",
    "    output_umd.append(results_umd)\n",
    "    output_qfactor.append(results_q)\n",
    "    output_tm.append(results_tm)\n",
    "    output_hm.append(results_hm)\n",
    "    output_sy.append(results_sy)\n",
    "\n",
    "    output_reg_scaled.append(results_scaled)\n",
    "    output_capm_scaled.append(results_capm_scaled)\n",
    "    output_umd_scaled.append(results_umd_scaled)\n",
    "    output_qfactor_scaled.append(results_q_scaled)\n",
    "    output_tm_scaled.append(results_tm_scaled)\n",
    "    output_hm_scaled.append(results_hm_scaled)\n",
    "    output_sy_scaled.append(results_sy_scaled)\n",
    "\n",
    "    backtest.to_csv(os.path.join(backtest_write, data_type + '_AdaBoost_' + 'ls' + '.csv'))\n",
    "    scaled_backtest.to_csv(os.path.join(backtest_write, data_type + '_AdaBoost_scaled_' + 'ls' + '.csv'))\n",
    "    plot_bf = pd.DataFrame(1.0 + backtest.mean(axis = 1), columns = ['backtest']).cumprod()\n",
    "    plot_mkt = (1.0 + ff_factors['Mkt-RF'].loc[plot_bf.index]).cumprod()\n",
    "    plot_df = pd.concat([plot_bf, plot_mkt], axis = 1).dropna()\n",
    "    ax = plot_df.plot()\n",
    "    ax.figure.savefig(os.path.join(image_write, data_type + '_AdaBoost_' + 'ls' + '.pdf'))\n",
    "\n",
    "\n",
    "    print(\"Long Only\")\n",
    "    backtest, weights, scaled_backtest = bt.run_backtest(returns, lookback_backtest, input_classifiers= [\"AdaBoost\"], inputs = feature_set, outputs = binary_returns,  ensemble = 'no', trade_type = 'long')\n",
    "    backtest_dict['AdaBoost_L'] = backtest.copy()\n",
    "    results, fm_sum = bt.factor_results(backtest, ff_factors_compare)\n",
    "    results_capm, fm_sum_capm = bt.factor_results_capm(backtest, ff_factors_compare)\n",
    "    results_umd, fm_sum_umd = bt.factor_results_umd(backtest, umd)\n",
    "    results_q, fm_sum_q = bt.factor_results_qfactor(backtest, q_factor)\n",
    "    results_sy, fm_sum_sy = bt.factor_results_stambaugh_yuan(backtest, stambaugh_yuan)\n",
    "    results_tm = bt.treynor_mazuy(backtest, ff_factors_compare)\n",
    "    results_hm = bt.henriksson_merton(backtest, ff_factors_compare)\n",
    "\n",
    "    results_scaled, fm_sum = bt.factor_results(scaled_backtest, ff_factors_compare)\n",
    "    results_capm_scaled, fm_sum_capm = bt.factor_results_capm(scaled_backtest, ff_factors_compare)\n",
    "    results_umd_scaled, fm_sum_umd = bt.factor_results_umd(scaled_backtest, umd)\n",
    "    results_q_scaled, fm_sum_q = bt.factor_results_qfactor(scaled_backtest, q_factor)\n",
    "    results_sy_scaled, fm_sum_sy = bt.factor_results_stambaugh_yuan(scaled_backtest, stambaugh_yuan)\n",
    "    results_tm_scaled = bt.treynor_mazuy(scaled_backtest, ff_factors_compare)\n",
    "    results_hm_scaled = bt.henriksson_merton(scaled_backtest, ff_factors_compare)\n",
    "\n",
    "    output_reg.append(results)\n",
    "    output_capm.append(results_capm)\n",
    "    output_umd.append(results_umd)\n",
    "    output_qfactor.append(results_q)\n",
    "    output_tm.append(results_tm)\n",
    "    output_hm.append(results_hm)\n",
    "    output_sy.append(results_sy)\n",
    "\n",
    "    output_reg_scaled.append(results_scaled)\n",
    "    output_capm_scaled.append(results_capm_scaled)\n",
    "    output_umd_scaled.append(results_umd_scaled)\n",
    "    output_qfactor_scaled.append(results_q_scaled)\n",
    "    output_tm_scaled.append(results_tm_scaled)\n",
    "    output_hm_scaled.append(results_hm_scaled)\n",
    "    output_sy_scaled.append(results_sy_scaled)\n",
    "\n",
    "    backtest.to_csv(os.path.join(backtest_write, data_type + '_AdaBoost_' + 'l' + '.csv'))\n",
    "    backtest.to_csv(os.path.join(backtest_write, data_type + '_AdaBoost_scaled_' + 'l' + '.csv'))\n",
    "    plot_bf = pd.DataFrame(1.0 + backtest.mean(axis = 1), columns = ['backtest']).cumprod()\n",
    "    plot_mkt = (1.0 + ff_factors['Mkt-RF'].loc[plot_bf.index]).cumprod()\n",
    "    plot_df = pd.concat([plot_bf, plot_mkt], axis = 1).dropna()\n",
    "    ax = plot_df.plot()\n",
    "    ax.figure.savefig(os.path.join(image_write, data_type + '_AdaBoost_' + 'l' + '.pdf'))\n",
    "\n",
    "\n",
    "    print(\"Short Only\")\n",
    "    backtest, weights, scaled_backtest = bt.run_backtest(returns, lookback_backtest, input_classifiers= [\"AdaBoost\"], inputs = feature_set, outputs = binary_returns,  ensemble = 'no', trade_type = 'short')\n",
    "    backtest_dict['AdaBoost_S'] = backtest.copy()\n",
    "    results, fm_sum = bt.factor_results(backtest, ff_factors_compare)\n",
    "    results_capm, fm_sum_capm = bt.factor_results_capm(backtest, ff_factors_compare)\n",
    "    results_umd, fm_sum_umd = bt.factor_results_umd(backtest, umd)\n",
    "    results_q, fm_sum_q = bt.factor_results_qfactor(backtest, q_factor)\n",
    "    results_sy, fm_sum_sy = bt.factor_results_stambaugh_yuan(backtest, stambaugh_yuan)\n",
    "    results_tm = bt.treynor_mazuy(backtest, ff_factors_compare)\n",
    "    results_hm = bt.henriksson_merton(backtest, ff_factors_compare)\n",
    "\n",
    "    results_scaled, fm_sum = bt.factor_results(scaled_backtest, ff_factors_compare)\n",
    "    results_capm_scaled, fm_sum_capm = bt.factor_results_capm(scaled_backtest, ff_factors_compare)\n",
    "    results_umd_scaled, fm_sum_umd = bt.factor_results_umd(scaled_backtest, umd)\n",
    "    results_q_scaled, fm_sum_q = bt.factor_results_qfactor(scaled_backtest, q_factor)\n",
    "    results_sy_scaled, fm_sum_sy = bt.factor_results_stambaugh_yuan(scaled_backtest, stambaugh_yuan)\n",
    "    results_tm_scaled = bt.treynor_mazuy(scaled_backtest, ff_factors_compare)\n",
    "    results_hm_scaled = bt.henriksson_merton(scaled_backtest, ff_factors_compare)\n",
    "\n",
    "    output_reg.append(results)\n",
    "    output_capm.append(results_capm)\n",
    "    output_umd.append(results_umd)\n",
    "    output_qfactor.append(results_q)\n",
    "    output_tm.append(results_tm)\n",
    "    output_hm.append(results_hm)\n",
    "    output_sy.append(results_sy)\n",
    "\n",
    "    output_reg_scaled.append(results_scaled)\n",
    "    output_capm_scaled.append(results_capm_scaled)\n",
    "    output_umd_scaled.append(results_umd_scaled)\n",
    "    output_qfactor_scaled.append(results_q_scaled)\n",
    "    output_tm_scaled.append(results_tm_scaled)\n",
    "    output_hm_scaled.append(results_hm_scaled)\n",
    "    output_sy_scaled.append(results_sy_scaled)\n",
    "\n",
    "    backtest.to_csv(os.path.join(backtest_write, data_type + '_AdaBoost_' + 's' + '.csv'))\n",
    "    scaled_backtest.to_csv(os.path.join(backtest_write, data_type + '_AdaBoost_scaled' + 's' + '.csv'))\n",
    "    plot_bf = pd.DataFrame(1.0 + backtest.mean(axis = 1), columns = ['backtest']).cumprod()\n",
    "    plot_mkt = (1.0 + ff_factors['Mkt-RF'].loc[plot_bf.index]).cumprod()\n",
    "    plot_df = pd.concat([plot_bf, plot_mkt], axis = 1).dropna()\n",
    "    ax = plot_df.plot()\n",
    "    ax.figure.savefig(os.path.join(image_write, data_type + '_AdaBoost_' + 's' + '.pdf'))\n",
    "\n",
    "\n",
    "    print(\"Ensemble System\")\n",
    "    backtest, weights, scaled_backtest = bt.run_backtest(returns, lookback_backtest, input_classifiers= [\"Voter\"], inputs = feature_set, outputs = binary_returns,  ensemble = 'no')  \n",
    "    backtest_dict['Ensemble'] = backtest.copy()\n",
    "    results, fm_sum = bt.factor_results(backtest, ff_factors_compare)\n",
    "    results_capm, fm_sum_capm = bt.factor_results_capm(backtest, ff_factors_compare)\n",
    "    results_umd, fm_sum_umd = bt.factor_results_umd(backtest, umd)\n",
    "    results_q, fm_sum_q = bt.factor_results_qfactor(backtest, q_factor)\n",
    "    results_sy, fm_sum_sy = bt.factor_results_stambaugh_yuan(backtest, stambaugh_yuan)\n",
    "    results_tm = bt.treynor_mazuy(backtest, ff_factors_compare)\n",
    "    results_hm = bt.henriksson_merton(backtest, ff_factors_compare)\n",
    "\n",
    "    results_scaled, fm_sum = bt.factor_results(scaled_backtest, ff_factors_compare)\n",
    "    results_capm_scaled, fm_sum_capm = bt.factor_results_capm(scaled_backtest, ff_factors_compare)\n",
    "    results_umd_scaled, fm_sum_umd = bt.factor_results_umd(scaled_backtest, umd)\n",
    "    results_q_scaled, fm_sum_q = bt.factor_results_qfactor(scaled_backtest, q_factor)\n",
    "    results_sy_scaled, fm_sum_sy = bt.factor_results_stambaugh_yuan(scaled_backtest, stambaugh_yuan)\n",
    "    results_tm_scaled = bt.treynor_mazuy(scaled_backtest, ff_factors_compare)\n",
    "    results_hm_scaled = bt.henriksson_merton(scaled_backtest, ff_factors_compare)\n",
    "\n",
    "    output_reg.append(results)\n",
    "    output_capm.append(results_capm)\n",
    "    output_umd.append(results_umd)\n",
    "    output_qfactor.append(results_q)\n",
    "    output_tm.append(results_tm)\n",
    "    output_hm.append(results_hm)\n",
    "    output_sy.append(results_sy)\n",
    "\n",
    "    output_reg_scaled.append(results_scaled)\n",
    "    output_capm_scaled.append(results_capm_scaled)\n",
    "    output_umd_scaled.append(results_umd_scaled)\n",
    "    output_qfactor_scaled.append(results_q_scaled)\n",
    "    output_tm_scaled.append(results_tm_scaled)\n",
    "    output_hm_scaled.append(results_hm_scaled)\n",
    "    output_sy_scaled.append(results_sy_scaled)\n",
    "\n",
    "    backtest.to_csv(os.path.join(backtest_write, data_type + '_ensemble_' + 'ls' + '.csv'))\n",
    "    scaled_backtest.to_csv(os.path.join(backtest_write, data_type + '_ensemble_scaled_' + 'ls' + '.csv'))\n",
    "    plot_bf = pd.DataFrame(1.0 + backtest.mean(axis = 1), columns = ['backtest']).cumprod()\n",
    "    plot_mkt = (1.0 + ff_factors['Mkt-RF'].loc[plot_bf.index]).cumprod()\n",
    "    plot_df = pd.concat([plot_bf, plot_mkt], axis = 1).dropna()\n",
    "    ax = plot_df.plot()\n",
    "    ax.figure.savefig(os.path.join(image_write, data_type + '_ensemble_' + 'ls' + '.pdf'))\n",
    "\n",
    "\n",
    "    print(\"Simple Binary Decision\")\n",
    "    scaled_backtest = returns.dropna(axis = 1, how = 'all').dropna(axis = 0, how = 'all')\n",
    "    weights = returns.dropna(axis = 1, how = 'all').dropna(axis = 0, how = 'all')\n",
    "    backtest.values[:] = 0\n",
    "    scaled_backtest.values[:] = 0\n",
    "    weights.values[:] = 0\n",
    "    signal = returns.rolling(lookback_backtest).mean().shift(1).dropna(axis = 1, how = 'all').dropna(axis = 0, how = 'all')\n",
    "    std = returns.rolling(lookback_backtest).std().shift(1).dropna(axis = 1, how = 'all').dropna(axis = 0, how = 'all')\n",
    "    std = .05/std\n",
    "    signal[signal >= 0] = 1.0\n",
    "    signal[signal < 0] = -1.0\n",
    "    backtest = (signal * returns).dropna()\n",
    "    backtest_dict['Binary'] = backtest.copy()\n",
    "    weights = signal\n",
    "    scaled_backtest = (signal * returns * std).dropna()\n",
    "\n",
    "\n",
    "\n",
    "    results, fm_sum = bt.factor_results(backtest, ff_factors_compare)\n",
    "    results_capm, fm_sum_capm = bt.factor_results_capm(backtest, ff_factors_compare)\n",
    "    results_umd, fm_sum_umd = bt.factor_results_umd(backtest, umd)\n",
    "    results_q, fm_sum_q = bt.factor_results_qfactor(backtest, q_factor)\n",
    "    results_sy, fm_sum_sy = bt.factor_results_stambaugh_yuan(backtest, stambaugh_yuan)\n",
    "    results_tm = bt.treynor_mazuy(backtest, ff_factors_compare)\n",
    "    results_hm = bt.henriksson_merton(backtest, ff_factors_compare)\n",
    "\n",
    "    results_scaled, fm_sum = bt.factor_results(scaled_backtest, ff_factors_compare)\n",
    "    results_capm_scaled, fm_sum_capm = bt.factor_results_capm(scaled_backtest, ff_factors_compare)\n",
    "    results_umd_scaled, fm_sum_umd = bt.factor_results_umd(scaled_backtest, umd)\n",
    "    results_q_scaled, fm_sum_q = bt.factor_results_qfactor(scaled_backtest, q_factor)\n",
    "    results_sy_scaled, fm_sum_sy = bt.factor_results_stambaugh_yuan(scaled_backtest, stambaugh_yuan)\n",
    "    results_tm_scaled = bt.treynor_mazuy(scaled_backtest, ff_factors_compare)\n",
    "    results_hm_scaled = bt.henriksson_merton(scaled_backtest, ff_factors_compare)\n",
    "\n",
    "    output_reg.append(results)\n",
    "    output_capm.append(results_capm)\n",
    "    output_umd.append(results_umd)\n",
    "    output_qfactor.append(results_q)\n",
    "    output_tm.append(results_tm)\n",
    "    output_hm.append(results_hm)\n",
    "    output_sy.append(results_sy)\n",
    "\n",
    "    output_reg_scaled.append(results_scaled)\n",
    "    output_capm_scaled.append(results_capm_scaled)\n",
    "    output_umd_scaled.append(results_umd_scaled)\n",
    "    output_qfactor_scaled.append(results_q_scaled)\n",
    "    output_tm_scaled.append(results_tm_scaled)\n",
    "    output_hm_scaled.append(results_hm_scaled)\n",
    "    output_sy_scaled.append(results_sy_scaled)\n",
    "\n",
    "\n",
    "    backtest.to_csv(os.path.join(backtest_write, data_type + '_simp_binary_' + 'ls' + '.csv'))\n",
    "    scaled_backtest.to_csv(os.path.join(backtest_write, data_type + '_simp_binary_scaled_' '_' + 'ls' + '.csv'))\n",
    "    plot_bf = pd.DataFrame(1.0 + backtest.mean(axis = 1), columns = ['backtest']).cumprod()\n",
    "    plot_mkt = (1.0 + ff_factors['Mkt-RF'].loc[plot_bf.index]).cumprod()\n",
    "    plot_df = pd.concat([plot_bf, plot_mkt], axis = 1).dropna()\n",
    "    ax = plot_df.plot()\n",
    "    ax.figure.savefig(os.path.join(image_write, data_type + '_sim_binary_scaled_' + 'ls' + '.pdf'))\n",
    "\n",
    "    plt.close('all')\n",
    "    \n",
    "    factor_results = summary_col(output_reg,stars=True,float_format='%0.4f',model_names=names)\n",
    "    factor_results.title = data_type\n",
    "    factor_results_capm = summary_col(output_capm,stars=True,float_format='%0.4f',model_names=names)\n",
    "    factor_results_capm.title = data_type + ' capm'\n",
    "    \n",
    "    factor_results_umd = summary_col(output_umd,stars=True,float_format='%0.4f',model_names=names)\n",
    "    factor_results_umd.title = data_type + ' umd'\n",
    "    \n",
    "    factor_results_q = summary_col(output_qfactor,stars=True,float_format='%0.4f',model_names=names)\n",
    "    factor_results_q.title = data_type + ' q factor'\n",
    "    \n",
    "    factor_results_sy = summary_col(output_sy,stars=True,float_format='%0.4f',model_names=names)\n",
    "    factor_results_sy.title = data_type + ' Stambaugh Yuan'\n",
    "                                                        \n",
    "    factor_results_tm = summary_col(output_tm,stars=True,float_format='%0.4f',model_names=names)\n",
    "    factor_results_tm.title = data_type + ' Treynor and Mazuy'\n",
    "                               \n",
    "    factor_results_hm = summary_col(output_hm,stars=True,float_format='%0.4f',model_names=names)\n",
    "    factor_results_hm.title = data_type + ' Henriksson and Merton'\n",
    "    \n",
    "    resultFile = open(os.path.join(backtest_wd,'outputs' ,outname),'a')\n",
    "    resultFile.write(factor_results.as_text())\n",
    "    resultFile.write('\\n')\n",
    "    resultFile.write(factor_results_capm.as_text())\n",
    "    resultFile.write('\\n')\n",
    "    resultFile.write(factor_results_umd.as_text())\n",
    "    resultFile.write('\\n')\n",
    "    resultFile.write(factor_results_q.as_text())\n",
    "    resultFile.write('\\n')\n",
    "    resultFile.write(factor_results_sy.as_text())\n",
    "    resultFile.write('\\n')\n",
    "    resultFile.write(factor_results_tm.as_text())\n",
    "    resultFile.write('\\n')\n",
    "    resultFile.write(factor_results_hm.as_text())\n",
    "    resultFile.write('\\n')\n",
    "    resultFile.close()\n",
    "    \n",
    "    resultFile = open(os.path.join(backtest_wd,'outputs' ,outnamelatex),'a')\n",
    "    resultFile.write(factor_results.as_latex())\n",
    "    resultFile.write('\\n')\n",
    "    resultFile.write(factor_results_capm.as_latex())\n",
    "    resultFile.write('\\n')\n",
    "    resultFile.write(factor_results_umd.as_latex())\n",
    "    resultFile.write('\\n')\n",
    "    resultFile.write(factor_results_q.as_latex())\n",
    "    resultFile.write('\\n')\n",
    "    resultFile.write(factor_results_sy.as_latex())\n",
    "    resultFile.write('\\n')\n",
    "    resultFile.write(factor_results_tm.as_latex())\n",
    "    resultFile.write('\\n')\n",
    "    resultFile.write(factor_results_hm.as_latex())\n",
    "    resultFile.write('\\n')\n",
    "    resultFile.close()\n",
    "    \n",
    "    add_factor = pd.DataFrame(factor_results.tables[0]).T['intercept']\n",
    "    add_factor.columns = [data_type + '_' +  str(lookback) + '_' + str(lookback_backtest) ]\n",
    "    add_factor.name = data_type + '_' +  str(lookback) + '_' + str(lookback_backtest) \n",
    "    add_factor_capm = pd.DataFrame(factor_results_capm.tables[0]).T['intercept']\n",
    "    add_factor_capm.columns = [data_type + '_' + str(lookback) + '_' + str(lookback_backtest) ]\n",
    "    add_factor_capm.name = data_type + '_' + str(lookback) + '_' + str(lookback_backtest)\n",
    "                               \n",
    "    factor_results_scaled = summary_col(output_reg_scaled,stars=True,float_format='%0.4f',model_names=names)\n",
    "    factor_results_scaled.title = data_type\n",
    "                               \n",
    "    factor_results_capm_scaled = summary_col(output_capm_scaled,stars=True,float_format='%0.4f',model_names=names)\n",
    "    factor_results_capm_scaled.title = data_type + ' capm'\n",
    "    \n",
    "    factor_results_umd_scaled = summary_col(output_umd_scaled,stars=True,float_format='%0.4f',model_names=names)\n",
    "    factor_results_umd_scaled.title = data_type + ' umd'\n",
    "    \n",
    "    factor_results_q_scaled = summary_col(output_qfactor_scaled,stars=True,float_format='%0.4f',model_names=names)\n",
    "    factor_results_q_scaled.title = data_type + ' q factor'\n",
    "\n",
    "    factor_results_sy_scaled = summary_col(output_sy_scaled,stars=True,float_format='%0.4f',model_names=names)\n",
    "    factor_results_sy_scaled.title = data_type + ' stambaugh yuan factor'\n",
    "                                                        \n",
    "    factor_results_tm_scaled = summary_col(output_tm_scaled,stars=True,float_format='%0.4f',model_names=names)\n",
    "    factor_results_tm_scaled.title = data_type + ' Treynor and Mazuy'\n",
    "                    \n",
    "    factor_results_hm_scaled = summary_col(output_hm_scaled,stars=True,float_format='%0.4f',model_names=names)\n",
    "    factor_results_hm_scaled.title = data_type + ' Henriksson and Merton'\n",
    "    \n",
    "    resultFile = open(os.path.join(backtest_wd,'outputs' ,outname_scaled),'a')\n",
    "    resultFile.write(factor_results_scaled.as_text())\n",
    "    resultFile.write('\\n')\n",
    "    resultFile.write(factor_results_capm_scaled.as_text())\n",
    "    resultFile.write('\\n')\n",
    "    resultFile.write(factor_results_umd_scaled.as_text())\n",
    "    resultFile.write('\\n')\n",
    "    resultFile.write(factor_results_q_scaled.as_text())\n",
    "    resultFile.write('\\n')\n",
    "    resultFile.write(factor_results_sy_scaled.as_text())\n",
    "    resultFile.write('\\n')\n",
    "    resultFile.write(factor_results_tm_scaled.as_text())\n",
    "    resultFile.write('\\n')\n",
    "    resultFile.write(factor_results_hm_scaled.as_text())\n",
    "    resultFile.write('\\n')\n",
    "    resultFile.close()\n",
    "    \n",
    "    resultFile = open(os.path.join(backtest_wd,'outputs' ,outnamelatex_scaled),'a')\n",
    "    resultFile.write(factor_results_scaled.as_latex())\n",
    "    resultFile.write('\\n')\n",
    "    resultFile.write(factor_results_capm_scaled.as_latex())\n",
    "    resultFile.write('\\n')\n",
    "    resultFile.write(factor_results_umd_scaled.as_latex())\n",
    "    resultFile.write('\\n')\n",
    "    resultFile.write(factor_results_q_scaled.as_latex())\n",
    "    resultFile.write('\\n')\n",
    "    resultFile.write(factor_results_sy_scaled.as_latex())\n",
    "    resultFile.write('\\n')\n",
    "    resultFile.write(factor_results_tm_scaled.as_latex())\n",
    "    resultFile.write('\\n')\n",
    "    resultFile.write(factor_results_hm_scaled.as_latex())\n",
    "    resultFile.write('\\n')\n",
    "    resultFile.close()\n",
    "    \n",
    "    add_factor_scaled = pd.DataFrame(factor_results_scaled.tables[0]).T['intercept']\n",
    "    add_factor_scaled.columns = [data_type + '_' +  str(lookback) + '_' + str(lookback_backtest) ]\n",
    "    add_factor_scaled.name = data_type + '_' +  str(lookback) + '_' + str(lookback_backtest) \n",
    "    add_factor_capm_scaled = pd.DataFrame(factor_results_capm_scaled.tables[0]).T['intercept']\n",
    "    print(' found table')\n",
    "    print(factor_results_capm_scaled.tables[0])\n",
    "    print(pd.DataFrame(factor_results_capm_scaled.tables[0]).T['intercept'])\n",
    "    add_factor_capm_scaled.columns = [data_type + '_' + str(lookback) + '_' + str(lookback_backtest) ]\n",
    "    add_factor_capm_scaled.name = data_type + '_' + str(lookback) + '_' + str(lookback_backtest)\n",
    "    \n",
    "    p_array_name = []\n",
    "    p_array = []\n",
    "    for k in backtest_dict.keys():\n",
    "        if 'binary' not in k:\n",
    "            s1 = backtest_dict[k].mean(axis = 1).dropna()\n",
    "            s2 = backtest_dict['Binary'].loc[s1.index,:]\n",
    "            t2, p2 = stats.ttest_ind(s1,s2)\n",
    "            p_array_name.append(k)\n",
    "            p_array.append(p2[0])\n",
    "    t_add = pd.DataFrame(p_array)\n",
    "    t_add.index = p_array_name \n",
    "    t_add.columns = [str(lookback) + '_' + str(lookback_backtest)]\n",
    "       \n",
    "                    \n",
    "    \n",
    "    return add_factor, add_factor_capm, factor_results, add_factor_scaled, add_factor_capm_scaled, t_add\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_ff5 = pd.DataFrame()\n",
    "alpha_capm = pd.DataFrame()\n",
    "\n",
    "alpha_ff5_scaled = pd.DataFrame()\n",
    "alpha_capm_scaled = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the famma french factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_factors = pd.read_csv('F-F_Research_Data_5_Factors_2x3.csv', index_col = 0)\n",
    "ff_factors.index = pd.to_datetime(ff_factors.index, format= '%Y%m')\n",
    "ff_factors = ff_factors / 100.0\n",
    "ff_factors[ff_factors == -999] = np.NaN\n",
    "ff_factors[ff_factors == -99.9] = np.NaN\n",
    "ff_factors = ff_factors.dropna(how = 'any', axis = 0)\n",
    "ff_factors = ff_factors[['Mkt-RF', 'SMB', 'HML', 'RMW', 'CMA']]\n",
    "ff_factors_compare = ff_factors.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Mkt-RF     SMB     HML     Mom\n",
      "1963-07-01 -0.0039 -0.0047 -0.0083  0.0100\n",
      "1963-08-01  0.0507 -0.0079  0.0167  0.0103\n",
      "1963-09-01 -0.0157 -0.0048  0.0018  0.0016\n",
      "1963-10-01  0.0253 -0.0129 -0.0010  0.0314\n",
      "1963-11-01 -0.0085 -0.0084  0.0171 -0.0075\n",
      "Index(['Mkt-RF', 'SMB', 'HML', 'Mom'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#get the momentum model\n",
    "mom = pd.read_csv('Mom.csv', index_col = 0)\n",
    "mom.index = pd.to_datetime(mom.index, format= '%Y%m')\n",
    "mom = mom / 100.0\n",
    "mom[mom == -999] = np.NaN\n",
    "mom[mom == -99.9] = np.NaN\n",
    "mom = mom.dropna(how = 'any', axis = 0)\n",
    "umd = pd.concat([ff_factors[['Mkt-RF', 'SMB', 'HML']], mom], axis = 1)\n",
    "umd = umd.dropna()\n",
    "cols_add = []\n",
    "for c in umd.columns:\n",
    "    cols_add.append(c.replace(' ', ''))\n",
    "umd.columns  = cols_add\n",
    "print(umd.head(5))\n",
    "print(umd.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               R_MKT      R_ME      R_IA     R_ROE      R_EG\n",
      "1967-01-01  0.081603  0.067282 -0.029434  0.018866 -0.022469\n",
      "1967-02-01  0.007202  0.016918 -0.002064  0.035317  0.025812\n",
      "1967-03-01  0.039691  0.019517 -0.016933  0.018876 -0.014789\n",
      "1967-04-01  0.038265 -0.007446 -0.029519  0.010983 -0.022351\n",
      "1967-05-01 -0.042419  0.029132  0.024686  0.005234  0.002749\n"
     ]
    }
   ],
   "source": [
    "q_factor = pd.read_csv('q5_factors.csv', index_col = 0)\n",
    "q_factor.index = pd.to_datetime(q_factor.index, format= '%Y%m')\n",
    "q_factor = q_factor / 100.0\n",
    "q_factor = q_factor[['R_MKT', 'R_ME', 'R_IA', 'R_ROE', 'R_EG']]\n",
    "print(q_factor.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             MKTRF       SMB      MGMT      PERF\n",
      "YYYYMM                                          \n",
      "1965-01-01  0.0354  0.025270 -0.007873  0.003137\n",
      "1965-02-01  0.0044  0.035961 -0.004724  0.008442\n",
      "1965-03-01 -0.0134  0.027277 -0.003277 -0.005297\n",
      "1965-04-01  0.0311  0.019929 -0.021635  0.030680\n",
      "1965-05-01 -0.0077  0.003112  0.006468 -0.006760\n"
     ]
    }
   ],
   "source": [
    "stambaugh_yuan =  pd.read_csv('M4-1.csv', index_col = 0)\n",
    "m4 = pd.read_csv('M4-1.csv', index_col = 0)\n",
    "m4.index = pd.to_datetime(m4.index, format= '%Y%m')\n",
    "\n",
    "stambaugh_yuan = m4.copy()\n",
    "stambaugh_yuan = stambaugh_yuan[['MKTRF','SMB','MGMT','PERF']]\n",
    "print(stambaugh_yuan.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            SENT1  SENT2\n",
      "Date                    \n",
      "1965-07-01  -2.11  -1.91\n",
      "1965-08-01  -2.17  -1.92\n",
      "1965-09-01  -2.05  -1.82\n",
      "1965-10-01  -1.91  -1.66\n",
      "1965-11-01  -1.80  -1.67\n"
     ]
    }
   ],
   "source": [
    "industry = pd.read_csv('48_Industry_Portfolios.csv', index_col = 0)\n",
    "industry.index = pd.to_datetime(industry.index, format= '%Y%m')\n",
    "industry[industry <= -99.] = np.nan\n",
    "industry = industry / 100.0\n",
    "\n",
    "ff_factors = pd.read_csv('F-F_Research_Data_5_Factors_2x3.csv', index_col = 0)\n",
    "ff_factors.index = pd.to_datetime(ff_factors.index, format= '%Y%m')\n",
    "ff_factors = ff_factors / 100.0\n",
    "ff_factors[ff_factors == -999] = np.NaN\n",
    "ff_factors[ff_factors == -99.9] = np.NaN\n",
    "ff_factors = ff_factors.dropna(how = 'any', axis = 0)\n",
    "\n",
    "factor_model = ff_factors.copy().dropna(axis = 0, how = 'any')\n",
    "rf = factor_model[['RF']]\n",
    "rf_subtract = rf.copy()\n",
    "factor_model = factor_model[['Mkt-RF', 'SMB', 'HML', 'RMW', 'CMA']]\n",
    "ff_factors_compare = factor_model.copy()\n",
    "\n",
    "rf_subtract.index = pd.to_datetime(rf_subtract.index, format = 'Y%-%m-%d')\n",
    "\n",
    "\n",
    "\n",
    "policy_uncertainty = pd.read_csv('US_Policy_Uncertainty_Data.csv', index_col = 0)\n",
    "policy_uncertainty.index = pd.to_datetime(policy_uncertainty.index, format= '%Y%m')\n",
    "\n",
    "goyal_welch = pd.read_csv('goyal_welch_data.csv', index_col = 0)\n",
    "goyal_welch = goyal_welch.loc[goyal_welch.index.dropna(),:]\n",
    "goyal_welch.index = goyal_welch.index.map(lambda x: int(x))\n",
    "goyal_welch.index = pd.to_datetime(goyal_welch.index, format= '%Y%m')\n",
    "goyal_welch = goyal_welch.dropna(axis = 0, how = 'all').dropna(axis = 1, how = 'all')\n",
    "for x in goyal_welch.columns:\n",
    "    goyal_welch[x] =  pd.to_numeric(goyal_welch[x], errors='coerce')\n",
    "    \n",
    "bw = pd.read_csv('b_w_is.csv', index_col = 0)\n",
    "bw.index = pd.to_datetime(bw.index, format= '%Y%m')\n",
    "print(bw.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            SMB  HML  RMW  CMA\n",
      "1963-07-01  0.0  0.0  1.0  0.0\n",
      "1963-08-01  0.0  1.0  1.0  0.0\n",
      "1963-09-01  0.0  1.0  0.0  1.0\n",
      "1963-10-01  0.0  0.0  1.0  0.0\n",
      "1963-11-01  0.0  1.0  0.0  1.0\n",
      "...         ...  ...  ...  ...\n",
      "2020-04-01  1.0  0.0  1.0  0.0\n",
      "2020-05-01  1.0  0.0  1.0  0.0\n",
      "2020-06-01  1.0  0.0  1.0  1.0\n",
      "2020-07-01  0.0  0.0  1.0  1.0\n",
      "2020-08-01  0.0  0.0  1.0  0.0\n",
      "\n",
      "[686 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "binary_ff = ff_factors.copy()\n",
    "binary_ff[binary_ff >= 0] = 1.0\n",
    "binary_ff[binary_ff < 0] = 0.0\n",
    "binary_ff = binary_ff[['SMB', 'HML', 'RMW', 'CMA']]\n",
    "print(binary_ff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_analysis(input_param):\n",
    "    lookback = input_param[0]\n",
    "    lookback_backtest = input_param[1]\n",
    "    if not os.path.exists(os.path.join(image_wd, str(lookback) + '_' + str(lookback_backtest))):\n",
    "        os.makedirs(os.path.join(image_wd, str(lookback) + '_' + str(lookback_backtest)))\n",
    "\n",
    "    image_write = os.path.join(image_wd, str(lookback) + '_' + str(lookback_backtest))\n",
    "\n",
    "    if not os.path.exists(os.path.join(backtest_wd, str(lookback) + '_' + str(lookback_backtest))):\n",
    "        os.makedirs(os.path.join(backtest_wd, str(lookback) + '_' + str(lookback_backtest)))\n",
    "    if not os.path.exists(os.path.join(backtest_wd, 'outputs')):\n",
    "        os.makedirs(os.path.join(backtest_wd, 'outputs'))\n",
    "\n",
    "    outname = 'output_results_' + str(lookback) + '_' + str(lookback_backtest) +  '.txt'\n",
    "    outnamelatex = 'output_results_latex_' + str(lookback) + '_' + str(lookback_backtest) +  '.txt'\n",
    "\n",
    "    backtest_write = os.path.join(backtest_wd, str(lookback) + '_' + str(lookback_backtest))\n",
    "\n",
    "    resultFile = open(os.path.join(backtest_wd,'outputs' ,outname),'w')\n",
    "    resultFile.close()\n",
    "\n",
    "    resultFile = open(os.path.join(backtest_wd,'outputs' ,outnamelatex),'w')\n",
    "    resultFile.close()\n",
    "\n",
    "    outname = 'output_results_scaled_' + str(lookback) + '_' + str(lookback_backtest) +  '.txt'\n",
    "    outnamelatex = 'output_results_latex_scaled_' + str(lookback) + '_' + str(lookback_backtest) +  '.txt'\n",
    "\n",
    "    backtest_write = os.path.join(backtest_wd, str(lookback) + '_' + str(lookback_backtest))\n",
    "\n",
    "    resultFile = open(os.path.join(backtest_wd,'outputs' ,outname),'w')\n",
    "    resultFile.close()\n",
    "\n",
    "    resultFile = open(os.path.join(backtest_wd,'outputs' ,outnamelatex),'w')\n",
    "    resultFile.close()\n",
    "\n",
    "    ####################################################\n",
    "    ####################################################    \n",
    "    #get the ff5 results\n",
    "    ####################################################\n",
    "    ####################################################\n",
    "    ####################################################\n",
    "\n",
    "\n",
    "\n",
    "    ff_factors = pd.read_csv('F-F_Research_Data_5_Factors_2x3.csv', index_col = 0)\n",
    "    ff_factors.index = pd.to_datetime(ff_factors.index, format= '%Y%m')\n",
    "    ff_factors = ff_factors / 100.0\n",
    "    ff_factors[ff_factors == -999] = np.NaN\n",
    "    ff_factors[ff_factors == -99.9] = np.NaN\n",
    "    ff_factors = ff_factors.dropna(how = 'any', axis = 0)\n",
    "\n",
    "    factor_model = ff_factors.copy()\n",
    "    factor_model = factor_model[['Mkt-RF']]\n",
    "    rf = ff_factors[['RF']]\n",
    "\n",
    "    index_symbol = 'Mkt-RF'\n",
    "\n",
    "    print(ff_factors.head(5))\n",
    "\n",
    "    #start date for data\n",
    "    start = '2004-01-01'\n",
    "    #end date for data\n",
    "    end = '2020-01-01'\n",
    "\n",
    "    #control the leverage\n",
    "    lev = 1.\n",
    "    stk_symbols  = factor_model.columns\n",
    "    returns = factor_model.copy()\n",
    "\n",
    "\n",
    "    #set the backtest parameters\n",
    "    bp = BacktestParameters(stk_symbols, start, end, lookback, lev  )\n",
    "    bt = Backtest(bp, returns, mode = 'insample')\n",
    "    data = Data(bp)\n",
    "\n",
    "    df_alpha, idiosyncratic_vol, binary_alpha = data.get_alpha(returns, factor_model, rf)\n",
    "    rolling_mean_12, rolling_mean_6, rolling_std_12, binary_mean_12,binary_mean_6,binary_returns, rolling_sum_12 = data.get_rolling_features(returns, 0.0)\n",
    "\n",
    "\n",
    "    #creates a dictionary of different features for analysis based on ticker keys\n",
    "    feature_list = [returns, rolling_std_12]\n",
    "\n",
    "    feature_set = data.get_features(feature_list)\n",
    "\n",
    "\n",
    "    for k in feature_set.keys():\n",
    "        feature_set[k] = pd.concat([feature_set[k], bw],axis = 1)\n",
    "        #feature_set[k] = (feature_set[k]) / (feature_set[k].rolling(12).max() - feature_set[k].rolling(12).min())\n",
    "        feature_set[k] = feature_set[k][(feature_set[k].index > binary_alpha.index.min()) & (feature_set[k].index < binary_alpha.index.max())]\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    for k in feature_set.keys():\n",
    "        feature_set[k] = pd.concat([feature_set[k], binary_ff],axis = 1)\n",
    "        feature_set[k] = feature_set[k][(feature_set[k].index > binary_alpha.index.min()) & (feature_set[k].index < binary_alpha.index.max())]\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    data_type = 'Market_Timing'\n",
    "\n",
    "    factor_results, factor_results_capm, factor_reg,factor_results_scaled, factor_results_capm_scaled,t_test = write_outputs(bt, ff_factors_compare, umd, q_factor,stambaugh_yuan, returns, lookback_backtest, feature_set, binary_returns,backtest_write, image_write, cur_wd, data_type )\n",
    "    return (factor_results, factor_results_capm, factor_reg,factor_results_scaled, factor_results_capm_scaled,t_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ttest = pd.DataFrame()\n",
    "alpha_ff5 = pd.DataFrame()\n",
    "alpha_capm = pd.DataFrame()\n",
    "p_ff5 = pd.DataFrame()\n",
    "\n",
    "lookback_list = [50,45,40,35,30,25,20,15,10]\n",
    "lookback_backtest_list = [50,45,40,35,30,25,20,15,12,11,10,9,8]\n",
    "\n",
    "parameters = []\n",
    "for lookback in lookback_list:\n",
    "    for lookback_backtest in lookback_backtest_list:\n",
    "        parameters.append((lookback, lookback_backtest))\n",
    "        \n",
    "pool = Pool(5)\n",
    "results = pool.map(main_analysis, parameters)\n",
    "pool.close()\n",
    "pool.join()\n",
    "    \n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in result:\n",
    "    factor_results = i[0]\n",
    "    factor_results_capm = i[1]\n",
    "    factor_results_scaled = i[3]\n",
    "    factor_results_capm_scaled = i[4]\n",
    "    t_test = i[5]\n",
    "    \n",
    "    if df_ttest.empty:\n",
    "        df_ttest = t_test.copy()\n",
    "    else:\n",
    "        df_ttest = pd.concat([df_ttest, t_test], axis = 1)\n",
    "\n",
    "    if alpha_ff5.empty:\n",
    "        alpha_ff5 = factor_results.copy()\n",
    "        alpha_capm = factor_results_capm.copy()\n",
    "    else:\n",
    "        alpha_ff5 = pd.concat( [alpha_ff5, factor_results], axis = 1)\n",
    "        alpha_capm = pd.concat( [alpha_capm, factor_results_capm], axis = 1)\n",
    "\n",
    "    if alpha_ff5_scaled.empty:\n",
    "        alpha_ff5_scaled = factor_results_scaled.copy()\n",
    "        alpha_capm_scaled = factor_results_capm_scaled.copy()\n",
    "    else:\n",
    "        alpha_ff5_scaled = pd.concat( [alpha_ff5_scaled, factor_results_scaled], axis = 1)\n",
    "        alpha_capm_scaled = pd.concat( [alpha_capm_scaled, factor_results_capm_scaled], axis = 1)   \n",
    "\n",
    "    alpha_ff5.to_csv('alphaff5_mt.csv')\n",
    "    alpha_capm.to_csv('alphacapm_mt.csv')\n",
    "    alpha_ff5_scaled.to_csv('alphaff5_scaled_mt.csv')\n",
    "    alpha_capm_scaled.to_csv('alphacapm_scaled_mt.csv')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_alpha_file(filename):\n",
    "    alpha = pd.read_csv(filename + '.csv', index_col = 0).T\n",
    "\n",
    "    alpha_clean  = alpha.copy()\n",
    "\n",
    "    alpha_clean.loc[:, 'lookback feature'] = ''\n",
    "    alpha_clean.loc[:, 'lookback ml model'] = ''\n",
    "\n",
    "\n",
    "    for count, row in alpha.iterrows():\n",
    "        alpha_clean.loc[count, 'lookback feature'] = count.split('_')[-2]\n",
    "        \n",
    "        alpha_clean.loc[count, 'lookback ml model'] = count.split('_')[-1]\n",
    "\n",
    "    alpha_clean.index.name = 'strategy'\n",
    "\n",
    "    alpha_clean.to_csv(filename + '_cleaned' + '.csv')\n",
    "    alpha_clean.to_latex(filename + '_cleaned' + '.txt')\n",
    "    \n",
    "    return alpha_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets clean up the input of the alpha parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "alpha_capm = clean_alpha_file('alphacapm_mt')\n",
    "alpha_ff5 = clean_alpha_file('alphaff5_mt')\n",
    "alpha_capm_scaled = clean_alpha_file('alphacapm_scaled_mt')\n",
    "alpha_ff5_scaled = clean_alpha_file('alphaff5_scaled_mt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_capm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the alphas for a backtest across individual portfolios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_capm_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_ff5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_ff5_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "model = 'Ensemble'\n",
    "df = alpha_ff5_scaled\n",
    "xdata = []\n",
    "ydata = []\n",
    "zdata =[ ]\n",
    "\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = Axes3D(fig)\n",
    "\n",
    "\n",
    "for ix, row in df.iterrows():\n",
    "    name = ix.split('_')\n",
    "    ml_lookback = float(name[-1])\n",
    "    feature_lookback = float(name[-2])\n",
    "    \n",
    "    xdata.append(ml_lookback)\n",
    "    ydata.append(feature_lookback)\n",
    "    zdata.append(float(str(row[model]).replace('*',''))*12.0*100.0)\n",
    "\n",
    "surf = ax.plot_trisurf(xdata, ydata, zdata, cmap=cm.jet, linewidth=0.1)\n",
    "fig.colorbar(surf, shrink=0.5, aspect=5)\n",
    "ax.set_xlabel('feature length')\n",
    "ax.set_ylabel('machine learning length')\n",
    "ax.set_zlabel('alpha')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = alpha_ff5_scaled.copy()\n",
    "\n",
    "for ix, row in df.iterrows():\n",
    "    name = ix.split('_')\n",
    "    df.loc[ix,'lookback feature'] = float(name[-2])\n",
    "    df.loc[ix,'lookback ml model'] = float(name[-1])\n",
    "    for model in df.columns:\n",
    "        if 'lookback' not in model:\n",
    "            df.loc[ix,model] = float(str(row[model]).replace('*',''))*12.0*100.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hold lookback feature constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_look = {}\n",
    "col_str = 'lookback feature'\n",
    "\n",
    "for l in df[col_str].unique():\n",
    "    df_look[l] = round(df[df[col_str] == l].mean(),2)\n",
    "    \n",
    "df_look = pd.DataFrame.from_dict(df_look)\n",
    "print(df_look)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_look.loc['Ensemble',:].plot.bar(x=col_str, y = 'alpha')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hold lookback ml model constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_look = {}\n",
    "col_str = 'lookback ml model'\n",
    "\n",
    "for l in df[col_str].unique():\n",
    "    df_look[l] = round(df[df[col_str] == l].mean(),2)\n",
    "    \n",
    "df_look = pd.DataFrame.from_dict(df_look)\n",
    "print(df_look)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_look.loc['Ensemble',:].plot.bar(x=col_str, y = 'alpha')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
